# Quality Assessment and Bias Detection

## GRADE System (Grading of Recommendations, Assessment, Development and Evaluation)

The GRADE system provides a structured approach to assessing quality of evidence and strength of recommendations.

### GRADE Quality of Evidence

#### Level A - High Quality Evidence
**Definition**: Very confident that true effect lies close to estimated effect

**Characteristics**:
- Multiple RCTs with no critical limitations
- Consistent findings across studies
- Large sample sizes
- Minimal risk of bias
- No serious indirectness or imprecision

**How to recognize**:
- Well-conducted randomized controlled trials
- Large, well-powered studies
- Low dropout rates
- Adequate randomization and blinding
- Consistent effect sizes across studies

**Example**: Meta-analysis of 15 RCTs, n>3,000, narrow confidence intervals, I² < 25%

#### Level B - Good Quality Evidence
**Definition**: Likely that true effect lies close to estimated effect

**Characteristics**:
- RCTs with minor limitations, OR
- Large observational studies with strong methodology
- Generally consistent findings
- Moderate sample sizes
- Low-to-moderate risk of bias

**How to recognize**:
- RCTs with minor methodological concerns
- Large cohort studies with matched groups
- Generally consistent results
- Adequate follow-up rates (>80%)
- Statistical power for primary outcomes

**Example**: 3-5 RCTs with some heterogeneity, n=500-2,000, good allocation concealment

#### Level C - Fair Quality Evidence
**Definition**: May be biased; confidence in estimate limited

**Characteristics**:
- RCTs with significant limitations, OR
- Observational studies with moderate methodology
- Inconsistent findings
- Small sample sizes
- Moderate risk of bias

**How to recognize**:
- RCTs lacking blinding
- Observational studies with adequate controls
- Moderate inconsistency between studies
- 50-80% follow-up rates
- Limited evidence addressing all relevant outcomes

**Example**: 2-3 RCTs with limitations, observational studies with confounding concerns, n=100-500

#### Level D - Poor Quality Evidence
**Definition**: Very likely biased; little confidence in estimate

**Characteristics**:
- Case reports, case series
- Uncontrolled studies
- Observational studies with major limitations
- Small sample sizes
- High risk of bias

**How to recognize**:
- No control group
- Observational studies with unclear confounding
- <50% follow-up rates
- Highly inconsistent or conflicting results
- Major methodological flaws

**Example**: Case series (n=10-20), uncontrolled studies, significant dropout, inconsistent results

### GRADE Downgrading Factors

Reduce quality level by 1-2 levels for:

1. **Limitations in Design/Execution** (Risk of Bias)
   - Allocation concealment absent
   - Lack of blinding
   - Incomplete outcome data
   - Selective outcome reporting
   - Other biases

2. **Inconsistency**
   - Results vary substantially across studies
   - Unexplained heterogeneity (I² > 50%)
   - Different directions of effect
   - Confidence intervals fail to overlap

3. **Indirectness**
   - Population differs from target
   - Intervention differs from planned
   - Outcomes differ from most relevant
   - Comparison arm differs

4. **Imprecision**
   - Small sample sizes
   - Few events (<100)
   - Wide confidence intervals
   - Crosses null effect threshold

5. **Publication Bias**
   - Asymmetric funnel plot
   - Predominantly positive results
   - Smaller studies show larger effects
   - Grey literature not adequately searched

### GRADE Upgrading Factors

Rarely, increase quality level by 1 level for:

1. **Large Effect Size**
   - Effect size relative risk >2 or <0.5
   - Dose-response relationship present

2. **All Plausible Confounding**
   - Confounders would reduce observed effect
   - Effect remains after accounting for confounders

## Cochrane Risk of Bias Tool (RoB 2)

Assess randomized controlled trials for bias across 5 dimensions:

### Domain 1: Bias Due to Randomization Process

**Key Questions**:
- Was allocation sequence generated randomly?
- Was allocation sequence concealed?
- Did baseline characteristics seem balanced?

**Assessment**:
- **Low risk**: Random sequence generated, allocation concealed, balanced baseline
- **Some concerns**: Unclear randomization or concealment, minor baseline imbalance
- **High risk**: Non-random sequence OR allocation not concealed OR major baseline imbalance

**Evidence**:
- ✓ Low risk: "Randomized using computer-generated list, sealed envelopes"
- ? Some concerns: "Randomized but allocation method not described"
- ✗ High risk: "Alternating allocation based on patient record number"

### Domain 2: Bias Due to Deviations from Intended Interventions

**Key Questions**:
- Were participants blinded to assignment?
- Were providers blinded?
- Were deviations from intended intervention balanced?

**Assessment**:
- **Low risk**: Blinding of participants and/or providers, or deviations unlikely to affect outcome
- **Some concerns**: Incomplete blinding or unclear impact of deviations
- **High risk**: No blinding and deviations likely affect outcome

**Evidence**:
- ✓ Low risk: "Double-blinded, participants and providers unaware of assignment"
- ? Some concerns: "Open-label but low dropout and balanced deviations"
- ✗ High risk: "Open-label with differential dropout by treatment group"

### Domain 3: Bias Due to Missing Outcome Data

**Key Questions**:
- Was outcome data complete?
- Was loss to follow-up balanced across groups?
- Were missing data handled appropriately?

**Assessment**:
- **Low risk**: <5% missing data, balanced dropout, or missing data imputation reasonable
- **Some concerns**: 5-10% missing data, slightly imbalanced dropout
- **High risk**: >10% missing data, markedly imbalanced dropout, or inappropriate handling

**Evidence**:
- ✓ Low risk: "Complete follow-up of 98% of participants, ITT analysis"
- ? Some concerns: "8% loss to follow-up with intention-to-treat analysis"
- ✗ High risk: "30% loss to follow-up, dropout differs 15% between groups"

### Domain 4: Bias Due to Measurement of the Outcome

**Key Questions**:
- Was outcome assessment blinded?
- Was outcome measurement objective or subjective?
- Were measurement methods consistent across groups?

**Assessment**:
- **Low risk**: Outcome assessed blind to intervention, or objective outcome, or consistent blinded assessment
- **Some concerns**: Unclear blinding for subjective outcomes
- **High risk**: Unblinded assessment of subjective outcomes, or inconsistent measurement methods

**Evidence**:
- ✓ Low risk: "Outcomes assessed by blinded lab technician using standardized assay"
- ? Some concerns: "Subjective symptom assessment by unblinded participants"
- ✗ High risk: "Subjective outcome assessment, assessor knew group assignment"

### Domain 5: Bias in Selection of the Reported Result

**Key Questions**:
- Was analysis pre-specified?
- Were all pre-specified outcomes reported?
- Were outcome measures selected appropriately?

**Assessment**:
- **Low risk**: Pre-registered trial with all outcomes reported, or pre-specified analysis
- **Some concerns**: Unclear if outcomes pre-specified, or some outcomes not reported
- **High risk**: No pre-specification, selective outcome reporting evident, outcome definition changed

**Evidence**:
- ✓ Low risk: "Trial pre-registered (NCT#), all pre-specified outcomes reported"
- ? Some concerns**: "Methods mention multiple outcomes; only 3 of 5 reported"
- ✗ High risk: "Originally measured 6 outcomes; only 2 reported due to non-significance"

### Overall Risk of Bias Assessment

- **Low risk**: Low risk in all domains
- **Some concerns**: At least one domain with "some concerns" but no high-risk domains
- **High risk**: At least one domain with high risk of bias

## CASP Quality Assessment Checklists

Structured checklists for appraising qualitative and quantitative studies.

### CASP Qualitative Checklist (10 questions)

1. **Clear Research Aims**
   - Was there a clear statement of the aims and objectives?
   - ✓ Yes: Clear aims stated
   - ? Can't tell: Aims unclear or implied
   - ✗ No: No clear aims

2. **Appropriate Methodology**
   - Is a qualitative methodology appropriate for the research question?
   - ✓ Yes: Qualitative appropriate, justified
   - ? Can't tell: Methodology rationale unclear
   - ✗ No: Quantitative method for qualitative question

3. **Research Design**
   - Was the research design appropriate to address the aims?
   - ✓ Yes: Design matches aims (ethnography, phenomenology, etc.)
   - ? Can't tell: Design not explicit
   - ✗ No: Design doesn't match aims

4. **Recruitment Strategy**
   - Was the recruitment strategy appropriate to the aims?
   - ✓ Yes: Purposive sampling justified, described
   - ? Can't tell: Sampling method unclear
   - ✗ No: Inappropriate sampling

5. **Data Collection**
   - Was the data collected in a way that addressed the research issue?
   - ✓ Yes: Rigorous data collection, documented
   - ? Can't tell: Collection process not described
   - ✗ No: Data collection inappropriate

6. **Researcher-Participant Relationship**
   - Has the relationship between researcher and participants been adequately considered?
   - ✓ Yes: Reflexivity discussed, potential bias addressed
   - ? Can't tell: Relationship not discussed
   - ✗ No: Relationship issues ignored

7. **Ethical Issues**
   - Have ethical issues been taken into consideration?
   - ✓ Yes: Ethical approval documented, informed consent
   - ? Can't tell: Ethical considerations not mentioned
   - ✗ No: Ethical issues not addressed

8. **Data Analysis**
   - Was the data analysis sufficiently rigorous?
   - ✓ Yes: Clear analysis process, iterative, traceable
   - ? Can't tell: Analysis process not clear
   - ✗ No: Analysis superficial or unclear

9. **Clear Statement of Findings**
   - Is there a clear statement of findings?
   - ✓ Yes: Findings clearly stated with evidence
   - ? Can't tell: Findings implied or unclear
   - ✗ No: Findings not clear

10. **Research Value**
    - How valuable is the research?
    - ✓ High: Contributes to knowledge, applicable
    - ? Medium: Some contribution, limited applicability
    - ✗ Low: Minimal contribution

**Overall Quality**:
- 8-10 "Yes": High quality
- 5-7 "Yes": Medium quality
- <5 "Yes": Low quality

### CASP Randomized Controlled Trial Checklist (13 questions)

1. **Clear Focused Question**
2. **Randomization Method** (adequate randomization?)
3. **Allocation Concealment**
4. **Participant Blinding**
5. **Provider Blinding**
6. **Outcome Assessor Blinding**
7. **Baseline Comparability**
8. **Baseline Outcome Measurement**
9. **Follow-up Completeness** (>80%)
10. **Intention-to-Treat Analysis**
11. **Analysis Appropriate** (correct statistical tests)
12. **Clinical Importance** (effect size relevant)
13. **Other Biases** (conflict of interest, funding, etc.)

## ROBINS-I (Risk of Bias In Non-randomized Studies of Interventions)

For observational and non-randomized studies:

### 7 Risk of Bias Domains

1. **Bias due to Confounding**
   - Are baseline confounders measured and controlled?
   - Low: Baseline characteristics measured, confounding controlled
   - High: Important confounders not measured or not controlled

2. **Bias in Selection of Participants**
   - Was participant selection based on intervention received?
   - Low: Selection independent of intervention
   - High: Selection biased toward those receiving intervention

3. **Bias in Classification of Interventions**
   - Was intervention classification accurate and objective?
   - Low: Clear, objective classification
   - High: Subjective or misclassified intervention

4. **Bias due to Deviations from Intended Intervention**
   - Were deviations documented and accounted for?
   - Low: Deviations balanced or appropriately analyzed
   - High: Deviations unbalanced or not analyzed

5. **Bias due to Missing Data**
   - Was data missing systematically?
   - Low: Complete data or missing data handled appropriately
   - High: Systematic missing data or inappropriate handling

6. **Bias in Measurement of Outcomes**
   - Were outcomes measured accurately?
   - Low: Objective or blinded measurement
   - High: Subjective or unblinded measurement

7. **Bias in Selection of Reported Results**
   - Were results selectively reported?
   - Low: Pre-specified analysis
   - High: Post-hoc analysis, selective reporting

## STROBE Checklist (for observational studies)

Ensures complete reporting of observational study characteristics:

### Key STROBE Reporting Items:

1. **Study Design**: Explicit description of design
2. **Setting**: Geographic location and relevant characteristics
3. **Participants**: Inclusion/exclusion criteria, sources
4. **Outcomes**: Primary and secondary outcomes, definitions
5. **Data Sources**: How were data collected?
6. **Bias**: Efforts to address potential bias
7. **Study Size**: Sample size and power calculation
8. **Quantitative Variables**: How were variables defined?
9. **Statistical Methods**: Analysis approach described
10. **Descriptive Data**: Baseline characteristics reported
11. **Outcome Data**: Complete outcome reporting
12. **Main Results**: Clear presentation of main findings
13. **Other Analyses**: Subgroup analysis, sensitivity analysis
14. **Summary**: Key findings summarized
15. **Interpretation**: Results interpreted considering evidence
16. **Generalizability**: Discussion of applicability
17. **Funding**: Funding sources disclosed
18. **Conflicts of Interest**: COI disclosure

## Bias Detection Methods

### Publication Bias

**Definition**: Tendency for positive/significant results to be published more often than negative/null results.

**How to detect**:

1. **Funnel Plot**
   - Plot effect size (x-axis) vs. study size (y-axis)
   - Symmetrical inverted triangle = no bias
   - Asymmetrical plot = publication bias likely
   - Small studies clustered on positive side = publication bias

2. **Egger's Test**
   - Statistical test for funnel plot asymmetry
   - p < 0.05 suggests publication bias
   - More powerful than visual inspection

3. **Trim and Fill Method**
   - Estimates "missing" studies due to bias
   - Provides adjusted pooled estimate

4. **Inspection of Results**
   - Check: Are positive results more common?
   - Expected: ~5% non-significant results (p > 0.05)
   - Suspicious: <2% non-significant results

**How to address**:
- Search for unpublished/grey literature
- Contact researchers for unpublished data
- Acknowledge publication bias in discussion
- Conduct sensitivity analysis excluding small studies
- Use adjusted effect size estimates

### Selection Bias

**Definition**: Systematic difference in characteristics of included vs. excluded studies.

**How to detect**:
1. Examine characteristics of excluded studies
2. Compare demographic distributions
3. Check for geographic/institutional clustering
4. Assess overrepresentation of specific research groups

**How to address**:
- Transparent inclusion/exclusion criteria
- Justify exclusions with specific reasons
- Sensitivity analysis including/excluding borderline studies
- Acknowledge bias limitations in discussion

### Methodological Bias

**Definition**: Studies using weaker designs produce different results than stronger designs.

**How to detect**:
1. Subgroup analysis by study design
2. Meta-regression by methodological quality
3. Sensitivity analysis: exclude low-quality studies
4. Compare RCTs vs. observational studies

**How to address**:
- Weight studies by quality (higher weight for better studies)
- Conduct sensitivity analyses
- Report results stratified by quality
- Discuss implications of methodological variation

### Reporting Bias

**Definition**: Selective reporting of outcomes or analyses based on results.

**How to detect**:
1. Compare methods to results sections
2. Check for pre-registration (clinical trials)
3. Note outcomes mentioned but not analyzed
4. Look for p-hacking indicators (many analyses, few significant)

**How to address**:
- Consult trial registries for pre-specified outcomes
- Contact authors for unreported outcomes
- Acknowledge selective reporting
- Use pre-specification and pre-registration when possible

## Quality Scoring Approaches

### Weighted Quality Scoring

Assign weights based on quality:

```
High quality (Grade A): weight = 1.0
Good quality (Grade B): weight = 0.75
Fair quality (Grade C): weight = 0.5
Poor quality (Grade D): weight = 0.25

Weighted average effect = Σ(effect × weight) / Σ(weights)
```

### Sensitivity Analysis

Examine how results change with different quality thresholds:

```
Primary analysis: Include all studies (Grade A-D)
Sensitivity 1: Exclude poor quality (Grade D)
Sensitivity 2: Exclude fair+ poor quality (Grade C-D)
Sensitivity 3: Only high quality (Grade A)

Report if conclusions change across analyses
```

### Summary Quality Assessment

Create table summarizing quality across dimensions:

| Study | Design | Sample Size | Risk of Bias | Reporting | Overall |
|-------|--------|-------------|--------------|-----------|---------|
| A | RCT | Large (n=500) | Low | Good | A |
| B | Cohort | Moderate (n=150) | Some | Fair | B |
| C | Case-control | Small (n=50) | High | Poor | D |

## Documentation Template

```
QUALITY ASSESSMENT

Study: [Citation]

GRADE Overall Quality: [A/B/C/D]

Risk of Bias Assessment:
- Selection bias: [Low/Some/High]
- Performance bias: [Low/Some/High]
- Detection bias: [Low/Some/High]
- Attrition bias: [Low/Some/High]
- Reporting bias: [Low/Some/High]

Quality Concerns:
1. [Issue 1]: [Impact on results]
2. [Issue 2]: [Impact on results]

Strengths:
1. [Strength 1]
2. [Strength 2]

Overall Assessment:
[Summary of quality and confidence in results]

Confidence in Effect Size: [High/Moderate/Low]
Recommendation: [Include/Exclude/Include with caution]
```

---

**For GRADE application, see SKILL.md. For quick quality checks, see QUICK_REFERENCE.md.**
