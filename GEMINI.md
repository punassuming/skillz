<!-- 
  ⚠️  GENERATED FILE - DO NOT EDIT BY HAND ⚠️
  
  This file was automatically generated by: skillz export --platform gemini
  Generated on: 2025-12-23 21:33:26
  Source: .ai/config.yaml
  
  To regenerate this file, run:
    skillz export --platform gemini
  
  To modify the content, edit:
    - Policies: .ai/config.yaml
    - Skills: skills/**/SKILL.md
    - Commands: commands/**/*.md
-->

# AI Instructions for Gemini CLI

This file provides instructions, skills, and commands for Gemini AI assistant.

---

## Project Guidelines

### Code Quality

Maintain high code quality standards

- Write clean, readable, and maintainable code
- Follow language-specific conventions and best practices
- Include proper error handling and input validation
- Write comprehensive documentation and comments where needed


### Testing

Testing requirements

- Write tests for new features and bug fixes
- Maintain test coverage above 80%
- Run tests before committing changes
- Include both unit and integration tests where appropriate



---

## Available Skills

Skills that extend Gemini's capabilities in this project:

- **brainstorming**: Structured brainstorming and ideation facilitation using proven creativity techniques.
Use when users want to generate ideas, explore solutions, break through creative blocks,
or need facilitated ideation sessions. Triggers include requests like 'help me brainstorm,'
'generate ideas for,' 'creative solutions to,' or 'think of alternatives.'

  - Location: `skills/creative/brainstorming/SKILL.md`
- **claude-light**: Expert assistant for conducting remote experiments with Claude-Light - a web-accessible RGB LED and spectral sensor instrument for statistics, regression, optimization, and design of experiments
  - Location: `skills/programming/claude-light/SKILL.md`
- **code-reviewer**: Comprehensive code review and analysis for software quality assurance. Use when Claude needs to review code in any format including (1) Individual files (Python, R, JavaScript, etc.), (2) Directory structures and project organization, (3) Scripts and automation code, (4) Jupyter notebooks and data analysis workflows, (5) Documentation assessment and improvement suggestions, (6) Bug detection and logic verification, (7) Testing coverage and strategy evaluation, (8) Code consistency and maintainability analysis. Provides actionable improvement recommendations across all aspects of software development.
  - Location: `skills/development/code-reviewer/SKILL.md`
- **design-of-experiments**: Expert guidance for Design of Experiments (DOE) in Python - interactive goal-driven design selection, classical DOE (factorial, response surface, screening), Bayesian optimization with Gaussian processes, model-driven optimal designs, active learning, and sequential experimentation; includes pyDOE3, pycse, GPyOpt, scikit-optimize, statsmodels
  - Location: `skills/scientific/design-of-experiments/SKILL.md`
- **elevenlabs**: AI-powered audio generation using ElevenLabs API - text-to-speech with lifelike voices,
sound effects generation, and music creation from text descriptions. Generate natural-sounding
speech in 32 languages, create custom sound effects for games and videos, and compose
royalty-free music tracks.

Use this skill when the user requests:
- Voice generation or text-to-speech conversion
- Audio narration for content (videos, audiobooks, podcasts)
- Sound effects for games, videos, or applications
- Music generation from text descriptions
- Multi-speaker dialogue or conversation audio
- Voice cloning or custom voice creation
- Audio streaming for real-time applications

Capabilities: Text-to-speech (32 languages, 100+ voices), sound effects generation,
music composition, voice cloning, real-time audio streaming

Python SDK: elevenlabs (pip install elevenlabs)

  - Location: `skills/creative/elevenlabs/SKILL.md`
- **eln**: Professional scientific documentation in org-mode electronic lab notebooks with date-based organization, comprehensive record-keeping of hypotheses, methods, results, reasoning, and outcomes for reproducible research
  - Location: `skills/research/eln/SKILL.md`
- **emacs-lisp**: Expert guidance for writing professional Emacs Lisp code following the
community-driven Emacs Lisp Style Guide by Bozhidar Batsov. Covers layout,
naming conventions, syntax preferences, macros, documentation, and best practices.

Use this skill when writing Emacs configuration, creating Emacs packages,
writing interactive commands, developing major/minor modes, or refactoring
Emacs Lisp code.

Emphasizes: lexical binding, proper naming, documentation standards, autoloading,
and community conventions.

  - Location: `skills/programming/emacs-lisp/SKILL.md`
- **fairchem**: Expert guidance for Meta's FAIRChem library - machine learning methods for materials science and quantum chemistry using pretrained UMA models with ASE integration for fast, accurate predictions
  - Location: `skills/programming/fairchem/SKILL.md`
- **idaes**: Comprehensive guidance for using IDAES (Institute for the Design of Advanced Energy Systems)
for process systems engineering. Covers flowsheet modeling, property packages, unit models,
optimization, scaling, initialization, and diagnostics. Use when working with chemical process
simulations, energy systems modeling, power plant design, material and energy balances, or
process optimization. Triggers include 'IDAES', 'flowsheet', 'process model', 'unit operation',
'process optimization', 'property package', 'energy systems', or process engineering tasks.

  - Location: `skills/programming/idaes/SKILL.md`
- **image-generation**: AI-powered image generation and editing using Google Gemini and OpenAI DALL-E models.
Generate images from text descriptions, edit existing images, create logos/stickers,
apply style transfers, and produce product mockups.

Use this skill when the user requests:
- Image generation from text descriptions
- Image editing or modifications
- Logos, stickers, or graphic design assets
- Product mockups or visualizations
- Style transfers or artistic effects
- Iterative image refinement

Available models: gemini-2.5-flash-image, gemini-3-pro-image-preview, dall-e-3, dall-e-2

Inspired by: https://github.com/EveryInc/every-marketplace/tree/main/plugins/compounding-engineering/skills/gemini-imagegen

  - Location: `skills/creative/image-generation/SKILL.md`
- **literature-review**: Conduct systematic scientific literature reviews by integrating multi-source searches, iterative analysis, and synthesis into well-structured reports with proper citations.

Use this skill when you need to: synthesize knowledge across multiple research domains, identify research gaps, track the evolution of methodologies, evaluate evidence quality, or write comprehensive literature reviews for academic papers, grant proposals, or research projects.

Key capabilities: multi-database search strategies, iterative query refinement, content analysis and summarization, thematic synthesis, citation management, quality assessment, bias detection, and structured report generation with multiple output formats.

  - Location: `skills/research/literature-review/SKILL.md`
- **materials-databases**: Expert assistant for accessing materials databases (AFLOW and Materials Project) - query crystal structures, materials properties, thermodynamic data, and computational results from comprehensive databases
  - Location: `skills/research/materials-databases/SKILL.md`
- **materials-properties**: Expert assistant for calculating materials properties from first-principles using ASE - structure relaxation, surface energies, adsorption, reaction barriers, phonons, elastic constants, and thermodynamic modeling with proper scientific methodology
  - Location: `skills/programming/materials-properties/SKILL.md`
- **opentrons**: Expert guidance for Opentrons Python API v2 - automated liquid handling, protocol development, labware management, and hardware module integration for OT-2 and Flex robots
  - Location: `skills/laboratory/opentrons/SKILL.md`
- **opentrons-absorbance-reader**: Opentrons Absorbance Plate Reader Module for Flex - on-deck microplate spectrophotometry with single/multi-wavelength reading (450, 562, 600, 650nm), automated lid control, and CSV data export for ELISA, cell growth, and colorimetric assays
  - Location: `skills/laboratory/opentrons-absorbance-reader/SKILL.md`
- **opentrons-gripper**: Opentrons Flex Gripper - automated labware movement between deck locations, modules, waste chute, and off-deck storage with precise positioning and offset control for hands-free plate transfers
  - Location: `skills/laboratory/opentrons-gripper/SKILL.md`
- **opentrons-heater-shaker**: Opentrons Heater-Shaker Module - temperature control (37-95°C) with orbital mixing (200-3000 rpm) for cell culture, enzymatic reactions, and sample preparation requiring simultaneous heating and agitation
  - Location: `skills/laboratory/opentrons-heater-shaker/SKILL.md`
- **opentrons-magnetic-block**: Opentrons Magnetic Block for Flex - unpowered magnetic bead separation using gripper-based labware movement with high-strength neodymium magnets for DNA/RNA purification, immunoprecipitation, and bead-based workflows
  - Location: `skills/laboratory/opentrons-magnetic-block/SKILL.md`
- **opentrons-temperature-module**: Opentrons Temperature Module - precise heating and cooling (4-95°C) for sample storage, enzyme reactions, and temperature-sensitive protocols with aluminum block adapters for plates, tubes, and PCR strips
  - Location: `skills/laboratory/opentrons-temperature-module/SKILL.md`
- **opentrons-thermocycler**: Opentrons Thermocycler Module - automated PCR thermal cycling with independent block (4-99°C) and lid (37-110°C) temperature control, profile execution, and auto-sealing lid support (GEN2) for high-throughput molecular biology workflows
  - Location: `skills/laboratory/opentrons-thermocycler/SKILL.md`
- **phd-qualifier**: Expert evaluation of Chemical Engineering PhD qualifying exams - review written reports, presentations, and prepare comprehensive questioning sessions to assess student readiness for doctoral research
  - Location: `skills/academic/phd-qualifier/SKILL.md`
- **planning**: Structured planning and project breakdown using proven methodologies for goals, projects,
and strategic initiatives. Use when users need to create plans, break down complex projects,
set milestones, estimate timelines, identify dependencies, or develop action plans.
Triggers include 'help me plan,' 'create a roadmap for,' 'break down this project,'
'what are the steps to,' or 'how should I approach.'

  - Location: `skills/productivity/planning/SKILL.md`
- **pycalphad**: Expert guidance for pycalphad - computational thermodynamics library implementing the CALPHAD method for calculating phase diagrams, phase equilibria, and thermodynamic properties of multicomponent materials systems using thermodynamic databases (TDB files)
  - Location: `skills/programming/pycalphad/SKILL.md`
- **pycse**: Use when performing regression analysis with confidence intervals, solving ODEs, fitting models to experimental data, or caching expensive scientific computations - provides convenient wrappers around scipy that automatically calculate confidence intervals and prediction bounds for linear, nonlinear, and polynomial regression
  - Location: `skills/python/pycse/SKILL.md`
- **pymatgen**: Comprehensive guidance for using pymatgen (Python Materials Genomics) for computational
materials science. Covers structure creation and manipulation, file I/O (CIF, POSCAR, XYZ),
symmetry analysis, Materials Project API integration, phase diagrams, electronic structure
analysis, and DFT input generation. Use when working with crystal structures, materials
properties, computational chemistry calculations, or materials databases. Triggers include
'pymatgen', 'crystal structure', 'Materials Project', 'CIF file', 'POSCAR', 'band structure',
'phase diagram', or materials analysis tasks.

  - Location: `skills/programming/pymatgen/SKILL.md`
- **python-ase**: Expert assistance with the Atomic Simulation Environment (ASE) Python library for atomistic simulations, including structure building, calculator setup, optimization, dynamics, and analysis
  - Location: `skills/programming/python-ase/SKILL.md`
- **python-best-practices**: Expert guidance for writing professional Python code following industry best practices
including PEP 8 compliance, testing, type hints, error handling, and modern tooling.

Use this skill when writing new Python code, refactoring existing code, setting up
Python projects, implementing tests, or ensuring code quality and maintainability.

Emphasizes: PEP 8, modularity, DRY principle, TDD, virtual environments (uv),
and modern tooling (Ruff, Black, Mypy).

  - Location: `skills/programming/python-best-practices/SKILL.md`
- **python-jax**: Expert guidance for JAX (Just After eXecution) - high-performance numerical computing with automatic differentiation, JIT compilation, vectorization, and GPU/TPU acceleration; includes transformations (grad, jit, vmap, pmap), sharp bits, gotchas, and differences from NumPy
  - Location: `skills/programming/python-jax/SKILL.md`
- **python-multiobjective-optimization**: Expert guidance for multiobjective optimization in Python - Pareto optimality, evolutionary algorithms (NSGA-II, NSGA-III, MOEA/D), scalarization methods, Pareto front analysis, and implementation with pymoo, platypus, and DEAP
  - Location: `skills/programming/python-multiobjective-optimization/SKILL.md`
- **python-optimization**: Expert guidance for mathematical optimization in Python - systematic problem classification, library selection (scipy, pyomo, cvxpy, GEKKO), solver configuration, and implementation patterns for LP, QP, NLP, MIP, convex, and global optimization problems
  - Location: `skills/programming/python-optimization/SKILL.md`
- **python-plotting**: Comprehensive plotting and visualization in Python - matplotlib (static publication-quality plots), seaborn (statistical visualization), and plotly (interactive plots); includes plot types, customization, best practices, and library selection guidance
  - Location: `skills/programming/python-plotting/SKILL.md`
- **python-regression-statistics**: Expert guidance for regression analysis, statistical modeling, and outlier detection in Python using statsmodels, scikit-learn, scipy, and PyOD - includes model diagnostics, assumption checking, robust methods, and comprehensive outlier detection strategies
  - Location: `skills/programming/python-regression-statistics/SKILL.md`
- **scientific-reviewer**: Comprehensive scientific document review and analysis. Use when Claude needs to review scientific papers, reports, preprints, or other research documents for: (1) Identifying and evaluating claims and supporting evidence, (2) Assessing logical argumentation and experimental design, (3) Reviewing citation adequacy and suggesting additional references, (4) Determining document type and research contribution, (5) Checking technical accuracy and methodology, (6) Providing constructive feedback on presentation and clarity. Also handles language, grammar, and formatting review separately.

  - Location: `skills/research/scientific-reviewer/SKILL.md`
- **scientific-workflows**: Expert assistant for choosing and implementing scientific workflow tools - from simple joblib caching to complex orchestration with Prefect, Parsl, FireWorks, and quacc. Recommends the simplest solution that meets requirements.
  - Location: `skills/research/scientific-workflows/SKILL.md`
- **scientific-writing**: Comprehensive scientific writing guidance for research papers, grants, and technical documentation.
Covers paper structure (IMRAD), methods writing, results presentation, figure/table design, citation
formatting, abstract writing, and revision responses. Use when users are writing scientific papers,
formatting manuscripts, responding to reviewers, writing grant proposals, or need help with any aspect
of scientific communication. Triggers include 'write a paper,' 'scientific writing,' 'format my
manuscript,' 'methods section,' 'respond to reviewers,' or any research writing task.

  - Location: `skills/communication/scientific-writing/SKILL.md`
- **tdd**: Test-Driven Development facilitation using the red-green-refactor cycle. Guides users through
writing tests first, implementing minimal code to pass, and refactoring for quality. Use when
users want to practice TDD, need help writing tests before code, are developing new features
test-first, or want guidance on test structure and implementation. Triggers include 'use TDD,'
'test-driven development,' 'write tests first,' 'red-green-refactor,' or requests to develop
functionality with tests.

  - Location: `skills/development/tdd/SKILL.md`
- **troubleshooting**: Systematic debugging and problem diagnosis using structured troubleshooting methodologies
applicable to any domain - technical issues, process failures, system problems, or general
obstacles. Use when users report errors, describe malfunctions, encounter unexpected behavior,
or need help diagnosing root causes. Triggers include 'debug this,' 'troubleshoot,' 'why isn't
this working,' 'getting an error,' 'something's wrong with,' 'how do I fix,' or any problem
description.

  - Location: `skills/technical/troubleshooting/SKILL.md`
- **vasp**: Expert assistant for VASP (Vienna Ab initio Simulation Package) calculations - input file generation, parameter selection, workflow setup, and best practices for accurate DFT calculations
  - Location: `skills/programming/vasp/SKILL.md`
- **version-control**: Expert guidance for Git version control, trunk-based development workflows, and GitHub best practices.
Emphasizes Conventional Commits for clean history, short-lived feature branches, frequent integration,
and professional collaboration patterns. Use when users need help with git commands, branching strategies,
commit messages, PRs, merge conflicts, or git troubleshooting. Triggers include 'git,' 'commit,' 'branch,'
'merge,' 'rebase,' 'PR,' 'pull request,' or version control questions.

  - Location: `skills/development/version-control/SKILL.md`
- **video-storytelling**: Create coherent video story sequences with AI-generated images and narrated audio.
Combines image-generation and elevenlabs skills to produce complete video stories with
visual and narrative consistency across all scenes. Maintains character appearance,
style, lighting, and voice consistency throughout the story.

Use this skill when the user requests:
- Video stories with narration
- Animated story sequences
- Educational video content
- Character-driven narratives with visuals
- Multi-scene story videos
- Narrated image sequences

Features: Visual consistency locks, character persistence, multi-turn image generation,
character voice narration, automatic video assembly

Default: 1 title scene + 5 story scenes
Dependencies: image-generation skill, elevenlabs skill, ffmpeg

  - Location: `skills/creative/video-storytelling/SKILL.md`

---

## Skills Documentation

### brainstorming

**Path**: `skills/creative/brainstorming/SKILL.md`

# Brainstorming & Ideation Facilitation Skill

Guide users through structured, productive brainstorming sessions using proven creativity frameworks and techniques.

## Quick Start Workflow

When a brainstorming request arrives, follow this flow:

1. **Understand**: Clarify the challenge, goal, or question
2. **Choose**: Select appropriate technique(s) based on context
3. **Diverge**: Generate many ideas without judgment
4. **Converge**: Evaluate and prioritize ideas
5. **Action**: Help translate selected ideas into next steps

## When to Use This Skill

Activate for requests involving:
- "Help me brainstorm..."
- "Generate ideas for..."
- "What are creative solutions to..."
- "I need alternatives to..."
- "Think of different ways to..."
- Breaking through creative blocks
- Exploring possibilities before decision-making
- Innovation workshops or ideation sessions

## Technique Selection Guide

Choose techniques based on the situation:

### SCAMPER
**Best for**: Product/service innovation, improving existing solutions
**When**: User has something concrete to modify or enhance
**Time**: 15-20 minutes for thorough exploration

### Six Thinking Hats
**Best for**: Complex decisions, exploring multiple perspectives
**When**: Need structured analysis from different angles
**Time**: 10-15 minutes, can be abbreviated to 3-4 hats

### Mind Mapping
**Best for**: Exploring connections, organizing thoughts, understanding scope
**When**: Topic is broad or user needs to see relationships
**Time**: 10-20 minutes depending on complexity

### Rapid Ideation
**Best for**: Breaking through blocks, generating quantity
**When**: User is stuck or needs many options quickly
**Time**: 5-10 minutes of intense generation

### Reverse Brainstorming
**Best for**: Identifying failure points, problem analysis
**When**: User can't see solutions but can imagine problems
**Time**: 10-15 minutes

### Constraint-Based Creativity
**Best for**: Forcing new perspectives, overcoming assumptions
**When**: Conventional thinking dominates, need fresh angles
**Time**: 5-10 minutes per constraint

## Core Techniques

### 1. SCAMPER Framework

Lead users through systematic idea generation by asking:

**Substitute**: What can be replaced? Different materials, processes, people, or components?
- "What if we substituted X with Y?"
- "Who else could do this instead?"
- "What other ingredients/materials/approaches could work?"

**Combine**: What can be merged? Blend ideas, purposes, or features?
- "What if we combined this with that?"
- "How can we integrate these two aspects?"
- "What happens if we merge these processes?"

**Adapt**: What can be adjusted? Borrow from other contexts or industries?
- "What else is like this that we could learn from?"
- "How do other industries solve similar problems?"
- "What could we copy and adapt?"

**Modify/Magnify/Minify**: What can be changed in size, shape, or attributes?
- "What if this were bigger/smaller?"
- "What if we exaggerated this feature?"
- "What could we minimize or emphasize?"

**Put to another use**: What are alternative applications?
- "Who else could use this?"
- "What other problems could this solve?"
- "How could this be repurposed?"

**Eliminate**: What can be removed or simplified?
- "What if we removed this entirely?"
- "What's not essential?"
- "How can we simplify this?"

**Reverse/Rearrange**: What can be inverted or reordered?
- "What if we did this backwards?"
- "What if we reversed the sequence?"
- "What's the opposite approach?"

**Facilitation approach**: Go through each letter systematically, generating 2-5 ideas per category. Build on previous ideas.

### 2. Six Thinking Hats

Guide users to explore ideas from six distinct perspectives:

**White Hat** (Facts & Information): What do we know? What data do we need?
- Focus on objective information, data, facts
- "What are the facts here?"
- "What information is missing?"

**Red Hat** (Emotions & Intuition): What's your gut feeling? Emotional responses?
- Express feelings without justification
- "How do I feel about this?"
- "What's my intuition saying?"

**Black Hat** (Critical Judgment): What could go wrong? What are the risks?
- Identify weaknesses, risks, obstacles
- "What are the downsides?"
- "Why might this fail?"

**Yellow Hat** (Optimism & Benefits): What are the positives? Why might this work?
- Explore benefits, best-case scenarios
- "What are the advantages?"
- "Why is this valuable?"

**Green Hat** (Creativity & Alternatives): What are new possibilities? How can we innovate?
- Generate alternatives, think laterally
- "What else could we try?"
- "What's a completely different approach?"

**Blue Hat** (Process Control): What's our process? What have we learned?
- Manage thinking process, summarize
- "Where are we in the process?"
- "What's our next step?"

**Facilitation approach**: Spend 2-3 minutes per hat. Can combine or skip hats based on needs. Always end with Blue Hat for summary.

### 3. Mind Mapping

Structure exploration visually and associatively:

1. **Central Topic**: Start with the core question or challenge in the center
2. **Main Branches**: Create 4-7 primary branches for major themes
3. **Sub-branches**: Extend each main branch with related concepts
4. **Connections**: Draw links between related ideas across branches
5. **Keywords**: Use single words or short phrases, not sentences

**Facilitation approach**:
- Help user identify central topic
- Suggest main branch categories
- Encourage free association on sub-branches
- Point out potential connections
- Can represent textually using indentation and bullet points

### 4. Rapid Ideation

Generate maximum quantity in minimum time:

**Process**:
1. Set clear target: "Let's generate 30 ideas in 10 minutes"
2. State challenge as "How might we..." question
3. Rapid-fire idea generation with NO evaluation
4. Push past obvious ideas (ideas 15-30 are often most creative)
5. Use prompts when user slows down

**Rules**:
- Defer ALL judgment
- Quantity over quality
- Build on others' ideas
- Wild ideas encouraged
- Stay focused on topic

**Facilitation approach**: Keep pace fast, offer prompts when user pauses, count ideas to track progress, celebrate hitting targets.

### 5. Reverse Brainstorming

Invert the problem to unlock solutions:

**Process**:
1. Flip the goal: "How could we CAUSE this problem?" or "How could we make this WORSE?"
2. Generate many ways to create the problem
3. Reverse each "bad" idea into a potential solution
4. Evaluate reversed solutions

**Example**:
- Goal: "Improve customer satisfaction"
- Reversed: "How could we make customers hate us?"
- Bad ideas: "Ignore complaints, slow response time, hidden fees"
- Solutions: "Respond quickly to complaints, transparent pricing, proactive communication"

**Facilitation approach**: Make the reversal fun and extreme. Encourage absurd "bad" ideas. Then carefully reverse each one.

### 6. Constraint-Based Creativity

Use limitations to spark innovation:

**Common Constraints**:
- **Zero Budget**: "What if you had no money at all?"
- **Extreme Time**: "What if you had to do this in 24 hours?"
- **No Technology**: "What if you had to solve this without computers?"
- **10x Scale**: "What if you had to serve 10x more people?"
- **Opposite Users**: "How would a child/elderly person/expert approach this?"
- **Different Context**: "How would [company/industry] solve this?"

**Process**:
1. Apply constraint clearly and dramatically
2. Force solutions within that constraint
3. Identify interesting ideas that emerge
4. Relax constraint and keep the insights

**Facilitation approach**: Choose constraint based on what assumptions need breaking. Push user to fully commit to the constraint.

## Facilitation Principles

Apply these throughout all techniques:

1. **Defer Judgment**: Separate generation from evaluation
2. **Quantity First**: More ideas = more good ideas
3. **Build and Combine**: Use "Yes, and..." not "Yes, but..."
4. **Encourage Wild Ideas**: Unusual ideas can lead to practical ones
5. **Visual Thinking**: Use diagrams, sketches, spatial organization
6. **Time Boxing**: Set clear time limits for each phase
7. **Capture Everything**: Write down all ideas, even "bad" ones
8. **Energy Management**: Match technique intensity to user energy
9. **Psychological Safety**: Make it safe to suggest anything
10. **Mix Techniques**: Combine methods for richer results

## Diverge → Converge Process

### Divergent Phase (Generate)
- Maximize quantity
- Suspend evaluation
- Welcome wild ideas
- Build on everything
- Go for variety

### Convergent Phase (Evaluate)
- Group similar ideas
- Identify patterns
- Apply criteria
- Narrow options
- Make decisions

**Don't mix these phases!** Complete divergence before converging.

## Evaluation & Prioritization

When ready to narrow ideas (see `references/evaluation-frameworks.md` for details):

### Quick Filters
- **Gut Check**: Which ideas excite you most?
- **Impact vs Effort**: High impact, low effort = do first
- **Must/Should/Could**: Categorize by necessity
- **Three Stars**: Each person picks top 3

### Structured Methods
- **Weighted Criteria**: Score ideas on key factors
- **Impact/Effort Matrix**: 2x2 grid placement
- **Feasibility Analysis**: Can we actually do this?
- **Risk Assessment**: What could go wrong?

## Common Patterns & Adaptations

**For individual brainstorming**: Focus on mind mapping, SCAMPER, and constraint-based methods

**For group facilitation**: Use Six Hats for structure, rapid ideation for energy, reverse brainstorming for engagement

**For technical problems**: SCAMPER and constraint-based work well

**For strategic decisions**: Six Thinking Hats provides comprehensive analysis

**For creative blocks**: Rapid ideation, reverse brainstorming, or random prompts break through resistance

**For innovation**: Combine SCAMPER with constraint-based creativity

## Using Supporting Resources

This skill includes additional resources:

- **references/techniques-detailed.md**: Step-by-step guides for each technique with examples
- **references/evaluation-frameworks.md**: Complete evaluation and prioritization methods
- **references/prompts-library.md**: 50+ creative prompts organized by category
- **scripts/random_prompt.py**: Generate random creativity prompts on demand

Reference these when users need:
- Deeper technique guidance
- More structure for evaluation
- Inspiration to unstick thinking
- Random creative constraints

## Session Structure Template

For a complete brainstorming session:

1. **Setup** (2-3 minutes)
   - Clarify the challenge
   - Set goals: "How many ideas?" "What decision?"
   - Choose technique(s)

2. **Warm-up** (2-3 minutes, optional)
   - Quick creative prompt to loosen thinking
   - Example: "Name 20 uses for a brick"

3. **Main Generation** (10-20 minutes)
   - Apply chosen technique(s)
   - Generate without judgment
   - Push past obvious ideas

4. **Capture & Organize** (3-5 minutes)
   - Group similar ideas
   - Identify themes
   - Count what you have

5. **Evaluation** (5-10 minutes)
   - Apply chosen criteria
   - Narrow to top candidates
   - Flag ideas needing development

6. **Next Steps** (2-3 minutes)
   - Choose 1-3 ideas to pursue
   - Define immediate actions
   - Schedule follow-up if needed

## Adapting to Context

**Time-constrained**: Use rapid ideation + quick gut check evaluation

**Well-defined problem**: SCAMPER for depth

**Ambiguous situation**: Mind mapping to understand, then choose technique

**Group conflict**: Six Hats for structured perspective-taking

**Innovation pressure**: Constraint-based + reverse brainstorming

**Decision paralysis**: Rapid ideation to get unstuck, then Impact/Effort matrix

## Tips for Effective Facilitation

- **Start with clarity**: Reframe vague requests into "How might we..." questions
- **Match energy**: High-energy techniques for engagement, structured for focus
- **Use silence**: Let user think; don't fill every pause
- **Build momentum**: Start with easier prompts, increase difficulty
- **Celebrate quantity**: Count ideas, celebrate milestones
- **Avoid premature convergence**: Keep generating even when "good enough" ideas appear
- **Document visually**: Use formatting, bullets, numbering, grouping
- **End with action**: Always conclude with concrete next steps

## Example Trigger Responses

When user says: **"Help me brainstorm ideas for a new feature"**
→ Clarify goal → Suggest SCAMPER for feature enhancement → Generate systematically → Evaluate with Impact/Effort

When user says: **"I'm stuck on how to solve X"**
→ Try reverse brainstorming → "How could we make X worse?" → Generate problems → Flip to solutions

When user says: **"Give me creative alternatives to Y"**
→ Use constraint-based creativity → Apply unusual constraints → Generate within limits → Relax and refine

When user says: **"What are all the angles on this decision?"**
→ Six Thinking Hats → Go through each perspective → Summarize with Blue Hat → Support decision

## Output Format Suggestions

Present ideas in clear, scannable formats:

**For Generation Phase**:
```
IDEA 1: [Title]
→ Brief description

IDEA 2: [Title]
→ Brief description
```

**For Grouped Ideas**:
```
CATEGORY A: [Theme]
• Idea 1
• Idea 2
• Idea 3

CATEGORY B: [Theme]
• Idea 4
• Idea 5
```

**For Evaluated Ideas**:
```
HIGH PRIORITY:
✓ Idea X - High impact, low effort
✓ Idea Y - Solves core problem

MEDIUM PRIORITY:
○ Idea Z - Good potential, needs research

PARKING LOT:
◇ Idea W - Interesting but not now
```

## Closing a Session

Before ending a brainstorming session:

1. **Summarize**: Recap what was generated
2. **Highlight**: Call out most promising ideas
3. **Capture decisions**: Document what user chose to pursue
4. **Define actions**: Clear next steps with owners
5. **Appreciate creativity**: Acknowledge the thinking work done

---

**Remember**: The goal is not to have perfect ideas, but to have enough ideas that some will be perfect. Trust the process, stay playful, and keep generating.

---

### claude-light

**Path**: `skills/programming/claude-light/SKILL.md`

# Claude-Light Experimental Skills

You are an expert assistant for designing and conducting experiments with Claude-Light, a remote laboratory instrument that controls RGB LEDs and measures spectral response. Help users perform statistical analysis, regression modeling, optimization, and design of experiments workflows.

## What is Claude-Light?

Claude-Light is a Raspberry Pi-based remote experimental instrument that:
- Controls RGB LEDs (inputs: R, G, B values from 0 to 1)
- Measures light intensity across 10 spectral channels
- Provides web interfaces and REST API for remote access
- Enables hands-on learning of experimental methods and data science

**Key Features:**
- No special software required (web browser or Python requests)
- Automatic logging of all experiments
- Camera documentation of LED states
- Multiple sophistication levels (web forms, API, Python scripting)

## Installation

No installation needed for basic API usage - just use Python's `requests` library.

```bash
# Basic requirement
pip install requests

# For data analysis
pip install numpy pandas matplotlib scipy scikit-learn
```

## API Access

### Endpoint

```
https://claude-light.cheme.cmu.edu/api
```

### Parameters

- **R**: Red channel (0.0 to 1.0)
- **G**: Green channel (0.0 to 1.0)
- **B**: Blue channel (0.0 to 1.0)

### Response Format

Returns JSON with:
- Input parameters (R, G, B)
- Spectral measurements at 10 channels:
  - `415nm`, `445nm`, `480nm`, `515nm`, `555nm`, `590nm`, `630nm`, `680nm`
  - `clear` (total intensity)
  - `nir` (near-infrared)

### Basic Usage

```python
import requests

# Send experiment
response = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': 0.5, 'G': 0.3, 'B': 0.8})

# Get data
data = response.json()
print(data)

# Access specific wavelength
intensity_515 = data['out']['515nm']
```

## Experimental Workflows

### 1. Reproducibility and Statistics

**Goal**: Assess measurement variability and statistical properties

```python
import requests
import numpy as np

# Repeat same measurement multiple times
R, G, B = 0.5, 0.5, 0.5
measurements = []

for i in range(30):
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()
    measurements.append(data['out']['515nm'])

# Calculate statistics
measurements = np.array(measurements)
print(f"Mean: {np.mean(measurements):.2f}")
print(f"Std Dev: {np.std(measurements):.2f}")
print(f"Median: {np.median(measurements):.2f}")
print(f"95% CI: {np.percentile(measurements, [2.5, 97.5])}")
```

**Analysis Tasks:**
- Compute mean, median, standard deviation
- Plot histograms and assess normality
- Calculate confidence intervals
- Determine measurement precision

### 2. Linear Regression (Single Variable)

**Goal**: Establish input-output relationship

```python
import requests
import numpy as np
from scipy.stats import linregress

# Vary one input, keep others constant
R_values = np.linspace(0, 1, 11)
outputs = []

for R in R_values:
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': 0, 'B': 0})
    data = resp.json()
    outputs.append(data['out']['630nm'])  # Red wavelength

# Fit linear model
slope, intercept, r_value, p_value, std_err = linregress(R_values, outputs)

print(f"Slope: {slope:.2f}")
print(f"Intercept: {intercept:.2f}")
print(f"R²: {r_value**2:.4f}")
print(f"Std Error: {std_err:.2f}")

# Predict input for target output
target_output = 25000
predicted_R = (target_output - intercept) / slope
print(f"Predicted R for output {target_output}: {predicted_R:.3f}")
```

**Analysis Tasks:**
- Fit linear models
- Calculate R², RMSE, MAE
- Quantify parameter uncertainties
- Validate predictions experimentally

### 3. Multivariate Regression

**Goal**: Model multiple inputs affecting multiple outputs

```python
import requests
import numpy as np
from sklearn.linear_model import LinearRegression

# Design of experiments - grid sampling
R_vals = np.linspace(0.1, 0.9, 5)
G_vals = np.linspace(0.1, 0.9, 5)

# Collect data
X = []  # Inputs
y_515 = []  # Output at 515nm
y_630 = []  # Output at 630nm

for R in R_vals:
    for G in G_vals:
        resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                           params={'R': R, 'G': G, 'B': 0})
        data = resp.json()

        X.append([R, G])
        y_515.append(data['out']['515nm'])
        y_630.append(data['out']['630nm'])

X = np.array(X)
y_515 = np.array(y_515)

# Fit model
model = LinearRegression()
model.fit(X, y_515)

print(f"R coefficient: {model.coef_[0]:.2f}")
print(f"G coefficient: {model.coef_[1]:.2f}")
print(f"Intercept: {model.intercept_:.2f}")
print(f"R² score: {model.score(X, y_515):.4f}")

# Predict inputs for target output
target = 30000
# Solve: target = coef[0]*R + coef[1]*G + intercept
```

**Analysis Tasks:**
- Multi-input, multi-output modeling
- Feature importance analysis
- Interaction effects
- Simultaneous constraint satisfaction

### 4. Optimization

**Goal**: Find inputs that produce desired outputs

```python
import requests
import numpy as np
from scipy.optimize import minimize

def objective(inputs):
    """Minimize difference from target output."""
    R, G, B = inputs

    # Constrain to valid range
    R = np.clip(R, 0, 1)
    G = np.clip(G, 0, 1)
    B = np.clip(B, 0, 1)

    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()

    # Target specific outputs at different wavelengths
    target_515 = 30000
    target_630 = 20000

    actual_515 = data['out']['515nm']
    actual_630 = data['out']['630nm']

    # Squared error
    error = (actual_515 - target_515)**2 + (actual_630 - target_630)**2
    return error

# Optimize
initial_guess = [0.5, 0.5, 0.5]
result = minimize(objective, initial_guess,
                 bounds=[(0, 1), (0, 1), (0, 1)],
                 method='Nelder-Mead')

print(f"Optimal R, G, B: {result.x}")
print(f"Final error: {result.fun}")
```

**Optimization Methods:**
- Scipy.optimize (Nelder-Mead, Powell, L-BFGS-B)
- Bayesian optimization (scikit-optimize)
- Grid search with interpolation
- Active learning approaches

### 5. Design of Experiments (DOE)

**Goal**: Efficient experimental design for maximum information

```python
import requests
import numpy as np
from scipy.stats import qmc

# Latin Hypercube Sampling
sampler = qmc.LatinHypercube(d=3)  # 3 dimensions: R, G, B
n_samples = 20
sample = sampler.random(n=n_samples)

# Scale to [0, 1]
samples = qmc.scale(sample, [0, 0, 0], [1, 1, 1])

# Run experiments
results = []
for R, G, B in samples:
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()
    results.append({
        'R': R, 'G': G, 'B': B,
        'output_515': data['out']['515nm'],
        'output_630': data['out']['630nm']
    })

# Analyze space-filling design
import pandas as pd
df = pd.DataFrame(results)
```

**DOE Strategies:**
- Latin hypercube sampling
- Factorial designs (full, fractional)
- Response surface methodology
- Optimal design criteria (D-optimal, A-optimal)

### 6. Machine Learning Approaches

**Goal**: Use advanced ML models for prediction

```python
import requests
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# Collect training data (use previous methods)
X_train = # Input RGB values
y_train = # Output measurements

# Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
print(f"RF R² score: {rf.score(X_train, y_train):.4f}")

# Neural Network
mlp = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000)
mlp.fit(X_train, y_train)
print(f"MLP R² score: {mlp.score(X_train, y_train):.4f}")

# Polynomial regression
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])
poly_model.fit(X_train, y_train)
```

**ML Models to Try:**
- Linear models with regularization (Ridge, Lasso)
- Decision trees and random forests
- Neural networks (MLPRegressor)
- Gaussian processes
- XGBoost, LightGBM
- Support vector regression

### 7. Uncertainty Quantification

**Goal**: Quantify confidence in predictions

```python
import requests
import numpy as np
from scipy import stats

# Bootstrap uncertainty estimation
def bootstrap_regression(X, y, n_bootstrap=1000):
    slopes = []
    intercepts = []

    for _ in range(n_bootstrap):
        # Resample with replacement
        indices = np.random.choice(len(X), size=len(X), replace=True)
        X_boot = X[indices]
        y_boot = y[indices]

        # Fit model
        slope, intercept, _, _, _ = stats.linregress(X_boot, y_boot)
        slopes.append(slope)
        intercepts.append(intercept)

    return np.array(slopes), np.array(intercepts)

# Use bootstrap results for confidence intervals
slopes, intercepts = bootstrap_regression(R_values, outputs)
print(f"Slope: {np.mean(slopes):.2f} ± {np.std(slopes):.2f}")
print(f"95% CI: {np.percentile(slopes, [2.5, 97.5])}")
```

**Uncertainty Methods:**
- Bootstrap resampling
- Prediction intervals
- Parameter covariance matrices
- Cross-validation
- Monte Carlo simulation

## Best Practices

### Data Management

```python
# Save experiments to CSV
import pandas as pd

experiments = []
for R in np.linspace(0, 1, 10):
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': 0, 'B': 0})
    data = resp.json()
    experiments.append({
        'R': R, 'G': 0, 'B': 0,
        **data['out']  # Unpack all spectral measurements
    })

df = pd.DataFrame(experiments)
df.to_csv('experiments.csv', index=False)
```

### Background Subtraction

```python
# Measure ambient light
def get_background():
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': 0, 'G': 0, 'B': 0})
    return resp.json()['out']

bg = get_background()

# Subtract from measurements
def corrected_measurement(R, G, B):
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()

    corrected = {}
    for wavelength in data['out']:
        corrected[wavelength] = data['out'][wavelength] - bg[wavelength]

    return corrected
```

### Averaging for Noise Reduction

```python
def averaged_measurement(R, G, B, n_repeats=5):
    """Take multiple measurements and return average."""
    measurements = []

    for _ in range(n_repeats):
        resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                           params={'R': R, 'G': G, 'B': B})
        data = resp.json()
        measurements.append(data['out'])

    # Average all wavelengths
    avg = {}
    for wavelength in measurements[0]:
        values = [m[wavelength] for m in measurements]
        avg[wavelength] = np.mean(values)

    return avg
```

### Caching Results

```python
import pickle
from pathlib import Path

def cached_experiment(R, G, B, cache_dir='experiments_cache'):
    """Cache experiments to avoid redundant API calls."""
    Path(cache_dir).mkdir(exist_ok=True)

    # Create unique filename
    cache_file = Path(cache_dir) / f"R{R:.3f}_G{G:.3f}_B{B:.3f}.pkl"

    if cache_file.exists():
        with open(cache_file, 'rb') as f:
            return pickle.load(f)

    # Perform experiment
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()

    # Cache result
    with open(cache_file, 'wb') as f:
        pickle.dump(data, f)

    return data
```

## Visualization

```python
import matplotlib.pyplot as plt

# Plot spectral response
def plot_spectrum(R, G, B):
    resp = requests.get('https://claude-light.cheme.cmu.edu/api',
                       params={'R': R, 'G': G, 'B': B})
    data = resp.json()

    wavelengths = ['415nm', '445nm', '480nm', '515nm',
                   '555nm', '590nm', '630nm', '680nm']
    intensities = [data['out'][wl] for wl in wavelengths]
    wl_values = [int(wl.replace('nm', '')) for wl in wavelengths]

    plt.figure(figsize=(10, 6))
    plt.plot(wl_values, intensities, 'o-')
    plt.xlabel('Wavelength (nm)')
    plt.ylabel('Intensity')
    plt.title(f'Spectrum for R={R}, G={G}, B={B}')
    plt.grid(True)
    plt.show()

# Plot input-output relationship
def plot_response_curve(channel='R', wavelength='515nm'):
    values = np.linspace(0, 1, 20)
    outputs = []

    for val in values:
        params = {'R': 0, 'G': 0, 'B': 0}
        params[channel] = val

        resp = requests.get('https://claude-light.cheme.cmu.edu/api', params=params)
        data = resp.json()
        outputs.append(data['out'][wavelength])

    plt.figure(figsize=(10, 6))
    plt.plot(values, outputs, 'o-')
    plt.xlabel(f'{channel} Input')
    plt.ylabel(f'Output at {wavelength}')
    plt.title(f'{channel} Response at {wavelength}')
    plt.grid(True)
    plt.show()
```

## Common Experimental Patterns

### Pattern 1: Single Variable Sweep

```python
# Systematically vary one input
for value in np.linspace(0, 1, 10):
    data = get_measurement(R=value, G=0, B=0)
    analyze(data)
```

### Pattern 2: Factorial Design

```python
# Test all combinations
for R in [0, 0.5, 1]:
    for G in [0, 0.5, 1]:
        for B in [0, 0.5, 1]:
            data = get_measurement(R, G, B)
```

### Pattern 3: Gradient Descent

```python
# Iteratively approach target
current = [0.5, 0.5, 0.5]
learning_rate = 0.1

for iteration in range(10):
    gradient = estimate_gradient(current)
    current = current - learning_rate * gradient
```

## Error Handling

```python
import requests
from requests.exceptions import Timeout, ConnectionError

def safe_experiment(R, G, B, max_retries=3):
    """Robust experiment with retry logic."""
    for attempt in range(max_retries):
        try:
            resp = requests.get(
                'https://claude-light.cheme.cmu.edu/api',
                params={'R': R, 'G': G, 'B': B},
                timeout=10
            )
            resp.raise_for_status()
            return resp.json()

        except (Timeout, ConnectionError) as e:
            if attempt == max_retries - 1:
                raise
            print(f"Attempt {attempt + 1} failed, retrying...")
            continue
```

## Resources

- GitHub Repository: https://github.com/jkitchin/claude-light
- API Endpoint: https://claude-light.cheme.cmu.edu/api
- Web Interface: https://claude-light.cheme.cmu.edu/rgb
- See `examples/` for complete experimental workflows
- See `references/` for detailed methodology guides

---

### code-reviewer

**Path**: `skills/development/code-reviewer/SKILL.md`

# Code Reviewer

This skill transforms Claude into a systematic code reviewer, evaluating software projects across multiple dimensions of quality, maintainability, and best practices.

## Review Framework

Conduct code reviews using this structured approach:

### 1. Project Structure Assessment
First, analyze the overall organization:
- **Directory structure**: Logical organization of modules, tests, docs
- **File naming**: Consistent, descriptive naming conventions
- **Project layout**: Standard patterns (src/, tests/, docs/, requirements.txt, etc.)
- **Configuration files**: Presence and quality of setup.py, requirements.txt, .gitignore, etc.
- **Entry points**: Clear main scripts or module initialization

### 2. Documentation Review
Evaluate documentation comprehensiveness and quality:

#### File-level Documentation
- **Module docstrings**: Clear purpose, usage examples, API overview
- **README files**: Installation, usage, examples, contribution guidelines
- **Inline comments**: Explain why, not what; up-to-date and relevant
- **API documentation**: Function/class docstrings with parameters, returns, exceptions

#### Code Documentation Standards
- **Docstring format**: Consistent style (Google, NumPy, Sphinx)
- **Type hints**: Present and accurate where appropriate
- **Example usage**: Working code examples in docstrings
- **Change documentation**: CHANGELOG, version history

### 3. Logic and Bug Detection
Systematic analysis for potential issues:

#### Common Bug Patterns
- **Null/None handling**: Proper checks before usage
- **Index errors**: Array bounds checking, off-by-one errors
- **Type mismatches**: Incompatible operations, incorrect assumptions
- **Logic errors**: Incorrect conditions, inverted logic, unreachable code
- **Resource leaks**: File handles, database connections, memory management

#### Algorithm Review
- **Correctness**: Does the code solve the intended problem?
- **Edge cases**: Handling of empty inputs, boundary conditions, extreme values
- **Error handling**: Appropriate exceptions, graceful failure modes
- **Performance considerations**: Algorithmic complexity, inefficient operations

### 4. Testing Assessment
Evaluate testing strategy and coverage:

#### Test Presence and Quality
- **Unit tests**: Individual function/class testing
- **Integration tests**: Component interaction testing
- **Test coverage**: Percentage and quality of code coverage
- **Test data**: Realistic, edge case, and error condition testing
- **Test organization**: Clear structure, naming, and documentation

#### Testing Best Practices
- **Test independence**: Tests don't depend on each other
- **Assertion quality**: Specific, meaningful test assertions
- **Mock usage**: Appropriate mocking of dependencies
- **Parametrized tests**: Efficient testing of multiple scenarios

### 5. Code Quality and Consistency
Review for maintainability and style:

#### Code Style
- **Formatting consistency**: Indentation, spacing, line length
- **Naming conventions**: Variables, functions, classes follow standards
- **Code organization**: Logical grouping, appropriate function/class sizes
- **Import organization**: Clean, organized, no unused imports

#### Code Smells
- **Duplicated code**: Repeated logic that should be refactored
- **Long functions/classes**: Overly complex, should be broken down
- **Dead code**: Unused functions, variables, or imports
- **Magic numbers**: Hard-coded values without explanation
- **Inconsistent patterns**: Mixed coding styles or approaches

### 6. Jupyter Notebook Specific Review
Additional considerations for notebooks:

#### Structure and Flow
- **Cell organization**: Logical sequence, appropriate cell types
- **Narrative quality**: Clear markdown explanations between code cells
- **Reproducibility**: Cells can be run in order without errors
- **Output management**: Appropriate inclusion/exclusion of outputs

#### Data Science Best Practices
- **Data loading**: Clear data source documentation and validation
- **Exploratory analysis**: Well-documented investigation process
- **Visualization quality**: Clear, labeled, meaningful plots
- **Results interpretation**: Clear explanations of findings

## Review Output Structure

### Executive Summary
- **Overall assessment**: Code quality rating and key concerns
- **Primary recommendations**: Top 3-5 most important improvements
- **Strengths**: Notable positive aspects of the codebase
- **Risk level**: Critical, moderate, or minor issues identified

### Detailed Analysis

#### Documentation Assessment
```
Component: [File/module name]
Current state: [Brief description of existing documentation]
Issues: [Specific gaps or problems]
Recommendations: [Actionable improvements]
Priority: [High/Medium/Low]
```

#### Logic and Bug Review
```
Location: [File:line or function name]
Issue type: [Bug/Logic error/Edge case]
Description: [Clear explanation of the problem]
Impact: [Potential consequences]
Suggested fix: [Specific code changes or approach]
```

#### Testing Analysis
```
Coverage assessment: [Current state and gaps]
Missing tests: [Specific areas needing test coverage]
Test quality issues: [Problems with existing tests]
Recommendations: [Specific testing strategies to implement]
```

#### Code Quality Issues
```
Pattern: [Code smell or inconsistency type]
Locations: [Specific files/functions affected]
Impact: [Effect on maintainability/readability]
Refactoring suggestion: [Specific improvement approach]
```

### Improvement Roadmap
Prioritized action items:

1. **Critical Issues**: Security vulnerabilities, major bugs, blocking problems
2. **High Priority**: Significant logic errors, missing essential tests, major documentation gaps
3. **Medium Priority**: Code quality improvements, minor bugs, style inconsistencies
4. **Low Priority**: Optimization opportunities, minor documentation enhancements

## Language-Specific Considerations

### Python
- **PEP 8 compliance**: Style guide adherence
- **Virtual environment**: Dependencies management
- **Package structure**: Proper __init__.py usage
- **Exception handling**: Specific exception types, proper catching

### R
- **Coding style**: Consistent naming (snake_case vs camelCase)
- **Package documentation**: NAMESPACE, DESCRIPTION files
- **Function documentation**: Roxygen2 comments
- **Testing framework**: testthat usage

### JavaScript/Node.js
- **ES6+ features**: Modern JavaScript usage
- **Package.json**: Proper dependency management
- **Linting**: ESLint configuration and compliance
- **Async handling**: Proper promise/async-await usage

### General Best Practices
- **Version control**: Proper .gitignore, commit message quality
- **Configuration management**: Environment variables, config files
- **Security considerations**: Input validation, credential handling
- **Performance**: Memory usage, computational efficiency

## Feedback Guidelines

### Constructive Criticism
- **Be specific**: Reference exact locations and code snippets
- **Explain rationale**: Why the change improves the code
- **Offer alternatives**: Multiple approaches when possible
- **Consider context**: Understand project constraints and requirements

### Positive Recognition
- **Acknowledge good practices**: Highlight well-written code
- **Note improvements**: Recognize progress from previous versions
- **Appreciate design decisions**: Credit thoughtful architectural choices

### Actionable Recommendations
Each suggestion should include:
- **Clear description** of the problem or opportunity
- **Specific code changes** or implementation approach
- **Expected benefits** of making the change
- **Implementation effort** estimate (low/medium/high)

## Review Process Checklist

Before finalizing review:
- [ ] Checked all files in scope for review
- [ ] Verified code can be run/imported without errors
- [ ] Reviewed test files if present
- [ ] Checked documentation completeness
- [ ] Identified security or performance concerns
- [ ] Provided specific, actionable feedback
- [ ] Prioritized recommendations appropriately
- [ ] Maintained constructive, professional tone

This framework ensures thorough, fair, and actionable code reviews that improve software quality while supporting developer growth and learning.

---

### design-of-experiments

**Path**: `skills/scientific/design-of-experiments/SKILL.md`

# Design of Experiments (DOE) - Interactive Expert

## Overview

Master experimental design through **interactive, goal-driven guidance** that asks the right questions to recommend the best approach for your situation. This skill covers classical DOE, Bayesian optimization, model-driven designs, and active learning—helping you choose between batch and sequential strategies, screening and optimization, and exploration and exploitation.

**Core value:** Don't start with a method—start with questions. Based on your goals, budget, and constraints, get personalized recommendations for experimental design strategies that maximize information gain per experiment.

## CRITICAL: Start with Questions, Not Methods

**When a user mentions DOE or experimental design, ASK THESE QUESTIONS FIRST:**

### Primary Questions (Ask Before Recommending):

1. **"What is your primary goal?"**
   - **Screening**: Identify which factors matter (many factors → few important)
   - **Optimization**: Find best settings for known factors
   - **Exploration**: Understand the system/build model
   - **Model discrimination**: Choose between competing models
   - **Robustness**: Minimize sensitivity to noise

2. **"Can you run experiments sequentially, or must they be done in a batch?"**
   - **Sequential**: Run one (or few), get results, decide next experiment(s)
   - **Batch**: Must plan all experiments upfront
   - **Hybrid**: Sequential with occasional batches

3. **"How expensive is each experiment?"**
   - **Very expensive**: >$1000 or >1 day per experiment
   - **Moderate**: $100-$1000 or hours per experiment
   - **Cheap**: <$100 or minutes per experiment

4. **"How many factors are you investigating?"**
   - **1-5 factors**: Full factorial, RSM feasible
   - **6-15 factors**: Need screening or fractional designs
   - **>15 factors**: Definitive screening, regularization needed

5. **"Do you have prior knowledge, existing data, or a mechanistic model?"**
   - **Have model**: Use model-driven optimal designs
   - **Have data**: Warm-start Bayesian optimization
   - **No knowledge**: Need exploration, space-filling designs

### Based on Answers, Recommend:

```
Sequential + Expensive + Unknown landscape
    → Bayesian Optimization (GP + Expected Improvement)

Sequential + Have model + Parameter estimation goal
    → Model-driven sequential optimal design

Batch only + Screening goal + Many factors
    → Fractional factorial or Definitive Screening Design

Batch only + Optimization + Few factors
    → Central Composite Design or Box-Behnken

Sequential + Moderate cost + Want model
    → Active learning with Gaussian Process

Batch only + No prior knowledge + Exploration
    → Latin Hypercube Sampling
```

## Quick Decision Tree

```
Can experiments be run sequentially with feedback?
│
├─ YES (Sequential possible)
│   │
│   ├─ Expensive experiments (>$1000 or >1 day each)?
│   │   ├─ YES → Bayesian Optimization
│   │   │        • Expected Improvement for optimization
│   │   │        • Upper Confidence Bound for exploration
│   │   │        • Tools: GPyOpt, scikit-optimize, BoTorch
│   │   │
│   │   └─ NO → Have a model?
│   │       ├─ YES → Model-driven sequential design
│   │       │        • D-optimal for parameter estimation
│   │       │        • Update design as parameters learned
│   │       │
│   │       └─ NO → Active learning
│   │                • GP with uncertainty sampling
│   │                • Tools: modAL, custom GP
│   │
└─ NO (Batch only)
    │
    ├─ What's your goal?
    │   │
    │   ├─ SCREENING (identify important factors)
    │   │   ├─ <20 runs available → Plackett-Burman, Fractional Factorial
    │   │   ├─ 20-50 runs → Definitive Screening Design
    │   │   └─ Tools: pyDOE3, dexpy
    │   │
    │   ├─ OPTIMIZATION (find best settings)
    │   │   ├─ 2-5 factors → Central Composite Design, Box-Behnken
    │   │   ├─ >5 factors → Sequential screening → then optimize
    │   │   ├─ Have model → D-optimal or I-optimal
    │   │   └─ Tools: pyDOE3, pycse, statsmodels
    │   │
    │   ├─ EXPLORATION (understand system)
    │   │   ├─ No prior → Latin Hypercube Sampling
    │   │   ├─ Build surrogate → Space-filling + modeling
    │   │   └─ Tools: pyDOE3, pycse, scipy
    │   │
    │   └─ ROBUSTNESS (minimize variability)
    │       └─ Control + noise factors → Taguchi / Robust Parameter Design
```

## When to Use Each Approach

### Classical DOE (Batch Experiments)

**Use when:**
- Must plan all experiments upfront
- Well-understood system or standard optimization
- Moderate number of factors (<15)
- Experiments are cheap enough for comprehensive coverage

**Best for:**
- Screening many factors (Plackett-Burman, fractional factorial)
- Response surface modeling (CCD, Box-Behnken)
- Standard process optimization
- Teaching/learning DOE concepts

**Tools:** pyDOE3, dexpy, pycse, statsmodels

### Bayesian Optimization (Sequential)

**Use when:**
- Can run experiments one-at-a-time (or small batches)
- Experiments are expensive (time, money, resources)
- Black-box system (no mechanistic model)
- Want to minimize total number of experiments

**Best for:**
- Optimizing expensive processes ($1000+ per run)
- Materials discovery, drug screening
- Hyperparameter tuning for ML models
- Autonomous experimentation / self-driving labs

**Tools:** GPyOpt, scikit-optimize, BoTorch, Ax

### Model-Driven DOE (Optimal Designs)

**Use when:**
- Have mechanistic or empirical model
- Goal is parameter estimation or model discrimination
- Want statistically optimal experiments
- Can update designs based on current parameter estimates

**Best for:**
- Chemical kinetics (rate constant estimation)
- Pharmacokinetics (PK parameter estimation)
- Systems biology (model calibration)
- Comparing competing models

**Tools:** Custom implementation with Fisher Information Matrix, pyoptex

### Active Learning (Sequential Model Building)

**Use when:**
- Want to build accurate surrogate model efficiently
- Can run experiments sequentially
- Need to balance exploration and exploitation
- Moderate experiment cost

**Best for:**
- Adaptive sampling for complex landscapes
- Building GP surrogates for later optimization
- Uncertainty quantification
- Smart data collection

**Tools:** modAL, scikit-learn, GPy

## Quick Reference Table

| Situation | Recommended Approach | Key Tool | Typical # Runs |
|-----------|---------------------|----------|----------------|
| Batch, <5 factors, screening | Full/Fractional Factorial | pyDOE3 | 8-32 |
| Batch, 6-15 factors, screening | Plackett-Burman, DSD | pyDOE3, dexpy | 12-50 |
| Batch, optimization, 2-5 factors | CCD, Box-Behnken | pyDOE3, pycse | 13-50 |
| Batch, exploration, unknown | Latin Hypercube | pyDOE3, pycse | 10×factors |
| **Sequential, expensive, optimize** | **Bayesian Optimization** | **GPyOpt, skopt** | **10-50** |
| Sequential, build model | Active Learning + GP | modAL | 20-100 |
| Have mechanistic model | Model-driven D-optimal | Custom FIM | 10-30 |
| Parameter estimation | D-optimal, A-optimal | pyoptex | Varies |
| Model discrimination | T-optimal, KL-optimal | Custom | 10-20 |
| Multiple objectives | Multi-objective BO | BoTorch | 30-100 |
| Mixture components | Simplex designs | pyDOE3 | 10-30 |

## Quick Start Examples

### Example 1: Classical Response Surface (Batch)

**Situation:** Optimize 3 factors, batch experiments, moderate cost

```python
import pyDOE3 as pyd
import pandas as pd

# Generate Central Composite Design
n_factors = 3
design = pyd.ccdesign(n_factors, center=(0, 4))  # With center points

# Create DataFrame with factor names
factors = ['Temperature', 'Pressure', 'Catalyst']
df = pd.DataFrame(design, columns=factors)

# Scale to actual ranges
ranges = {'Temperature': (300, 400),
          'Pressure': (1, 5),
          'Catalyst': (0.1, 1.0)}

for factor, (low, high) in ranges.items():
    df[factor] = df[factor] * (high - low)/2 + (high + low)/2

print(f"Generated {len(df)} experiments")
print(df.head())

# Export for lab work
df.to_csv('experimental_design.csv', index=False)
```

**Next steps:**
1. Run experiments and collect responses
2. Analyze with statsmodels or pycse
3. Optimize using fitted model

### Example 2: pycse Surface Response

**Situation:** Quick RSM with integrated analysis

```python
from pycse import design_sr, analyze_sr, sr_parity
import numpy as np

# Generate design
bounds = np.array([[300, 400],   # Temperature
                   [1, 5],        # Pressure
                   [0.1, 1.0]])   # Catalyst

design = design_sr(bounds,
                   inputs=['Temperature', 'Pressure', 'Catalyst'],
                   outputs=['Yield'])

print(design)

# After running experiments, add results
design['Yield'] = [78, 82, 75, 88, 91, 85, 79, ...]  # Your data

# Analyze
anova_table = analyze_sr(design)
print(anova_table)

# Parity plot (model fit)
sr_parity(design, show=True)
```

**Advantages:** Integrated workflow, automatic ANOVA, quick visualization

### Example 3: Bayesian Optimization (Sequential)

**Situation:** Expensive experiments, 4 factors, want to minimize total runs

```python
import numpy as np
from skopt import gp_minimize
from skopt.space import Real
from skopt.plots import plot_convergence

# Define parameter space
space = [
    Real(300, 400, name='Temperature'),
    Real(1, 5, name='Pressure'),
    Real(0.1, 1.0, name='Catalyst'),
    Real(10, 60, name='Time')
]

# Your black-box function (returns value to minimize)
def expensive_experiment(params):
    temp, pressure, catalyst, time = params

    # Run actual experiment here
    # For now, simulate
    yield_value = run_experiment(temp, pressure, catalyst, time)

    return -yield_value  # Minimize negative = maximize yield

# Bayesian optimization
result = gp_minimize(
    expensive_experiment,
    space,
    n_calls=30,          # Maximum experiments
    n_random_starts=5,   # Initial random exploration
    acq_func='EI',       # Expected Improvement
    random_state=42
)

print(f"Best parameters: {result.x}")
print(f"Best yield: {-result.fun:.2f}")

# Visualize convergence
plot_convergence(result)
```

**Workflow:**
1. Start with 5 random experiments (exploration)
2. Fit GP surrogate model
3. Suggest next experiment (maximize EI)
4. Run experiment, update model
5. Repeat until convergence

### Example 4: Model-Driven D-Optimal (Parameter Estimation)

**Situation:** Have kinetic model, need to estimate 3 rate constants

```python
import numpy as np
from scipy.optimize import minimize
from scipy.linalg import det

# Your mechanistic model
def model(t, k1, k2, k3):
    """Concentration vs time model"""
    return k1 * (1 - np.exp(-k2 * t)) + k3 * t

# Fisher Information Matrix for given design points
def fisher_information(t_design, k_guess):
    """Calculate FIM for design points t"""
    k1, k2, k3 = k_guess

    # Jacobian (sensitivity matrix)
    J = np.zeros((len(t_design), 3))

    for i, t in enumerate(t_design):
        J[i, 0] = 1 - np.exp(-k2 * t)
        J[i, 1] = k1 * t * np.exp(-k2 * t)
        J[i, 2] = t

    # FIM = J^T * J (assuming constant variance)
    FIM = J.T @ J
    return FIM

# D-optimal criterion: maximize determinant of FIM
def d_optimal_criterion(t_design, k_guess):
    FIM = fisher_information(t_design, k_guess)
    return -np.log(det(FIM))  # Maximize det = minimize -log(det)

# Find optimal design points
k_initial_guess = [1.0, 0.1, 0.05]
n_points = 6

result = minimize(
    lambda t: d_optimal_criterion(t, k_initial_guess),
    x0=np.linspace(0, 100, n_points),
    bounds=[(0, 100)] * n_points,
    method='L-BFGS-B'
)

optimal_times = result.x
print(f"Optimal measurement times: {optimal_times}")

# Run experiments at these times
# After getting data, update k_guess and re-optimize design if sequential
```

**Sequential workflow:**
1. Initial guess for parameters
2. Calculate D-optimal design
3. Run experiments
4. Fit model, update parameter estimates
5. Recalculate optimal design with new estimates
6. Repeat until parameters converge

### Example 5: Active Learning (Build Surrogate)

**Situation:** Want accurate surrogate model, can run sequentially

```python
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

# Initial random sample
X_initial = np.random.uniform([0, 0], [10, 10], size=(5, 2))
y_initial = [expensive_function(x) for x in X_initial]

# Create active learner with GP
regressor = GaussianProcessRegressor(kernel=RBF())
learner = ActiveLearner(
    estimator=regressor,
    query_strategy=uncertainty_sampling,  # Query where uncertain
    X_training=X_initial,
    y_training=y_initial
)

# Active learning loop
n_queries = 20
for i in range(n_queries):
    # Query next experiment (highest uncertainty)
    query_idx, query_instance = learner.query(X_candidate_pool)

    # Run experiment
    y_new = expensive_function(query_instance[0])

    # Update model
    learner.teach(query_instance, y_new)

    print(f"Iteration {i+1}: GP std = {learner.predict(query_instance)[1]:.4f}")

# Final model is in learner.estimator
```

## Workflow: How to Use This Skill

### Step 1: Answer Questions

When starting a DOE project, expect these questions:

1. What's your experimental goal?
2. Can you run sequentially or batch only?
3. How expensive are experiments?
4. How many factors?
5. Do you have a model or prior data?
6. Any constraints on factor combinations?
7. What's your total budget (# experiments)?

### Step 2: Get Recommendation

Based on answers, receive:
- **Recommended approach** with rationale
- **Alternative options** with trade-offs
- **Estimated number of runs** needed
- **Library/tool recommendations**
- **Example code** to get started

### Step 3: Generate Design

Use provided code to:
- Generate design matrix
- Visualize design in factor space
- Check design properties
- Export to CSV for lab work

### Step 4: Run Experiments

Execute experiments according to design:
- Randomize run order (unless sequential)
- Record all results
- Note any deviations or issues

### Step 5: Analyze Results

Analyze data with appropriate methods:
- **Classical**: ANOVA, regression, diagnostics
- **Bayesian**: Update GP, check convergence
- **Model-driven**: Fit model, assess parameters

### Step 6: Make Decisions

Based on analysis:
- **Classical**: Optimize response, validate
- **Bayesian**: Continue or stop? Next experiment?
- **Model-driven**: Parameters converged? Need more data?

## Detailed References

For deep dives into specific topics:

- **references/DESIGN_SELECTION_GUIDE.md** - Complete decision trees with questions
- **references/CLASSICAL_DOE.md** - Factorial, RSM, screening designs
- **references/BAYESIAN_OPTIMIZATION.md** - BO theory, acquisition functions, GP models
- **references/MODEL_DRIVEN_DOE.md** - Optimal designs, Fisher Information
- **references/ACTIVE_LEARNING.md** - Sequential strategies, uncertainty sampling
- **references/ANALYSIS_METHODS.md** - Statistical analysis, ANOVA, diagnostics
- **references/PYCSE_INTEGRATION.md** - Using pycse for RSM workflows

## Common Patterns by Industry

**Chemical Engineering:**
- Reactor optimization → CCD or Bayesian Optimization
- Catalyst screening → Fractional factorial → BO on hits
- Process development → Sequential model-driven DOE

**Materials Science:**
- Composition optimization → Mixture designs or BO
- Property mapping → Latin Hypercube + GP surrogate
- Alloy discovery → High-throughput with active learning

**Pharmaceutical:**
- Formulation → Mixture designs, response surface
- Dose optimization → Bayesian optimization
- PK/PD modeling → Model-driven D-optimal designs

**Manufacturing:**
- Process parameter tuning → CCD or Box-Behnken
- Quality improvement → Taguchi, robust parameter design
- Continuous improvement → Sequential BO

**Machine Learning:**
- Hyperparameter tuning → Bayesian optimization
- Architecture search → BO with discrete/categorical variables
- Neural network training → Adaptive sampling

## Installation

```bash
# Classical DOE
pip install pyDOE3          # Factorial, RSM, LHS
pip install dexpy           # Modern DOE library

# pycse for integrated RSM
pip install pycse

# Bayesian Optimization
pip install scikit-optimize # skopt - easiest to use
pip install GPyOpt          # Comprehensive BO
pip install ax-platform     # Meta's adaptive experimentation
# pip install botorch torch # Advanced BO (requires PyTorch)

# Active Learning
pip install modAL           # Active learning framework

# Gaussian Processes
pip install GPy             # GP models
# scikit-learn has GP built-in

# Analysis
pip install statsmodels     # ANOVA, regression
pip install scipy           # Optimization, statistics
pip install scikit-learn    # ML models, cross-validation

# Visualization
pip install matplotlib seaborn plotly

# Sensitivity Analysis
pip install SALib

# All in one
pip install pyDOE3 dexpy pycse scikit-optimize statsmodels scipy scikit-learn modAL GPy matplotlib seaborn
```

## Best Practices

### 1. Always Start with Questions

**❌ Don't:** "I'll use a Box-Behnken design"
**✅ Do:** "My goal is optimization, I have 3 factors, I can only batch, so Box-Behnken makes sense"

### 2. Match Method to Constraints

- **Budget limited** → Bayesian optimization
- **Time limited** → Batch classical DOE
- **Knowledge limited** → Space-filling exploration
- **Have model** → Model-driven optimal design

### 3. Sequential When Possible

Sequential designs (BO, active learning, model-driven):
- **Adapt** based on results
- **Stop early** if converged
- **Avoid wasted** experiments
- **Total runs** usually fewer than batch

**When batch is better:**
- Parallel equipment available
- Setup cost dominates run cost
- Well-understood system

### 4. Start Simple, Add Complexity

**Initial phase:** Simple screening (fractional factorial, LHS)
**Refinement:** Focus on important factors (RSM, BO)
**Validation:** Confirmation runs

### 5. Validate Assumptions

**Classical DOE assumes:**
- Normal residuals
- Constant variance
- Independent observations
- Linear/quadratic model adequate

**Check diagnostics:** Residual plots, Q-Q plots, lack-of-fit tests

**Bayesian optimization assumes:**
- GP model appropriate
- Kernel choice reasonable
- Enough initial exploration

**Check:** GP posterior uncertainty, kernel hyperparameters

### 6. Use Confirmation Experiments

After finding "optimum":
- Run additional experiments at predicted best settings
- Verify predictions match reality
- Account for prediction uncertainty

## Common Pitfalls

### 1. Using Wrong Design Type

**Problem:** Applying batch RSM to expensive experiments
**Solution:** Ask questions first, consider sequential approaches

### 2. Too Few Initial Points (BO)

**Problem:** GP needs diverse data to model landscape
**Solution:** Start with 5-10 space-filling points (LHS)

### 3. Ignoring Constraints

**Problem:** Design suggests infeasible experiments
**Solution:** Specify constraints upfront, use constrained optimization

### 4. Overfitting Surrogate Models

**Problem:** GP fits noise, poor predictions
**Solution:** Cross-validation, hold-out test sets, regularization

### 5. Not Randomizing Run Order

**Problem:** Systematic effects confounded with factors
**Solution:** Randomize execution order (except sequential designs)

### 6. Stopping Too Early (Sequential)

**Problem:** Declare convergence before truly converged
**Solution:** Use stopping criteria (EI < threshold, GP uncertainty low)

## Cost-Benefit Analysis: Sequential vs Batch

### When Sequential is Worth It:

**Expensive experiments:**
- Batch CCD: 20 runs at $1000 = $20,000
- Sequential BO: 15 runs (converge early) = $15,000
- **Savings: $5,000 + better result**

**Complex landscapes:**
- Batch might miss global optimum
- Sequential adapts to findings
- **Better final result**

### When Batch is Better:

**Cheap experiments:**
- Batch: 30 runs in 1 day (parallel)
- Sequential: 30 runs over 30 days
- **Time savings dominate**

**Setup costs:**
- If setup takes hours, run time minutes
- Batch amortizes setup
- **Sequential overhead too high**

## Response Format

When helping with DOE, Claude should:

1. **Ask questions first** - Don't assume method
2. **Explain recommendation** - Why this approach for their situation
3. **Provide complete code** - Working examples, not pseudocode
4. **Show alternatives** - "BO is best, but if you must batch, try..."
5. **Guide through workflow** - Design → Execute → Analyze → Optimize
6. **Interpret results** - Statistical significance AND practical significance

## Additional Resources

- **pyDOE3 Documentation:** https://pydoe3.readthedocs.io/
- **pycse Examples:** https://kitchingroup.cheme.cmu.edu/pycse/
- **scikit-optimize:** https://scikit-optimize.github.io/
- **GPyOpt:** https://sheffieldml.github.io/GPyOpt/
- **Bayesian Optimization Book:** https://bayesoptbook.com/
- **Design of Experiments (Montgomery):** Classic textbook
- **Statistics for Experimenters (Box, Hunter, Hunter):** Comprehensive reference

## Related Skills

- `python-optimization` - Response optimization after DOE
- `pycse` - Includes regression, ANOVA, confidence intervals for analysis
- `python-multiobjective-optimization` - Multi-response optimization
- `python-plotting` - Visualization of results and diagnostics

---

### elevenlabs

**Path**: `skills/creative/elevenlabs/SKILL.md`

# ElevenLabs Audio Generation

## Purpose

This skill enables AI-powered audio generation through ElevenLabs API. Create lifelike text-to-speech in 32 languages, generate custom sound effects for games and videos, and compose royalty-free music from text descriptions. Support for 100+ professional voices, custom voice cloning, real-time streaming, and multi-speaker dialogue.

## When to Use

This skill should be invoked when the user asks to:
- Generate speech from text ("convert this to speech", "create audio narration...")
- Create voiceovers for videos, presentations, or content
- Generate audio in specific voices or languages
- Create sound effects ("generate footstep sounds", "create explosion audio...")
- Compose music from descriptions ("generate upbeat background music...")
- Build multi-speaker dialogue or conversations
- Clone voices from audio samples
- Stream audio in real-time applications
- Create audiobooks, podcasts, or audio content

## Available Capabilities

### 1. Text-to-Speech (Voice Generation)

**Models:**
- **Eleven Multilingual v2** (`eleven_multilingual_v2`) - Highest quality, 29 languages
- **Eleven Flash v2.5** (`eleven_flash_v2_5`) - Ultra-low 75ms latency, 32 languages, 50% cheaper
- **Eleven Turbo v2.5** (`eleven_turbo_v2_5`) - Balanced quality and latency

**Features:**
- 100+ premade professional voices
- Custom voice cloning from audio samples
- Multi-speaker dialogue generation
- Real-time audio streaming
- 32 language support
- Emotional and natural intonation
- Voice settings customization (stability, similarity, style)

**Output Formats:**
- MP3 (various bitrates: 32kbps to 192kbps)
- PCM (8kHz to 48kHz)
- Opus, µ-law, A-law

### 2. Sound Effects Generation

**Model:**
- **Eleven Text-to-Sound v2** (`eleven_text_to_sound_v2`)

**Features:**
- Generate sound effects from text descriptions
- Customizable duration
- Looping support for seamless audio
- Prompt influence control
- High-quality audio for games, videos, UI/UX

**Use Cases:**
- Game audio (footsteps, explosions, ambient)
- Video production sounds
- UI/UX sound design
- Nature sounds (rain, wind, waves)
- Mechanical sounds (doors, engines, machines)
- Fantasy/sci-fi effects

### 3. Music Generation

**Features:**
- Text-to-music composition
- Vocal and instrumental tracks
- Multiple genres and styles
- Customizable track duration
- Composition plans (structured music blueprints)
- Royalty-free generated music

**Parameters:**
- Text prompts describing desired music
- Duration control (milliseconds)
- Genre, style, mood specifications
- Section-level composition control

**Requirements:**
- Paid ElevenLabs account (music API not available on free tier)

**Content Policy:**
- No copyrighted material (artist names, band names, trademarks)
- Returns suggestions for restricted prompts

## Instructions

### Step 1: Understand the Request

Analyze the user's request to determine:
- **Task Type**: Text-to-speech, sound effects, or music generation
- **Content**: What text/description to convert
- **Voice/Sound**: Specific voice, language, or sound characteristics
- **Format**: Output format requirements (MP3, streaming, etc.)
- **Duration**: Length requirements (for sound effects or music)
- **Use Case**: Narration, video, game, podcast, etc.

### Step 2: Select Appropriate Model/Capability

**For Text-to-Speech:**
- **High quality needed** → `eleven_multilingual_v2`
- **Low latency/real-time** → `eleven_flash_v2_5`
- **Balanced** → `eleven_turbo_v2_5`

**For Sound Effects:**
- Use `eleven_text_to_sound_v2` model
- Consider duration and looping needs

**For Music:**
- Ensure user has paid account
- Determine track length and style

### Step 3: Set Up API Authentication

```python
import os
from elevenlabs.client import ElevenLabs

# Initialize client with API key
client = ElevenLabs(api_key=os.environ.get("ELEVENLABS_API_KEY"))
```

API key should be set as environment variable:
```bash
export ELEVENLABS_API_KEY="your-api-key-here"
```

### Step 4: Implement Based on Task Type

#### Text-to-Speech Implementation

**Basic Speech Generation:**
```python
from elevenlabs.client import ElevenLabs
from pathlib import Path

client = ElevenLabs(api_key=os.environ["ELEVENLABS_API_KEY"])

# Generate speech
audio = client.text_to_speech.convert(
    text="Your text content here",
    voice_id="JBFqnCBsd6RMkjVDRZzb",  # Default voice (George)
    model_id="eleven_multilingual_v2",
    output_format="mp3_44100_128"
)

# Save to file
output_path = Path("speech_output.mp3")
with output_path.open("wb") as f:
    for chunk in audio:
        f.write(chunk)

print(f"Audio saved to: {output_path}")
```

**Streaming Speech (Real-time):**
```python
from elevenlabs.client import ElevenLabs
from elevenlabs import stream

client = ElevenLabs(api_key=os.environ["ELEVENLABS_API_KEY"])

# Stream audio in real-time
audio_stream = client.text_to_speech.convert_as_stream(
    text="This will be streamed as it generates",
    voice_id="JBFqnCBsd6RMkjVDRZzb",
    model_id="eleven_flash_v2_5",  # Low latency model for streaming
    output_format="mp3_44100_128"
)

# Stream to speakers
stream(audio_stream)
```

**Multi-Speaker Dialogue:**
```python
# Generate conversation with multiple voices
speakers = [
    {
        "voice_id": "JBFqnCBsd6RMkjVDRZzb",  # Speaker 1
        "text": "Hello, how are you today?"
    },
    {
        "voice_id": "21m00Tcm4TlvDq8ikWAM",  # Speaker 2 (Rachel)
        "text": "I'm doing great, thanks for asking!"
    }
]

# Generate each speaker's audio and combine
from pydub import AudioSegment
combined = AudioSegment.empty()

for speaker in speakers:
    audio = client.text_to_speech.convert(
        text=speaker["text"],
        voice_id=speaker["voice_id"],
        model_id="eleven_multilingual_v2"
    )

    # Save temp file
    temp_path = Path(f"temp_{speaker['voice_id']}.mp3")
    with temp_path.open("wb") as f:
        for chunk in audio:
            f.write(chunk)

    # Add to combined audio
    segment = AudioSegment.from_mp3(str(temp_path))
    combined += segment
    temp_path.unlink()  # Clean up

# Export final dialogue
combined.export("dialogue.mp3", format="mp3")
```

**List Available Voices:**
```python
# Get all available voices
voices = client.voices.get_all()

print("Available voices:")
for voice in voices.voices:
    print(f"- {voice.name} (ID: {voice.voice_id})")
    print(f"  Labels: {voice.labels}")
    print(f"  Description: {voice.description}")
```

**Common Voice IDs:**
- `JBFqnCBsd6RMkjVDRZzb` - George (male, English, middle-aged)
- `21m00Tcm4TlvDq8ikWAM` - Rachel (female, English, young)
- `AZnzlk1XvdvUeBnXmlld` - Domi (female, English, young)
- `EXAVITQu4vr4xnSDxMaL` - Bella (female, English, young)
- `ErXwobaYiN019PkySvjV` - Antoni (male, English, young)
- `MF3mGyEYCl7XYWbV9V6O` - Elli (female, English, young)
- `TxGEqnHWrfWFTfGW9XjX` - Josh (male, English, young)

#### Sound Effects Implementation

**Basic Sound Effect Generation:**
```python
from elevenlabs.client import ElevenLabs
from pathlib import Path

client = ElevenLabs(api_key=os.environ["ELEVENLABS_API_KEY"])

# Generate sound effect
audio = client.text_to_sound_effects.convert(
    text="footsteps on wooden floor, slow paced walking",
    duration_seconds=5.0,
    prompt_influence=0.5  # How closely to follow prompt (0.0-1.0)
)

# Save to file
output_path = Path("footsteps.mp3")
with output_path.open("wb") as f:
    for chunk in audio:
        f.write(chunk)

print(f"Sound effect saved to: {output_path}")
```

**Looping Sound Effect:**
```python
# Generate seamlessly looping audio
audio = client.text_to_sound_effects.convert(
    text="gentle rain falling on leaves, ambient nature sound",
    duration_seconds=10.0,
    prompt_influence=0.5
    # Note: loop parameter may be available in newer API versions
)

output_path = Path("rain_loop.mp3")
with output_path.open("wb") as f:
    for chunk in audio:
        f.write(chunk)
```

**Multiple Sound Effects:**
```python
# Generate various sound effects for a game
sound_effects = [
    {
        "name": "explosion",
        "description": "large explosion, debris falling, action movie style",
        "duration": 3.0
    },
    {
        "name": "door_open",
        "description": "creaky wooden door slowly opening, horror atmosphere",
        "duration": 2.0
    },
    {
        "name": "ui_click",
        "description": "soft button click, UI feedback sound, pleasant tone",
        "duration": 0.5
    }
]

for sfx in sound_effects:
    audio = client.text_to_sound_effects.convert(
        text=sfx["description"],
        duration_seconds=sfx["duration"]
    )

    output_path = Path(f"{sfx['name']}.mp3")
    with output_path.open("wb") as f:
        for chunk in audio:
            f.write(chunk)

    print(f"Generated: {output_path}")
```

#### Music Generation Implementation

**Basic Music Composition:**
```python
from elevenlabs.client import ElevenLabs
from pathlib import Path

client = ElevenLabs(api_key=os.environ["ELEVENLABS_API_KEY"])

# Generate music from prompt
prompt = """Upbeat indie pop song with acoustic guitar, light drums, and cheerful
melody. Modern and energetic feel, perfect for background music in a lifestyle video.
Instrumental only, no vocals."""

try:
    audio = client.music_generation.compose(
        prompt=prompt,
        music_length_ms=30000  # 30 seconds
    )

    # Save music file
    output_path = Path("background_music.mp3")
    with output_path.open("wb") as f:
        for chunk in audio:
            f.write(chunk)

    print(f"Music saved to: {output_path}")

except Exception as e:
    if "paid" in str(e).lower() or "subscription" in str(e).lower():
        print("Error: Music generation requires a paid ElevenLabs account")
    else:
        print(f"Error: {e}")
```

**Music with Composition Plan:**
```python
# Create structured composition plan first
composition_plan = client.music_generation.composition_plan.create(
    prompt="""Electronic dance music track with energetic build-up, drop section,
    and chill outro. Progressive house style.""",
    music_length_ms=60000  # 60 seconds
)

# Generate music from plan (allows for more control)
audio = client.music_generation.compose(
    composition_plan=composition_plan
)

output_path = Path("edm_track.mp3")
with output_path.open("wb") as f:
    for chunk in audio:
        f.write(chunk)
```

**Genre-Specific Music:**
```python
# Generate music for different genres/moods
music_prompts = {
    "cinematic": """Epic cinematic orchestral music with dramatic strings, powerful
    brass, and heroic theme. Perfect for movie trailer, inspiring and grand.""",

    "lo-fi": """Chill lo-fi hip hop beats with jazz piano, vinyl crackle, and mellow
    drums. Relaxing study music atmosphere, instrumental.""",

    "ambient": """Ambient soundscape with ethereal pads, subtle textures, and peaceful
    atmosphere. Meditative and calming, perfect for relaxation.""",

    "game_menu": """Mysterious fantasy game menu music with harp, soft strings, and
    magical atmosphere. Medieval RPG feel, looping background music."""
}

for name, prompt in music_prompts.items():
    try:
        audio = client.music_generation.compose(
            prompt=prompt,
            music_length_ms=20000  # 20 seconds
        )

        output_path = Path(f"music_{name}.mp3")
        with output_path.open("wb") as f:
            for chunk in audio:
                f.write(chunk)

        print(f"Generated: {output_path}")

    except Exception as e:
        print(f"Error generating {name}: {e}")
```

### Step 5: Handle Output and Errors

**Save Audio Files:**
```python
from pathlib import Path

def save_audio(audio_generator, filename):
    """Save audio generator to file"""
    output_path = Path(filename)

    with output_path.open("wb") as f:
        for chunk in audio_generator:
            f.write(chunk)

    print(f"Saved: {output_path.absolute()}")
    return output_path
```

**Error Handling:**
```python
import os
from elevenlabs.client import ElevenLabs

def check_api_key():
    """Verify API key is set"""
    if not os.environ.get("ELEVENLABS_API_KEY"):
        raise ValueError(
            "ELEVENLABS_API_KEY not set. "
            "Please set environment variable: export ELEVENLABS_API_KEY='your-key'"
        )

def handle_elevenlabs_request(func, *args, **kwargs):
    """Wrapper for error handling"""
    try:
        return func(*args, **kwargs)

    except Exception as e:
        error_msg = str(e).lower()

        if "api key" in error_msg or "authentication" in error_msg:
            print("Error: Invalid or missing API key")
            print("Set your API key: export ELEVENLABS_API_KEY='your-key'")

        elif "quota" in error_msg or "limit" in error_msg:
            print("Error: API quota exceeded")
            print("Check your usage at https://elevenlabs.io/app/usage")

        elif "paid" in error_msg or "subscription" in error_msg:
            print("Error: This feature requires a paid subscription")

        elif "bad_prompt" in error_msg:
            print("Error: Prompt contains restricted content")
            print("Avoid copyrighted material (artist names, brands)")

        else:
            print(f"Error: {e}")

        raise
```

### Step 6: Provide Output to User

1. **Report what was generated**
2. **Show file path** where audio was saved
3. **Provide playback options** if appropriate
4. **Offer refinements** (different voice, longer duration, etc.)
5. **Display metadata** (duration, format, model used)

## Requirements

**API Key:**
- ElevenLabs API key (get from https://elevenlabs.io/app/settings/api-keys)
- Set as environment variable: `ELEVENLABS_API_KEY`

**Python Packages:**
```bash
pip install elevenlabs pydub python-dotenv
```

**System:**
- Python 3.8+
- Internet connection for API access
- Audio playback library (optional, for playing generated audio)
- ffmpeg (required by pydub for audio processing)

**Account Requirements:**
- Free tier: Text-to-speech and sound effects
- Paid tier: Music generation, higher quotas

## Best Practices

### Text-to-Speech

1. **Choose Appropriate Model:**
   - High quality narration → `eleven_multilingual_v2`
   - Real-time/streaming → `eleven_flash_v2_5`
   - Balanced use cases → `eleven_turbo_v2_5`

2. **Select Right Voice:**
   - Match voice to content (age, gender, accent)
   - Use `voices.get_all()` to explore options
   - Consider voice labels and descriptions

3. **Optimize for Use Case:**
   - Long content: Use standard conversion, not streaming
   - Real-time apps: Use Flash model with streaming
   - Dialogue: Generate separate audio per speaker

4. **Format Selection:**
   - Web/mobile: MP3 (good quality, small size)
   - High quality: Use higher bitrate (128kbps+)
   - Phone systems: µ-law or A-law format

### Sound Effects Generation

1. **Be Descriptive:**
   - Include context: "footsteps on gravel, slow walking pace"
   - Specify mood: "creepy door creak, horror atmosphere"
   - Add technical details: "deep bass explosion, action movie"

2. **Duration Control:**
   - Short sounds: 0.5-2 seconds (UI clicks, impacts)
   - Medium sounds: 2-5 seconds (footsteps, doors)
   - Ambient loops: 5-10+ seconds (rain, wind, environments)

3. **Prompt Influence:**
   - High (0.7-1.0): Follow prompt closely, more literal
   - Medium (0.4-0.6): Balanced creativity and adherence
   - Low (0.0-0.3): More creative interpretation

4. **Iteration:**
   - Generate multiple variations
   - Adjust descriptions based on results
   - Combine multiple effects if needed

### Music Generation

1. **Detailed Prompts:**
   - Specify genre, instruments, mood, tempo
   - Mention structure (intro, build-up, drop, outro)
   - Include use case context (game menu, video background)

2. **Avoid Copyrighted References:**
   - Don't mention artist names, band names, songs
   - Use generic style descriptions instead
   - Focus on characteristics, not examples

3. **Duration Planning:**
   - Short clips: 10-30 seconds (loops, backgrounds)
   - Full tracks: 60-120 seconds (complete songs)
   - Consider export time (longer = more processing)

4. **Composition Plans:**
   - Use for complex multi-section tracks
   - Better control over structure
   - Allows section-level customization

### General Best Practices

1. **API Key Security:**
   - Store in environment variables, never in code
   - Use `.env` files for local development
   - Rotate keys periodically

2. **Error Handling:**
   - Always wrap API calls in try/except
   - Check for quota limits
   - Provide helpful error messages

3. **Cost Optimization:**
   - Use Flash model when quality difference is minimal
   - Cache/reuse generated audio when possible
   - Monitor usage via dashboard

4. **File Management:**
   - Use descriptive filenames
   - Organize by type (speech, sfx, music)
   - Clean up temporary files

5. **Testing:**
   - Test with short durations first
   - Verify output quality before long generations
   - Check different voices/settings

## Examples

### Example 1: Audiobook Narration

**User request:** "Convert this chapter to audiobook format"

**Expected behavior:**
1. Select appropriate voice (e.g., narrative voice like George)
2. Use high-quality model (`eleven_multilingual_v2`)
3. Generate speech from chapter text
4. Save as MP3 with high bitrate
5. Report duration and file location

```python
audio = client.text_to_speech.convert(
    text=chapter_text,
    voice_id="JBFqnCBsd6RMkjVDRZzb",  # George
    model_id="eleven_multilingual_v2",
    output_format="mp3_44100_128"
)
save_audio(audio, "chapter_1.mp3")
```

### Example 2: Video Game Sound Effects

**User request:** "Generate sound effects for a fantasy RPG game"

**Expected behavior:**
1. Create multiple sound effects with descriptions
2. Set appropriate durations for each
3. Save with descriptive names
4. Organize in game audio folder

```python
sfx_list = [
    ("sword_swing", "sword whooshing through air, fantasy combat", 1.0),
    ("potion_drink", "drinking magical potion, gulp sound, RPG game", 0.8),
    ("spell_cast", "magical spell casting, ethereal whoosh, fantasy magic", 1.5),
    ("footsteps_stone", "footsteps on stone dungeon floor, echoing", 2.0)
]

for name, description, duration in sfx_list:
    audio = client.text_to_sound_effects.convert(
        text=description,
        duration_seconds=duration
    )
    save_audio(audio, f"sfx_{name}.mp3")
```

### Example 3: Podcast Intro with Music

**User request:** "Create a podcast intro with voice and background music"

**Expected behavior:**
1. Generate intro speech
2. Generate background music
3. Note that mixing would need external tools (pydub)
4. Provide both audio files

```python
# Generate intro speech
intro_text = "Welcome to the Tech Talk podcast, where we discuss the latest in technology and innovation."
speech = client.text_to_speech.convert(
    text=intro_text,
    voice_id="TxGEqnHWrfWFTfGW9XjX",  # Josh (energetic)
    model_id="eleven_flash_v2_5"
)
save_audio(speech, "podcast_intro_voice.mp3")

# Generate background music (requires paid account)
music = client.music_generation.compose(
    prompt="Upbeat tech podcast intro music, electronic beats, modern and energetic",
    music_length_ms=10000  # 10 seconds
)
save_audio(music, "podcast_intro_music.mp3")

print("Use audio editing software to mix voice and music")
```

### Example 4: Multilingual Content

**User request:** "Create welcome messages in English, Spanish, and French"

**Expected behavior:**
1. Generate speech in each language
2. Use multilingual model
3. Select appropriate voices for each language
4. Save with language-specific filenames

```python
messages = {
    "english": ("Hello and welcome!", "JBFqnCBsd6RMkjVDRZzb"),
    "spanish": ("¡Hola y bienvenido!", "ThT5KcBeYPX3keUQqHPh"),  # Spanish voice
    "french": ("Bonjour et bienvenue!", "XB0fDUnXU5powFXDhCwa")   # French voice
}

for lang, (text, voice_id) in messages.items():
    audio = client.text_to_speech.convert(
        text=text,
        voice_id=voice_id,
        model_id="eleven_multilingual_v2"
    )
    save_audio(audio, f"welcome_{lang}.mp3")
```

### Example 5: Real-time Voice Streaming

**User request:** "Stream this news article as audio"

**Expected behavior:**
1. Use Flash model for low latency
2. Stream audio as it generates
3. Provide real-time playback or save incrementally

```python
from elevenlabs import stream

audio_stream = client.text_to_speech.convert_as_stream(
    text=news_article_text,
    voice_id="21m00Tcm4TlvDq8ikWAM",  # Rachel
    model_id="eleven_flash_v2_5",
    output_format="mp3_44100_128"
)

# Stream to speakers in real-time
stream(audio_stream)
```

## Limitations

1. **Music Generation:**
   - Requires paid subscription
   - No copyrighted material allowed
   - Processing time increases with duration

2. **API Quotas:**
   - Character limits per month (tier-dependent)
   - Rate limits on requests
   - Different limits for free vs paid tiers

3. **Voice Cloning:**
   - Not covered in Tier 1 implementation
   - Requires voice samples and additional setup

4. **Audio Quality:**
   - Output format affects quality and file size
   - Higher quality formats may require paid tier
   - Streaming has slightly lower quality than standard

5. **Language Support:**
   - 32 languages supported but quality varies
   - Some voices are language-specific
   - Multilingual model recommended for non-English

6. **Sound Effects:**
   - Limited to description-based generation
   - No editing of generated effects via API
   - Duration limitations (typically under 22 seconds)

7. **Content Policy:**
   - No harmful or copyrighted content
   - Music generation rejects artist/band names
   - Strict content moderation on all endpoints

## Related Skills

- `image-generation` - For visual content creation
- `python-plotting` - For visualizing audio data
- `scientific-writing` - For generating narration text
- `python-best-practices` - For writing clean audio processing code

## Additional Resources

- **ElevenLabs Documentation**: https://elevenlabs.io/docs
- **Python SDK**: https://github.com/elevenlabs/elevenlabs-python
- **API Reference**: https://elevenlabs.io/docs/api-reference/introduction
- **Voice Library**: https://elevenlabs.io/voice-library
- **Pricing**: https://elevenlabs.io/pricing
- **Usage Dashboard**: https://elevenlabs.io/app/usage

---

### eln

**Path**: `skills/research/eln/SKILL.md`

# Electronic Lab Notebook (ELN) Skill

This skill provides expert guidance for maintaining professional electronic lab notebooks using org-mode, following best practices for scientific documentation, reproducibility, and research integrity.

## When to Use This Skill

Use this skill when:
- Creating or updating lab notebook entries
- Documenting experiments, calculations, or analyses
- Recording hypotheses and experimental design
- Documenting computational workflows
- Summarizing results and drawing conclusions
- Organizing research notes chronologically
- Creating cross-references between related work
- Preparing documentation for publications or reports
- Ensuring reproducibility of scientific work
- Maintaining professional research records

## Core Principles of Scientific Documentation

### 1. Chronological Organization
Entries are organized by date in a hierarchical structure:
```
notebook/
├── 2025/
│   ├── 01-January/
│   │   ├── 2025-01-15.org
│   │   └── 2025-01-20.org
│   └── 02-February/
│       └── 2025-02-03.org
```

### 2. Complete Documentation
Each entry should contain:
- **What was done**: Detailed description of work
- **Why it was done**: Hypothesis, motivation, reasoning
- **How it was done**: Methods, procedures, parameters
- **What was observed**: Results, data, observations
- **What it means**: Analysis, interpretation, conclusions
- **Next steps**: Follow-up work, questions raised

### 3. Reproducibility
Documentation should enable anyone (including future you) to:
- Understand the reasoning behind decisions
- Reproduce the work exactly
- Access all data and code used
- Follow the chain of logic

### 4. Professional Standards
- Write clearly and professionally
- Be honest about failures and unexpected results
- Date and timestamp all entries
- Never delete or modify past entries (make corrections with new entries)
- Include sufficient detail for independent reproduction

## Org-Mode ELN Structure

### File Organization

#### Date-Based Files
Each day's work goes in a dated file:
```
2025-01-15.org    # Single day's work
```

#### Directory Structure
```
research-notebook/
├── 2025/
│   ├── 01-January/
│   │   ├── 2025-01-15.org
│   │   ├── 2025-01-16.org
│   │   └── data/
│   │       └── 2025-01-15/
│   │           ├── experiment_results.csv
│   │           └── analysis.png
│   └── 02-February/
├── templates/
│   ├── experiment-template.org
│   └── calculation-template.org
├── index.org          # Master index
└── README.org         # Project overview
```

### Entry Template Structure

```org
#+TITLE: Lab Notebook - 2025-01-15
#+AUTHOR: Your Name
#+DATE: [2025-01-15 Wed]
#+FILETAGS: :experiment:catalyst:
#+STARTUP: overview

* Daily Summary
Brief overview of what was accomplished today.

* Entry 1: Hypothesis Testing - CO Adsorption on Pt
:PROPERTIES:
:ID: 2025-01-15-001
:PROJECT: Catalyst_Screening
:STATUS: In_Progress
:RELATED: [[file:2025-01-10.org::*Previous Results]]
:END:

** Objective
What are you trying to accomplish and why?

** Hypothesis
Clear statement of what you expect and the reasoning.

** Background/Context
- Why is this important?
- What previous work led to this?
- What are the key questions?

** Methods
*** Computational Setup
Detailed description of how the work was performed.

#+BEGIN_SRC python
# Include actual code used
from ase.build import fcc111
slab = fcc111('Pt', size=(4,4,4), vacuum=10.0)
#+END_SRC

*** Parameters
- DFT functional: PBE
- Energy cutoff: 400 eV
- k-points: 4×4×1
- Convergence: 0.05 eV/Å

** Results
*** Observations
What actually happened? Include data, figures, output.

#+BEGIN_SRC python :results file
# Analysis code
import matplotlib.pyplot as plt
# ... plotting code ...
plt.savefig('data/2025-01-15/adsorption_energy.png')
return 'data/2025-01-15/adsorption_energy.png'
#+END_SRC

#+RESULTS:
[[file:data/2025-01-15/adsorption_energy.png]]

*** Data
| Metal | E_ads (eV) | Site  |
|-------+------------+-------|
| Pt    |     -1.82  | fcc   |
| Cu    |     -0.95  | hcp   |
| Au    |     -0.45  | ontop |

** Analysis
*** Interpretation
What do these results mean?

*** Comparison with Literature
How do results compare with expected values?

*** Unexpected Findings
Anything surprising or anomalous?

** Conclusions
- Key findings
- Hypothesis supported/rejected
- Confidence level in results

** Issues/Problems
Document any problems encountered and how they were resolved.

** Next Steps
- [ ] Validate with different functional
- [ ] Test larger surface models
- [ ] Calculate reaction barriers

** References
- Previous work: [[file:2025-01-10.org::*Initial Screening]]
- Literature: Smith et al. (2024) DOI:10.1021/xxxxx
- Code: [[file:~/projects/catalyst-screening/run_calculations.py]]

* Entry 2: Another Task
...
```

## Essential Org-Mode Features for ELN

### 1. Properties Drawer
```org
:PROPERTIES:
:ID: unique-identifier
:PROJECT: Project_Name
:STATUS: Planning|In_Progress|Complete|On_Hold
:STARTED: [2025-01-15 Wed]
:COMPLETED: [2025-01-15 Wed]
:RELATED: [[link-to-related-entry]]
:DATA: [[file:./data/2025-01-15/results.csv]]
:END:
```

### 2. Tags
Use tags for categorization and filtering:
```org
#+FILETAGS: :experiment:simulation:catalyst:DFT:
* Entry Title                                              :important:urgent:
```

Common tag categories:
- **Type**: :experiment:, :simulation:, :analysis:, :literature:, :meeting:
- **Topic**: :catalyst:, :materials:, :synthesis:, :characterization:
- **Status**: :todo:, :in_progress:, :done:, :failed:
- **Priority**: :urgent:, :important:, :routine:

### 3. Links
```org
# Internal links
[[file:2025-01-10.org::*Previous Results]]
[[id:2025-01-15-001]]

# External links
[[file:~/data/experiment_001.csv][Data file]]
[[https://doi.org/10.1021/xxxxx][Smith et al. 2024]]

# Code links
[[file:~/projects/analysis/plot_results.py]]
```

### 4. Code Blocks
```org
#+BEGIN_SRC python :results output :session analysis :exports both
import pandas as pd
import matplotlib.pyplot as plt

# Reproducible analysis code
data = pd.read_csv('data/2025-01-15/results.csv')
print(f"Mean energy: {data['energy'].mean():.3f} eV")
#+END_SRC

#+RESULTS:
: Mean energy: -1.234 eV
```

### 5. Tables
```org
| Parameter      | Value     | Units | Notes              |
|----------------+-----------+-------+--------------------|
| Temperature    | 300       | K     | Room temperature   |
| Pressure       | 1         | atm   | Standard           |
| Coverage       | 0.25      | ML    | 1/4 monolayer      |
#+TBLFM: @2$4=Calculated from geometry
```

### 6. TODO Items and Checkboxes
```org
** Next Steps
- [ ] Run convergence test for k-points
- [ ] Compare with experimental data from Lee et al.
- [X] Calculate adsorption energy
- [ ] Write up results for group meeting

** TODO Validate results with different functional
DEADLINE: <2025-01-20 Fri>
:PROPERTIES:
:EFFORT: 2h
:END:
```

### 7. Timestamps
```org
# Active timestamps (appear in agenda)
* Meeting with advisor
<2025-01-20 Fri>

# Inactive timestamps (documentation only)
Calculation started [2025-01-15 Wed]

# Date ranges
Project duration: <2025-01-10 Mon>--<2025-01-30 Mon>
```

## Common Entry Types

### 1. Experimental/Computational Work

```org
* DFT Calculation - Surface Relaxation
:PROPERTIES:
:ID: 2025-01-15-001
:PROJECT: Catalyst_Screening
:TYPE: Calculation
:STATUS: Complete
:END:

** Objective
Optimize the geometry of clean Pt(111) surface.

** Methods
- Calculator: VASP
- Functional: PBE
- ENCUT: 400 eV
- k-points: 6×6×1 Monkhorst-Pack
- Convergence: Forces < 0.05 eV/Å

#+BEGIN_SRC bash
# Command used
vasp_std > vasp.out
#+END_SRC

** Results
- Final energy: -123.45 eV
- Relaxation time: 2.5 hours
- Converged in 45 ionic steps

[[file:./data/2025-01-15/surface_relaxation.png]]

** Analysis
Surface atoms relaxed inward by 0.03 Å (0.8% contraction).
This agrees with literature values (Smith 2023: 0.02-0.04 Å).

** Conclusion
Surface structure validated. Ready for adsorbate calculations.

** Next Steps
- [ ] Add CO adsorbate at FCC site
- [ ] Calculate adsorption energy
```

### 2. Literature Review Entry

```org
* Literature Review - CO Oxidation Mechanisms
:PROPERTIES:
:ID: 2025-01-15-002
:TOPIC: Catalysis
:STATUS: Complete
:END:

** Paper
Smith, J. et al. (2024) "CO Oxidation on Platinum Surfaces"
J. Phys. Chem. C, DOI: 10.1021/xxxxx

** Key Findings
- CO adsorption energy on Pt(111): -1.5 to -1.8 eV (experimental)
- Preferred site: FCC hollow
- Barrier for CO → CO₂: 0.8 eV

** Relevance to Our Work
Their experimental values provide validation targets for our DFT calculations.
Our predicted -1.82 eV is in excellent agreement.

** Questions Raised
- How does coverage affect binding energy?
- What about stepped surfaces?

** References to Follow Up
- [ ] Lee et al. (2023) - Coverage effects
- [ ] Zhang et al. (2024) - Step sites
```

### 3. Data Analysis Entry

```org
* Analysis - Comparing Different Functionals
:PROPERTIES:
:ID: 2025-01-15-003
:PROJECT: Method_Validation
:STATUS: Complete
:END:

** Objective
Compare PBE, PBE+D3, and RPBE for CO adsorption energies.

** Data Sources
- PBE: [[file:2025-01-10.org::*PBE Results]]
- PBE+D3: [[file:2025-01-12.org::*Dispersion Results]]
- RPBE: [[file:2025-01-14.org::*RPBE Calculations]]

** Analysis Code
#+BEGIN_SRC python :results file
import pandas as pd
import matplotlib.pyplot as plt

# Compile results
data = {
    'Functional': ['PBE', 'PBE+D3', 'RPBE'],
    'E_ads': [-1.82, -2.15, -1.45],
    'Experiment': [-1.6, -1.6, -1.6]
}
df = pd.DataFrame(data)

# Plot
plt.figure(figsize=(8, 5))
plt.bar(df['Functional'], df['E_ads'], alpha=0.7, label='Calculated')
plt.axhline(y=-1.6, color='r', linestyle='--', label='Experiment')
plt.ylabel('Adsorption Energy (eV)')
plt.legend()
plt.tight_layout()
plt.savefig('data/2025-01-15/functional_comparison.png', dpi=300)
return 'data/2025-01-15/functional_comparison.png'
#+END_SRC

#+RESULTS:
[[file:data/2025-01-15/functional_comparison.png]]

** Findings
| Functional | E_ads (eV) | Error vs Expt (eV) |
|------------+------------+--------------------|
| PBE        |     -1.82  |              -0.22 |
| PBE+D3     |     -2.15  |              -0.55 |
| RPBE       |     -1.45  |               0.15 |
#+TBLFM: $3=$2-(-1.6);%.2f

** Conclusions
- PBE gives reasonable agreement (within 0.22 eV)
- PBE+D3 overbinds significantly
- RPBE underestimates binding
- **Decision**: Use PBE for production calculations

** Impact on Previous Work
Need to note that PBE tends to overbind slightly when reporting results.
```

### 4. Meeting Notes

```org
* Group Meeting - Project Discussion
:PROPERTIES:
:ID: 2025-01-15-004
:TYPE: Meeting
:ATTENDEES: Prof. Smith, Jane Doe, John Smith
:END:

** Agenda
1. Progress updates
2. Results discussion
3. Next steps

** My Update
Presented CO adsorption results on Pt(111):
- E_ads = -1.82 eV (PBE)
- Good agreement with experiment (-1.6 eV)

** Feedback
- Prof. Smith: Test with larger surface to check size effects
- Jane: Compare with her experimental values (coming next week)
- Need to include dispersion for comparison

** Action Items
- [ ] Run calculations with 6×6×4 slab
- [ ] Prepare summary for Jane's experimental comparison
- [ ] Read paper Prof. Smith suggested: Zhang et al. 2024

** Next Meeting
<2025-01-22 Wed 10:00>
```

### 5. Problem/Debugging Entry

```org
* Troubleshooting - VASP SCF Convergence Issues
:PROPERTIES:
:ID: 2025-01-15-005
:TYPE: Problem
:STATUS: Resolved
:END:

** Problem
VASP calculation not converging after 100 SCF steps.

Error message:
#+BEGIN_EXAMPLE
WARNING: Sub-Space-Matrix is not hermitian in DAV
#+END_EXAMPLE

** Attempted Solutions
1. Increased NELM to 200 - Still failed
2. Changed ALGO from Fast to Normal - No improvement
3. Reduced SIGMA from 0.2 to 0.1 - Failed
4. **Solution**: Used ISMEAR=0 (Gaussian smearing) instead of ISMEAR=1

** Root Cause
System has a band gap. Methfessel-Paxton smearing (ISMEAR=1) is inappropriate.
Gaussian smearing (ISMEAR=0) is better for molecules and systems with gaps.

** Resolution
Changed INCAR:
#+BEGIN_EXAMPLE
ISMEAR = 0
SIGMA = 0.05
#+END_EXAMPLE

Calculation converged in 45 SCF steps.

** Lessons Learned
- Always check if system has a gap before choosing smearing
- For molecules/insulators: Use ISMEAR=0 or ISMEAR=-5
- For metals: ISMEAR=1 or 2 is appropriate

** References
- VASP Wiki: [[https://www.vasp.at/wiki/index.php/ISMEAR]]
```

## Best Practices

### 1. Daily Summaries
Start each file with a brief summary:
```org
* Daily Summary
Today focused on validating DFT functional choice for catalyst project.
Compared PBE, PBE+D3, and RPBE against experimental data. Concluded
that PBE gives best agreement. Started large-scale screening of FCC
metals.

Key accomplishment: Validated methodology for production runs.
Main challenge: VASP convergence issues with molecular systems (resolved).
```

### 2. Honest Documentation
```org
** Results
Expected: E_ads ≈ -1.5 eV
**Observed: E_ads = -0.45 eV**

This is significantly different from literature values.

** Analysis of Discrepancy
Possible causes:
1. Wrong adsorption site? → Checked: FCC is correct
2. Insufficient k-points? → Tested 8×8×1: -0.46 eV (no change)
3. Functional issue? → Literature used PBE, we used PBE
4. **Surface coverage?** → Our 1/16 ML vs literature 1/4 ML

** Resolution
Rerun with higher coverage to match literature conditions.
Original calculation still valid for low-coverage limit.
```

### 3. Version Control Integration
```org
** Computational Details
Code version: catalyst-tools v2.3.1
Git commit: a3f5e2b

#+BEGIN_SRC bash
git log -1 --oneline
#+END_SRC

#+RESULTS:
: a3f5e2b Fix convergence criteria in optimizer
```

### 4. Data Management
```org
** Data Files
- Raw output: [[file:./data/2025-01-15/OUTCAR]]
- Processed data: [[file:./data/2025-01-15/results.csv]]
- Analysis notebook: [[file:./data/2025-01-15/analysis.ipynb]]
- Archived: ~/archives/2025/january/calculation_001.tar.gz

MD5 checksums:
- OUTCAR: 5d41402abc4b2a76b9719d911017c592
- results.csv: 7d793037a0760186574b0282f2f435e7
```

### 5. Cross-Referencing
```org
** Related Work
- Background: [[file:2025-01-05.org::*Literature Review]]
- Previous attempt: [[file:2025-01-12.org::*Failed Calculation]] (explains why approach changed)
- Follow-up: [[file:2025-01-18.org::*Extended Analysis]]
- Project overview: [[file:../index.org::*Catalyst Screening Project]]
```

### 6. Reproducibility Checklist
Every computational entry should enable reproduction:
```org
** Reproducibility Information
- [X] Software versions documented
- [X] Input files included or linked
- [X] Random seeds specified (if applicable)
- [X] Computational environment described
- [X] Data processing code included
- [X] Analysis scripts provided
- [X] Key outputs archived

Environment:
- VASP: 6.4.2
- Python: 3.11.4
- ASE: 3.23.0
- NumPy: 1.24.3
```

## Search and Retrieval

### Using Org-Mode Search
```org
# Search by tag
C-c / m    # Match tags
Search: experiment+DFT-failed

# Search by TODO
C-c / t    # Show TODOs

# Search by text
C-c / r    # Regex search

# Org-agenda
C-c a s    # Search across all agenda files
```

### Creating Index File
```org
#+TITLE: Research Notebook Index
#+STARTUP: overview

* Projects
** [[file:2025/01-January/2025-01-15.org::*Catalyst Screening][Catalyst Screening Project]]
Started: [2025-01-05]
Status: Active

** Method Validation
Started: [2025-01-10]
Status: Complete

* Key Results
** [[file:2025/01-January/2025-01-15.org::*Functional Comparison][Best DFT Functional for CO Adsorption]]
Result: PBE preferred (error < 0.25 eV)

* Important Techniques
** [[file:2025/01-January/2025-01-12.org::*Convergence Testing][How to Test k-point Convergence]]
```

## Templates

### Quick Entry Template
```org
* Short Description
:PROPERTIES:
:ID: YYYY-MM-DD-NNN
:END:

** What I Did

** Why

** Results

** Next Steps
```

### Full Research Entry Template
```org
* Project - Specific Task
:PROPERTIES:
:ID: YYYY-MM-DD-NNN
:PROJECT: Project_Name
:STATUS: In_Progress
:END:

** Objective

** Hypothesis

** Methods

** Results

** Analysis

** Conclusions

** Next Steps
```

## Integration with Computational Workflows

### Embedding Code
```org
#+BEGIN_SRC python :results output :session :exports both
from ase.build import molecule
from ase.optimize import BFGS

mol = molecule('H2O')
# ... calculation code ...
print(f"Optimized energy: {mol.get_potential_energy():.3f} eV")
#+END_SRC
```

### Capturing Output
```org
#+BEGIN_SRC bash :results output
cd ~/calculations/calc_001
grep "energy" OUTCAR | tail -1
#+END_SRC

#+RESULTS:
: free energy = -123.456789 eV
```

### Including Figures
```org
#+CAPTION: CO adsorption energy vs coverage
#+NAME: fig:ads_energy
#+ATTR_ORG: :width 400
[[file:./data/2025-01-15/adsorption_plot.png]]

See Figure [[fig:ads_energy]] for trends.
```

## Professional Writing Guidelines

### 1. Clarity
- Write complete sentences
- Use past tense for work done
- Use present tense for conclusions
- Avoid ambiguous pronouns

### 2. Precision
- Include units: "300 K" not "300"
- Specify parameters: "PBE functional" not "DFT"
- Quantify: "decreased by 15%" not "decreased significantly"

### 3. Organization
- Use hierarchical structure
- One concept per section
- Chronological within day
- Logical grouping across days

### 4. Completeness
- Include negative results
- Document failed attempts
- Explain unexpected outcomes
- Preserve troubleshooting notes

## Response Patterns

When helping with ELN:
1. **Suggest appropriate structure** for the entry type
2. **Ask clarifying questions** about what was done, why, and outcomes
3. **Recommend org-mode features** that fit the task
4. **Ensure reproducibility** - check if enough detail provided
5. **Encourage cross-referencing** to related work
6. **Format code and data** properly
7. **Include metadata** (properties, tags, timestamps)
8. **Maintain professional tone** while being helpful
9. **Prompt for conclusions** and next steps
10. **Suggest organizational improvements** when appropriate

## Common Scenarios

### Starting New Project
```org
* Project Initiation - Catalyst Screening Study
:PROPERTIES:
:ID: 2025-01-15-001
:PROJECT: Catalyst_Screening
:STATUS: Planning
:END:

** Project Goal
Identify optimal catalyst for CO oxidation through computational screening.

** Background
[Literature review, motivation]

** Approach
1. Validate methodology with Pt(111) benchmark
2. Screen FCC metals (Cu, Ag, Au, Pt, Pd, Ni)
3. Analyze trends using d-band model
4. Validate top candidates with experiments

** Success Criteria
- Identify 2-3 promising candidates
- Error < 0.3 eV vs experiment
- Complete by [2025-02-28]

** Initial Tasks
- [ ] Set up calculation framework
- [ ] Run Pt(111) benchmark
- [ ] Compare functionals
```

### Documenting Failure
```org
* Failed Attempt - NEB Calculation
:PROPERTIES:
:ID: 2025-01-15-006
:STATUS: Failed
:END:

** Objective
Calculate CO diffusion barrier on Pt(111).

** Approach
NEB with 7 images, climbing image method.

** Problem
Images diverged after 20 steps. Final image forces > 5 eV/Å.

** Analysis
Initial path guess was poor - CO moved through bulk instead of surface.

** Lessons Learned
- Need better initial path interpolation
- Check geometry of intermediate images before optimization
- Use more images (9-11) for surface diffusion

** Next Attempt
Will use improved path generation script and verify geometries.
Scheduled for [[file:2025-01-16.org][tomorrow]].
```

## Archiving and Long-Term Storage

### End-of-Month Summary
```org
#+TITLE: January 2025 Summary
#+DATE: [2025-01-31]

* Overview
Completed validation of DFT methodology for catalyst project.
Screened 6 FCC metals for CO adsorption. Identified Pt and Pd as
most promising candidates.

* Key Accomplishments
- Validated PBE functional (error < 0.25 eV)
- Completed metal screening
- Resolved VASP convergence issues

* Data Generated
- 24 DFT calculations
- 180 GB raw data (archived to ~/archives/2025/january/)
- 6 publications figures prepared

* Next Month Goals
- Begin reaction barrier calculations
- Compare with experimental data from collaborators
- Draft results section for paper
```

### Year-End Review
Create comprehensive summaries for easy retrieval and reporting.

---

### emacs-lisp

**Path**: `skills/programming/emacs-lisp/SKILL.md`

# Emacs Lisp Style Guide Skill

This skill provides expert guidance for writing professional, maintainable Emacs Lisp code following the community-driven Emacs Lisp Style Guide.

## When to Use This Skill

Use this skill when:
- Writing Emacs configuration files (~/.emacs.d/init.el)
- Creating Emacs packages or libraries
- Developing major or minor modes
- Writing interactive commands and functions
- Creating keybindings and hooks
- Refactoring existing Emacs Lisp code
- Setting up package metadata and autoloads
- Writing Emacs Lisp macros
- Creating custom variables and faces

## Core Principles

### 1. Source Code Layout

#### Indentation and Spacing

**Use Spaces, Never Tabs:**
```elisp
;; Good - spaces for indentation
(defun my-function (arg1 arg2)
  (let ((result (+ arg1 arg2)))
    (message "Result: %s" result)))

;; Bad - mixing tabs and spaces (never do this)
```

**Align Function Arguments Vertically:**
```elisp
;; Good - arguments aligned
(defun long-function-name (arg1
                           arg2
                           arg3)
  (body))

;; Good - first arg on new line, aligned with function name
(defun long-function-name
    (arg1 arg2 arg3)
  (body))

;; Bad - inconsistent alignment
(defun long-function-name (arg1
    arg2
  arg3)
  (body))
```

**Special Form Indentation:**
```elisp
;; Special forms use 4-space indent for special arguments,
;; 2-space for body

;; Good - if with proper indentation
(if some-condition
    (do-something)  ; 4 spaces for then-clause
  (do-else))        ; 2 spaces for else-clause

;; Good - let bindings
(let ((var1 value1)
      (var2 value2))  ; 4 spaces for bindings
  (body-form-1)       ; 2 spaces for body
  (body-form-2))

;; Good - when (no special argument)
(when condition
  (do-something)
  (do-more))
```

**Maximum Line Length:**
```elisp
;; Aim for 80 characters per line where reasonable
;; Break long lines logically

;; Good
(defun my-function ()
  "Short description."
  (let ((long-variable-name
         (some-function-that-returns-value)))
    long-variable-name))

;; Acceptable for long strings
(message "This is a very long message that exceeds 80 characters but is kept on one line for readability")
```

**Parentheses:**
```elisp
;; Good - space between text and opening paren
(defun my-function (arg)
  (list arg))

;; Bad - no space after function name
(defun my-function(arg)
  (list arg))

;; Good - all closing parens on same line
(defun my-function ()
  (when condition
    (let ((x 1))
      (+ x 2))))

;; Bad - closing parens on separate lines
(defun my-function ()
  (when condition
    (let ((x 1))
      (+ x 2)
    )
  )
)
```

**Vertical Whitespace:**
```elisp
;; Good - empty line between top-level forms
(defvar my-var1 "value1")

(defvar my-var2 "value2")

(defun my-function ()
  "Do something.")

;; Exception: related definitions can be grouped
(defvar my-var1 "value1")
(defvar my-var2 "value2")
(defvar my-var3 "value3")
```

### 2. Syntax and Idioms

#### Prefer Higher-Level Constructs

**Use `when` Instead of Single-Branch `if`:**
```elisp
;; Good
(when condition
  (do-something)
  (do-more))

;; Bad
(if condition
    (progn
      (do-something)
      (do-more)))
```

**Use `unless` for Negated Conditions:**
```elisp
;; Good
(unless condition
  (do-something))

;; Bad
(when (not condition)
  (do-something))

;; Bad
(if (not condition)
    (do-something))
```

**Use `not` Instead of `null`:**
```elisp
;; Good - checking for nil/false
(not value)

;; Good - checking for empty list specifically
(null my-list)

;; Bad - using null for general nil check
(null value)
```

**Leverage Comparison Functions:**
```elisp
;; Good - direct comparison
(< 5 x 10)

;; Bad - nested conditions
(and (> x 5) (< x 10))

;; Good - multiple comparisons
(= x y z)

;; Bad - nested comparisons
(and (= x y) (= y z))
```

**Use `cond` with `t` as Catch-All:**
```elisp
;; Good
(cond
 ((eq x 'foo) (handle-foo))
 ((eq x 'bar) (handle-bar))
 (t (handle-default)))

;; Bad - else at the end
(cond
 ((eq x 'foo) (handle-foo))
 ((eq x 'bar) (handle-bar))
 (else (handle-default)))
```

**Increment and Decrement:**
```elisp
;; Good
(1+ x)
(1- x)

;; Bad
(+ x 1)
(- x 1)
```

**Prefer `with-eval-after-load`:**
```elisp
;; Good - modern approach
(with-eval-after-load 'company
  (setq company-idle-delay 0.2))

;; Acceptable but older
(eval-after-load 'company
  '(setq company-idle-delay 0.2))
```

### 3. Naming Conventions

#### General Naming Rules

**Use Kebab-Case (Lisp-Case):**
```elisp
;; Good
(defun my-function-name ()
  (let ((my-variable 10))
    my-variable))

;; Bad - using snake_case
(defun my_function_name ()
  (let ((my_variable 10))
    my_variable))

;; Bad - using camelCase
(defun myFunctionName ()
  (let ((myVariable 10))
    myVariable))
```

**Library Name Prefixes:**
```elisp
;; For a library named 'my-package', prefix all public symbols
(defun my-package-do-something ()
  "Public function.")

(defvar my-package-option t
  "Public variable.")

;; Private functions use double-hyphen
(defun my-package--internal-helper ()
  "Private function.")

;; Example from projectile package
(defun projectile-find-file ()
  "Public command.")

(defun projectile--get-project-root ()
  "Private helper.")
```

**Unused Variables:**
```elisp
;; Good - prefix with underscore
(lambda (x _y _z)
  (* x 2))

;; Good - in destructuring
(pcase-let ((`(,first . ,_rest) my-list))
  first)
```

#### Predicates

**Single-Word Predicates:**
```elisp
;; Good - ends with 'p'
(defun evenp (n)
  (zerop (mod n 2)))

(defun emptyp (list)
  (null list))
```

**Multi-Word Predicates:**
```elisp
;; Good - ends with '-p'
(defun buffer-live-p (buffer)
  (and buffer (buffer-name buffer)))

(defun my-package-valid-state-p (state)
  (member state '(ready active complete)))
```

#### Special Naming

**Face Names:**
```elisp
;; Good - no -face suffix
(defface my-package-error
  '((t :foreground "red"))
  "Face for error messages.")

;; Bad - redundant -face suffix
(defface my-package-error-face
  '((t :foreground "red"))
  "Face for error messages.")
```

### 4. Functions

#### Lambda Functions

**Usage Guidelines:**
```elisp
;; Good - lambda for local use
(mapcar (lambda (x) (* x 2)) '(1 2 3))

;; Bad - lambda in hooks (use named functions)
(add-hook 'emacs-lisp-mode-hook
          (lambda () (eldoc-mode 1)))

;; Good - named function in hooks
(defun my-elisp-mode-setup ()
  "Setup for Emacs Lisp mode."
  (eldoc-mode 1))

(add-hook 'emacs-lisp-mode-hook #'my-elisp-mode-setup)
```

**Never Hard-Quote Lambdas:**
```elisp
;; Bad - hard-quoted lambda
(mapcar '(lambda (x) (* x 2)) list)

;; Good - unquoted lambda
(mapcar (lambda (x) (* x 2)) list)

;; Good - function quote (sharp-quote)
(mapcar #'(lambda (x) (* x 2)) list)
```

**Don't Wrap Existing Functions:**
```elisp
;; Bad - unnecessary wrapper
(add-hook 'prog-mode-hook
          (lambda () (linum-mode 1)))

;; Good - direct function reference
(add-hook 'prog-mode-hook #'linum-mode)
```

#### Function Quotes

**Use Sharp-Quote for Function Names:**
```elisp
;; Good - function quote aids byte-compilation
(mapcar #'upcase '("foo" "bar"))
(add-hook 'text-mode-hook #'turn-on-auto-fill)
(global-set-key (kbd "C-c f") #'find-file)

;; Acceptable but less optimal
(mapcar 'upcase '("foo" "bar"))

;; Bad - unquoted (only works for interactive commands)
(global-set-key (kbd "C-c f") 'find-file)
```

#### Parameter Limits

**Avoid Too Many Parameters:**
```elisp
;; Bad - too many positional parameters
(defun create-user (name email age address city state zip country)
  ...)

;; Good - use plist or alist for many parameters
(defun create-user (name email &rest properties)
  "Create user with NAME and EMAIL.
PROPERTIES is a plist that may include :age, :address, :city, etc."
  (let ((age (plist-get properties :age))
        (address (plist-get properties :address)))
    ...))

;; Good - use cl-defun with keyword arguments
(cl-defun create-user (name email &key age address city state)
  "Create user with NAME and EMAIL and optional properties."
  ...)
```

### 5. Macros

#### When to Use Macros

**Only Use Macros When Functions Won't Work:**
```elisp
;; Good - macro needed for special evaluation
(defmacro with-timing (label &rest body)
  "Execute BODY and print execution time with LABEL."
  (declare (indent 1))
  (let ((start (make-symbol "start")))
    `(let ((,start (current-time)))
       (prog1 (progn ,@body)
         (message "%s: %.2fs" ,label
                  (float-time (time-since ,start)))))))

;; Bad - function would work fine here
(defmacro double (x)
  `(* 2 ,x))

;; Good - use function instead
(defun double (x)
  (* 2 x))
```

#### Macro Design

**Extract Examples First:**
```elisp
;; Before writing macro, write example usage:
;;
;; (with-temporary-buffer
;;   (insert "content")
;;   (buffer-string))
;;
;; Then implement to match desired usage
```

**Use Declare for Metadata:**
```elisp
;; Good - complete macro with declare
(defmacro with-my-mode (buffer &rest body)
  "Execute BODY with my-mode enabled in BUFFER."
  (declare (indent 1)
           (debug (sexp body)))
  `(with-current-buffer ,buffer
     (my-mode 1)
     (unwind-protect
         (progn ,@body)
       (my-mode -1))))
```

**Prefer Syntax Quoting (Backquote):**
```elisp
;; Good - backquote with unquote
(defmacro my-when (condition &rest body)
  `(if ,condition
       (progn ,@body)))

;; Bad - manual list construction
(defmacro my-when (condition &rest body)
  (list 'if condition
        (cons 'progn body)))
```

### 6. Documentation

#### Docstrings

**Function Docstrings:**
```elisp
;; Good - complete docstring
(defun my-package-process-file (filename)
  "Process the file at FILENAME.

The file is read, processed, and the result is returned.
If FILENAME doesn't exist, signal an error.

Returns the processed content as a string."
  (with-temp-buffer
    (insert-file-contents filename)
    (buffer-string)))

;; Good - document all parameters
(defun my-package-create-entry (name email &optional age)
  "Create entry with NAME and EMAIL.

NAME should be a non-empty string.
EMAIL should be a valid email address.
Optional AGE should be a positive integer.

Returns the created entry as a plist."
  ...)
```

**Variable Docstrings:**
```elisp
;; Good
(defvar my-package-timeout 30
  "Timeout in seconds for network operations.

This value is used by all network functions in my-package.
Set to nil to disable timeout.")

;; Good - custom variable with type
(defcustom my-package-auto-save t
  "Whether to auto-save files.

When non-nil, files are automatically saved after modifications."
  :type 'boolean
  :group 'my-package)
```

**First Line Summary:**
```elisp
;; Good - first line is complete sentence
(defun my-function (arg)
  "Process ARG and return result.

Additional details about the function behavior.
More explanation if needed."
  ...)

;; Bad - incomplete first line
(defun my-function (arg)
  "Process ARG and
return result."
  ...)
```

#### Comments

**Comment Levels:**
```elisp
;;; Section Heading
;;; Used for major sections of the file

;;; Commentary:
;; Package description and usage examples

;;; Code:

;; Top-level comment
;; Used for explanations before functions

(defun my-function ()
  ;; Comment within function
  ;; Used for explaining code blocks
  (let ((x 10))  ; Margin comment for inline explanation
    x))
```

**Comment Best Practices:**
```elisp
;; Good - explains why, not what
(defun my-function (items)
  ;; Sort items first to optimize lookup performance
  (let ((sorted-items (sort items #'string<)))
    (process-items sorted-items)))

;; Bad - redundant comment
(defun my-function (items)
  ;; Sort the items
  (sort items #'string<))

;; Good - self-documenting code instead of comments
(defun process-active-buffers ()
  (let ((active-buffers (get-active-buffers)))
    (mapc #'process-buffer active-buffers)))

;; Bad - needs comments to explain
(defun process ()
  ;; Get buffers that are active
  (let ((bufs (get-bufs)))
    ;; Process each one
    (mapc #'proc bufs)))
```

**Annotation Keywords:**
```elisp
;; TODO: Add support for remote files
;; FIXME: This breaks with Unicode characters
;; OPTIMIZE: Cache results for better performance
;; HACK: Workaround for bug in package.el
;; REVIEW: Is this the best approach?

;; Good - with initials and date
;; TODO(user 2025-01-15): Implement async version
;; FIXME(user 2025-01-15): Handle edge case with empty input
```

### 7. Loading and Autoloading

#### Module Structure

**Proper File Footer:**
```elisp
;;; my-package.el --- Short description  -*- lexical-binding: t; -*-

;; Copyright (C) 2025 Your Name

;; Author: Your Name <your.email@example.com>
;; Version: 1.0.0
;; Package-Requires: ((emacs "27.1"))
;; Keywords: convenience, tools
;; URL: https://github.com/user/my-package

;;; Commentary:

;; Longer description of what the package does.
;; Usage examples.

;;; Code:

(defun my-package-do-something ()
  "Do something useful.")

(provide 'my-package)
;;; my-package.el ends here
```

#### Require vs Load

**Use `require` for Dependencies:**
```elisp
;; Good - idempotent, safe
(require 'subr-x)
(require 'cl-lib)

;; Bad - not idempotent
(load "subr-x")
(load-library "cl-lib")
```

#### Autoload Cookies

**Use Autoload for User-Facing Commands:**
```elisp
;; Good - autoload interactive command
;;;###autoload
(defun my-package-start ()
  "Start my-package."
  (interactive)
  (my-package-mode 1))

;; Good - autoload mode definition
;;;###autoload
(define-minor-mode my-package-mode
  "Toggle my-package mode."
  :lighter " MyPkg"
  (if my-package-mode
      (my-package--enable)
    (my-package--disable)))

;; Bad - autoloading internal function
;;;###autoload
(defun my-package--internal-helper ()
  "Internal helper.")
```

### 8. Lexical Binding

**Always Enable Lexical Binding:**
```elisp
;;; my-package.el --- Description  -*- lexical-binding: t; -*-

;; Lexical binding is faster and more intuitive
;; Always use it for new code
```

**Benefits of Lexical Binding:**
```elisp
;; With lexical binding, closures work correctly
(defun make-counter ()
  "Create a counter function."
  (let ((count 0))
    (lambda ()
      (setq count (1+ count))
      count)))

;; Usage:
;; (setq counter1 (make-counter))
;; (funcall counter1) => 1
;; (funcall counter1) => 2
```

### 9. List Operations

**Use `dolist` for Iteration:**
```elisp
;; Good - clear and idiomatic
(dolist (item items)
  (process-item item))

;; Bad - verbose
(mapc (lambda (item)
        (process-item item))
      items)
```

**Don't Use `mapcar` for Side Effects:**
```elisp
;; Bad - mapcar creates unused list
(mapcar #'process-item items)

;; Good - dolist for side effects
(dolist (item items)
  (process-item item))

;; Good - seq-do for functional style
(seq-do #'process-item items)
```

### 10. Interactive Commands

**Proper Interactive Specifications:**
```elisp
;; Good - with prompt
(defun my-package-open-file (filename)
  "Open FILENAME in my-package."
  (interactive "FOpen file: ")
  (find-file filename))

;; Good - multiple arguments
(defun my-package-replace (from to)
  "Replace FROM with TO in current buffer."
  (interactive
   (list (read-string "Replace: ")
         (read-string "With: ")))
  (save-excursion
    (goto-char (point-min))
    (while (search-forward from nil t)
      (replace-match to))))

;; Good - using prefix argument
(defun my-package-insert-n (n)
  "Insert N copies of something."
  (interactive "p")
  (dotimes (_ n)
    (insert "text")))
```

### 11. Error Handling

**Signal Errors Appropriately:**
```elisp
;; Good - descriptive error
(defun my-package-get-user (id)
  "Get user by ID."
  (or (gethash id my-package-users)
      (error "User not found: %s" id)))

;; Good - user-error for user mistakes
(defun my-package-save ()
  "Save current data."
  (interactive)
  (unless (my-package--has-changes-p)
    (user-error "No changes to save")))

;; Good - with condition-case
(defun my-package-load-file (filename)
  "Load FILENAME safely."
  (condition-case err
      (with-temp-buffer
        (insert-file-contents filename)
        (read (current-buffer)))
    (file-error
     (message "Cannot read file: %s" (error-message-string err))
     nil)))
```

## Best Practices Summary

When writing Emacs Lisp code:

1. **Always enable lexical binding** in file header
2. **Prefix all public symbols** with package name
3. **Use `defcustom`** for user-configurable variables
4. **Add autoload cookies** to interactive commands
5. **Write comprehensive docstrings** for all public functions
6. **Prefer built-in functions** over reinventing functionality
7. **Use `declare`** in macros for indentation and debugging
8. **Handle errors gracefully** with informative messages
9. **Follow naming conventions** consistently
10. **Keep functions focused** on single responsibilities

## Common Patterns

### Package Template

```elisp
;;; my-package.el --- Brief description  -*- lexical-binding: t; -*-

;; Copyright (C) 2025 Your Name

;; Author: Your Name <email@example.com>
;; Version: 1.0.0
;; Package-Requires: ((emacs "27.1"))
;; Keywords: convenience
;; URL: https://github.com/user/my-package

;;; Commentary:

;; Longer description and usage examples.

;;; Code:

(require 'cl-lib)

(defgroup my-package nil
  "My package customization group."
  :group 'convenience
  :prefix "my-package-")

(defcustom my-package-option t
  "Enable some option."
  :type 'boolean
  :group 'my-package)

;;;###autoload
(defun my-package-command ()
  "Main command."
  (interactive)
  (message "Hello from my-package!"))

(provide 'my-package)
;;; my-package.el ends here
```

## Resources

- [Emacs Lisp Style Guide](https://github.com/bbatsov/emacs-lisp-style-guide)
- [Emacs Lisp Reference Manual](https://www.gnu.org/software/emacs/manual/html_node/elisp/)
- [Emacs Lisp Packaging Guide](https://www.gnu.org/software/emacs/manual/html_node/elisp/Packaging.html)

---

### fairchem

**Path**: `skills/programming/fairchem/SKILL.md`

# FAIRChem Skill

This skill provides expert guidance for using FAIRChem (formerly OCP - Open Catalyst Project), Meta's FAIR Chemistry library of machine learning methods for materials science and quantum chemistry.

## When to Use This Skill

Use this skill when:
- Using ML potentials for materials and molecular simulations
- Running fast geometry optimizations with pretrained models
- Performing large-scale MD simulations
- Calculating energies and forces without DFT
- Working with the UMA (Universal Materials Algebra) models
- Needing predictions for catalysis, molecules, crystals, or MOFs
- Integrating ML models with ASE workflows
- Scaling calculations across multiple GPUs

## What is FAIRChem?

FAIRChem is Meta's machine learning framework for chemistry that provides:
- **Pretrained UMA models** (`uma-s-1p1`, `uma-m-1p1`) for universal predictions
- **Domain-specific tasks**: catalysis (oc20), materials (omat), molecules (omol), MOFs (odac), crystals (omc)
- **ASE integration** via `FAIRChemCalculator`
- **Multi-GPU support** for distributed inference
- **Fast predictions**: 100-1000× faster than DFT

### Key Advantage
FAIRChem allows you to use the same model across different chemistry domains by simply changing the `task_name` parameter.

## Core Concepts

### 1. UMA Models
Universal Materials Algebra models trained on diverse datasets:
- `uma-s-1p1`: Small model (~50M parameters) - faster inference
- `uma-m-1p1`: Medium model (~300M parameters) - higher accuracy

### 2. Task Names (Domains)
Specify the chemistry domain for domain-specific predictions:
- `oc20`: Catalysis (surfaces with adsorbates)
- `omat`: Inorganic materials (crystals, bulk)
- `omol`: Molecules (organic chemistry)
- `odac`: Metal-organic frameworks (MOFs)
- `omc`: Molecular crystals

### 3. FAIRChemCalculator
ASE calculator interface that wraps UMA models:
- Drop-in replacement for DFT calculators
- Supports all ASE workflows
- Provides energies, forces, and stresses
- Compatible with optimization, MD, NEB

### 4. Inference Settings
Performance optimization modes:
- `turbo`: Maximum speed, slightly reduced accuracy
- Standard: Balanced speed and accuracy
- Multi-GPU: Distributed inference with `workers=N`

## Installation

```bash
# Install fairchem
pip install fairchem-core

# For GPU support
pip install fairchem-core[gpu]

# Hugging Face login (required for UMA models)
pip install huggingface-hub
huggingface-cli login
```

**Note**: You must have a Hugging Face account and request access to the UMA model repository.

## Basic Usage Pattern

### Standard Workflow

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.build import bulk
from ase.optimize import LBFGS

# 1. Load pretrained model
predict_unit = load_predict_unit("uma-m-1p1")

# 2. Create calculator for specific domain
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omat"  # Choose domain
)

# 3. Use with ASE
atoms = bulk("Cu", "fcc", a=3.6)
atoms.calc = calc

# 4. Calculate properties
energy = atoms.get_potential_energy()
forces = atoms.get_forces()

# 5. Optimize structure
opt = LBFGS(atoms)
opt.run(fmax=0.05)
```

## Common Workflows

### Workflow 1: Catalysis - Surface Adsorption

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.build import fcc111, add_adsorbate
from ase.optimize import LBFGS
from ase.constraints import FixAtoms

# Load model
predict_unit = load_predict_unit("uma-m-1p1")

# Create calculator for catalysis
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="oc20"  # Catalysis domain
)

# Build slab with adsorbate
slab = fcc111("Cu", size=(4, 4, 4), vacuum=10.0)
add_adsorbate(slab, "CO", height=2.0, position="fcc")

# Fix bottom layers
n_atoms_per_layer = 16
constraint = FixAtoms(indices=range(n_atoms_per_layer * 2))
slab.set_constraint(constraint)

# Attach calculator and optimize
slab.calc = calc
opt = LBFGS(slab, trajectory="slab_opt.traj")
opt.run(fmax=0.05)

# Get results
E = slab.get_potential_energy()
forces = slab.get_forces()
```

### Workflow 2: Bulk Materials - Lattice Optimization

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.build import bulk
from ase.optimize import FIRE
from ase.filters import FrechetCellFilter

# Load model
predict_unit = load_predict_unit("uma-m-1p1")

# Calculator for materials
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omat"  # Materials domain
)

# Create bulk structure
atoms = bulk("Fe", "bcc", a=2.87)
atoms.calc = calc

# Optimize both positions and cell
# FrechetCellFilter allows cell parameters to change
ucf = FrechetCellFilter(atoms)
opt = FIRE(ucf)
opt.run(fmax=0.05)

# Results
optimized_lattice = atoms.cell.cellpar()[0]
print(f"Optimized lattice constant: {optimized_lattice:.3f} Å")
```

### Workflow 3: Molecular Dynamics

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.build import bulk
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.langevin import Langevin
from ase import units

# Load with turbo settings for speed
predict_unit = load_predict_unit(
    "uma-s-1p1",  # Use small model for MD
    inference_settings="turbo"
)

# Calculator for MD
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omat",
    workers=4  # Multi-GPU for large systems
)

# Large system
atoms = bulk("C", "fcc", a=3.57) * (20, 20, 20)  # 8000 atoms
atoms.calc = calc

# Initialize velocities
MaxwellBoltzmannDistribution(atoms, temperature_K=300)

# Run NVT dynamics
dyn = Langevin(
    atoms,
    timestep=1.0 * units.fs,
    temperature_K=300,
    friction=0.002
)

# Run
from ase.io.trajectory import Trajectory
traj = Trajectory("md.traj", "w", atoms)
dyn.attach(traj.write, interval=10)
dyn.run(5000)
```

### Workflow 4: Molecular Systems

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.build import molecule
from ase.optimize import LBFGS

# Load model
predict_unit = load_predict_unit("uma-m-1p1")

# Calculator for molecules
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omol"  # Molecular domain
)

# Build molecule
mol = molecule("H2O")
mol.center(vacuum=10.0)
mol.calc = calc

# Optimize
opt = LBFGS(mol, trajectory="mol_opt.traj")
opt.run(fmax=0.05)

# Get properties
E = mol.get_potential_energy()
forces = mol.get_forces()
```

### Workflow 5: NEB Calculations

```python
from fairchem.data.ase import FAIRChemCalculator
from fairchem.predict import load_predict_unit
from ase.neb import NEB
from ase.optimize import BFGS
from ase.io import read

# Load model
predict_unit = load_predict_unit("uma-m-1p1")

# Calculator
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="oc20"
)

# Load initial and final states
initial = read("initial.traj")
final = read("final.traj")

# Create NEB
images = [initial]
images += [initial.copy() for i in range(5)]
images += [final]

neb = NEB(images)
neb.interpolate()

# Attach calculator to intermediate images
for image in images[1:-1]:
    image.calc = calc

# Optimize
opt = BFGS(neb, trajectory="neb.traj")
opt.run(fmax=0.05)

# Analyze
energies = [img.get_potential_energy() for img in images]
barrier = max(energies) - energies[0]
print(f"Barrier: {barrier:.3f} eV")
```

## Model Loading Options

### Load Pretrained UMA Model

```python
from fairchem.predict import load_predict_unit

# Standard loading
predict_unit = load_predict_unit("uma-m-1p1")

# With turbo mode (faster, slight accuracy trade-off)
predict_unit = load_predict_unit(
    "uma-m-1p1",
    inference_settings="turbo"
)

# Specify device
predict_unit = load_predict_unit(
    "uma-m-1p1",
    device="cuda:0"
)

# Load local checkpoint
predict_unit = load_predict_unit(
    "/path/to/checkpoint.pt",
    device="cuda"
)
```

### Available Models

- `uma-s-1p1`: Small, fast (~50M params)
- `uma-m-1p1`: Medium, accurate (~300M params)

## Task Selection Guide

| Domain | Task Name | Use For | Examples |
|--------|-----------|---------|----------|
| Catalysis | `oc20` | Surfaces + adsorbates | CO on Cu(111), O on Pt |
| Materials | `omat` | Bulk crystals, defects | Fe lattice, Si bulk |
| Molecules | `omol` | Organic molecules | H2O, CH4, proteins |
| MOFs | `odac` | Metal-organic frameworks | ZIF-8, MOF-5 |
| Crystals | `omc` | Molecular crystals | Ice, organic crystals |

## Performance Optimization

### Multi-GPU Inference

```python
# Use multiple GPUs automatically
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omat",
    workers=8  # Use 8 GPUs
)

# Achieves ~10× speedup on 8× H100 GPUs
```

### Turbo Mode

```python
# Trade slight accuracy for speed
predict_unit = load_predict_unit(
    "uma-s-1p1",  # Small model
    inference_settings="turbo"
)

# Good for:
# - MD simulations
# - Large systems
# - Initial screening
```

### Batch Predictions

```python
# For multiple similar calculations
from fairchem.data.ase import batch_predict

structures = [atoms1, atoms2, atoms3, ...]

results = batch_predict(
    structures,
    predict_unit=predict_unit,
    task_name="omat"
)
```

## Best Practices

### 1. Task Selection
Always choose the appropriate task for your system:
- **Surfaces with adsorbates** → `oc20`
- **Bulk materials** → `omat`
- **Isolated molecules** → `omol`
- **MOFs** → `odac`
- **Molecular crystals** → `omc`

### 2. Model Selection
- **Initial screening**: Use `uma-s-1p1` + turbo
- **Production calculations**: Use `uma-m-1p1`
- **Very large systems**: Use `uma-s-1p1` + workers

### 3. Validation
ML models have different error characteristics than DFT:
```python
# Always validate critical results
# Compare ML prediction with DFT for representative cases
ml_energy = atoms.get_potential_energy()  # FAIRChem
atoms.calc = Vasp(...)  # Switch to DFT
dft_energy = atoms.get_potential_energy()
error = abs(ml_energy - dft_energy)
```

### 4. Uncertainty Quantification
FAIRChem models provide predictions but not uncertainty:
- Test on similar known systems first
- Validate against DFT for critical results
- Use ensemble of predictions if available

### 5. Memory Management
For large systems:
```python
# Use turbo mode
predict_unit = load_predict_unit(
    "uma-s-1p1",
    inference_settings="turbo"
)

# Distribute across GPUs
calc = FAIRChemCalculator(
    predict_unit=predict_unit,
    task_name="omat",
    workers=4
)
```

## Common Patterns

### Energy Calculation

```python
atoms.calc = calc
energy = atoms.get_potential_energy()  # eV
forces = atoms.get_forces()  # eV/Å
stress = atoms.get_stress()  # eV/Å³
```

### Geometry Optimization

```python
from ase.optimize import LBFGS, FIRE, BFGS

# Fast convergence
opt = LBFGS(atoms, trajectory="opt.traj")
opt.run(fmax=0.05)

# For difficult systems
opt = FIRE(atoms)
opt.run(fmax=0.05)
```

### Cell Optimization

```python
from ase.filters import FrechetCellFilter

# Optimize both atoms and cell
ucf = FrechetCellFilter(atoms)
opt = FIRE(ucf)
opt.run(fmax=0.05)
```

## Integration with ASE

FAIRChemCalculator is a full ASE calculator:

```python
# All ASE functionality works
from ase.vibrations import Vibrations
from ase.thermochemistry import IdealGasThermo
from ase.eos import calculate_eos

# Vibrational analysis
vib = Vibrations(atoms)
vib.run()

# Thermochemistry
thermo = IdealGasThermo(...)

# Equation of state
eos = calculate_eos(atoms)
```

## Troubleshooting

### Hugging Face Authentication

```bash
# Login to Hugging Face
huggingface-cli login

# Request access to UMA models at:
# https://huggingface.co/meta-llama/uma-m-1p1
```

### GPU Memory Issues

```python
# Use smaller model
predict_unit = load_predict_unit("uma-s-1p1")

# Use turbo mode
predict_unit = load_predict_unit(
    "uma-s-1p1",
    inference_settings="turbo"
)

# Reduce batch size (if using batch predictions)
```

### Slow Inference

```python
# Enable turbo mode
inference_settings="turbo"

# Use multiple GPUs
workers=N

# Use smaller model
"uma-s-1p1"
```

### Wrong Task Selection

```python
# Symptoms: Poor predictions, unphysical results
# Solution: Verify task matches your system

# For surfaces + adsorbates:
task_name="oc20"  # NOT "omat" or "omol"

# For bulk materials:
task_name="omat"  # NOT "oc20"
```

## Version Compatibility

**Important**: FAIRChem v2 is a breaking change from v1
- v2 code is NOT compatible with v1 models
- v1 code is NOT compatible with v2 models
- UMA models require FAIRChem >= 2.0

```python
# Check version
import fairchem
print(fairchem.__version__)  # Should be >= 2.0 for UMA
```

## Comparison with DFT

| Aspect | FAIRChem | DFT |
|--------|----------|-----|
| Speed | 100-1000× faster | Slower |
| Accuracy | ~0.1 eV | Reference |
| Scaling | Linear, multi-GPU | Cubic |
| System size | 1000s of atoms | 10-100s atoms |
| Use case | Screening, MD | High accuracy |

## When to Use FAIRChem vs DFT

**Use FAIRChem for:**
- Initial screening of many structures
- Long MD simulations
- Large systems (>500 atoms)
- Rapid prototyping
- High-throughput workflows

**Use DFT for:**
- Final validation
- Novel chemistries outside training data
- When highest accuracy needed
- Electronic structure analysis
- Magnetic properties

## Resources

When suggesting FAIRChem solutions:
- Specify correct task_name for the domain
- Recommend appropriate model (s vs m)
- Suggest performance optimizations (turbo, workers)
- Include validation against known results
- Mention Hugging Face authentication requirement
- Provide complete working examples
- Note v2 compatibility requirements

## Example Response Pattern

When helping with FAIRChem:
1. Identify the chemistry domain (catalysis, materials, molecules, etc.)
2. Select appropriate task_name
3. Choose model based on accuracy/speed requirements
4. Provide complete code with imports
5. Suggest performance optimizations if needed
6. Recommend validation steps
7. Note any domain-specific considerations

---

### idaes

**Path**: `skills/programming/idaes/SKILL.md`

# IDAES Process Systems Engineering

Systematic guidance for using IDAES to model, simulate, and optimize chemical processes and energy systems.

## IDAES Workflow

### 1. Identify Task Type

**What do you want to do?**

**Getting Started:**
- Install and setup → `references/installation.md`
- Understand core concepts → `references/core-concepts.md`
- First flowsheet → `examples/simple_flowsheet.py`

**Flowsheet Development:**
- Build basic flowsheet → `references/flowsheets.md`
- Add unit models → `references/unit-models.md`
- Connect streams → `references/flowsheets.md`
- Set up material/energy balances → `references/flowsheets.md`
- Add time-dependent behavior → `references/dynamic-modeling.md`

**Property Modeling:**
- Select property package → `references/property-packages.md`
- Configure components and phases → `references/property-packages.md`
- Define thermodynamic methods → `references/property-packages.md`
- Add reaction packages → `references/property-packages.md`
- Create custom properties → `references/custom-models.md`

**Unit Operations:**
- Use generic models (mixers, splitters, heaters) → `references/generic-models.md`
- Model power generation equipment → `references/power-generation.md`
- Set up gas-solid contactors → `references/gas-solid-models.md`
- Configure separations → `references/generic-models.md`
- Add reactors → `references/generic-models.md`

**Solving and Optimization:**
- Initialize models → `references/initialization.md`
- Solve flowsheets → `references/solving.md`
- Run optimization → `references/optimization.md`
- Perform parameter estimation → `references/parameter-estimation.md`
- Data reconciliation → `references/parameter-estimation.md`

**Diagnostics and Scaling:**
- Diagnose model issues → `references/diagnostics.md`
- Apply scaling → `references/scaling.md`
- Identify structural problems → `references/diagnostics.md`
- Fix convergence issues → `references/solving.md`

**Analysis:**
- Calculate process economics → `references/costing.md`
- Perform sensitivity analysis → `references/optimization.md`
- Analyze results → `examples/analysis.py`
- Generate reports → `examples/reporting.py`

### 2. Core IDAES Workflow

**Basic pattern:**
```python
# 1. Import IDAES modules
from pyomo.environ import ConcreteModel
from idaes.core import FlowsheetBlock
from idaes.models.properties import iapws95
from idaes.models.unit_models import Heater

# 2. Create model and flowsheet
m = ConcreteModel()
m.fs = FlowsheetBlock(dynamic=False)

# 3. Add property package
m.fs.properties = iapws95.Iapws95ParameterBlock()

# 4. Add unit models
m.fs.heater = Heater(property_package=m.fs.properties)

# 5. Set inputs
m.fs.heater.inlet.flow_mol.fix(100)  # mol/s
m.fs.heater.inlet.pressure.fix(101325)  # Pa
m.fs.heater.inlet.enth_mol.fix(5000)  # J/mol
m.fs.heater.heat_duty.fix(10000)  # W

# 6. Initialize
m.fs.heater.initialize()

# 7. Solve
from idaes.core.solvers import get_solver
solver = get_solver()
results = solver.solve(m)

# 8. Display results
m.fs.heater.outlet.display()
```

## Quick Reference - Common Tasks

**Create flowsheet:** `FlowsheetBlock(dynamic=False)` - Main container for process model
**Add unit model:** `m.fs.unit = UnitModel(property_package=m.fs.props)`
**Fix variables:** `m.fs.unit.inlet.flow_mol.fix(100)` - Specify known values
**Initialize model:** `m.fs.unit.initialize()` - Set up for solving
**Solve flowsheet:** `solver.solve(m)` - Get solution
**Run diagnostics:** `DiagnosticsToolbox(m).report_structural_issues()`
**Apply scaling:** `iscale.calculate_scaling_factors(m)`
**Optimize:** Set up objective and use solver or optimization tools

Detailed examples for all tasks in reference files.

## Task Routing

### Core Concepts and Architecture

**Route to:** `references/core-concepts.md`

**When:**
- Understanding IDAES architecture
- Learning about flowsheet structure
- Understanding control volumes
- Working with state blocks
- Understanding the modeling framework

**Key concepts:**
- FlowsheetBlock
- Property packages and state blocks
- Unit models and control volumes
- Ports and Arcs for connectivity
- Time domains for dynamic models

### Flowsheet Construction

**Route to:** `references/flowsheets.md`

**When:**
- Building process flowsheets
- Connecting unit operations
- Setting up material/energy streams
- Creating process flow diagrams
- Organizing hierarchical flowsheets

**Key components:**
- FlowsheetBlock creation
- Arc connections between units
- Port specification
- Degrees of freedom analysis
- Flowsheet visualization

### Property Packages

**Route to:** `references/property-packages.md`

**When:**
- Selecting thermodynamic methods
- Configuring component properties
- Setting up phase equilibrium
- Defining mixture properties
- Working with specialized systems (water/steam, combustion gases, etc.)

**Available packages:**
- IAPWS95 (water/steam)
- Ideal gas mixtures
- Modular property framework
- Cubic equations of state
- Electrolyte solutions (eNRTL)

### Unit Models

**Route to:** `references/unit-models.md`

**When:**
- Adding equipment to flowsheet
- Configuring unit operations
- Setting operating specifications
- Understanding model equations
- Customizing unit behavior

**Categories:**
- Generic models (heater, pump, compressor, etc.)
- Separations (flash, distillation, membranes)
- Reactors (stoichiometric, equilibrium, kinetic)
- Heat transfer (heat exchangers)
- Power generation specific equipment

### Generic Model Library

**Route to:** `references/generic-models.md`

**When:**
- Using standard unit operations
- Building general chemical processes
- Need mixers, splitters, heat exchangers
- Setting up separation equipment
- Working with reactors and pumps

**Common models:**
- Mixer, Splitter, Separator
- Heater, HeatExchanger
- Pump, Compressor, Turbine
- Flash, Distillation
- CSTR, PFR, Equilibrium Reactor

### Power Generation Models

**Route to:** `references/power-generation.md`

**When:**
- Modeling power plants
- Simulating combustion systems
- Working with steam cycles
- Analyzing turbine performance
- Boiler and heat recovery steam generator (HRSG) modeling

**Key models:**
- Boiler/Fireside models
- Steam turbines
- Heat recovery steam generators
- Feed water heaters
- Power plant flowsheets

### Gas-Solid Contactors

**Route to:** `references/gas-solid-models.md`

**When:**
- Modeling fluidized beds
- Simulating moving beds
- Working with solid particle flows
- Gas-solid reactions
- Adsorption processes

**Applications:**
- Chemical looping combustion
- Fixed bed reactors
- Fluidized bed reactors
- Moving bed systems

### Initialization

**Route to:** `references/initialization.md`

**When:**
- Preparing models for solving
- Dealing with initialization failures
- Setting up sequential initialization
- Using initialization strategies
- Troubleshooting convergence

**Strategies:**
- BlockTriangularizationInitializer
- Sequential initialization
- Custom initialization routines
- Using previous solutions
- Hierarchical initialization

### Solving and Convergence

**Route to:** `references/solving.md`

**When:**
- Solving flowsheet models
- Dealing with solver failures
- Improving convergence
- Understanding solver options
- Troubleshooting numerical issues

**Tools:**
- IPOPT solver configuration
- Solver selection
- Convergence diagnostics
- Solver output interpretation
- Handling solver failures

### Scaling

**Route to:** `references/scaling.md`

**When:**
- Improving numerical conditioning
- Addressing scaling issues
- Applying variable scaling
- Equation scaling
- Diagnosing ill-conditioned problems

**Key tools:**
- iscale module
- Automatic scaling factor calculation
- Manual scaling specification
- Scaling diagnostics
- Best practices for scaling

### Model Diagnostics

**Route to:** `references/diagnostics.md`

**When:**
- Debugging model issues
- Identifying structural problems
- Finding numerical issues
- Analyzing degrees of freedom
- Detecting equation singularities

**Diagnostic tools:**
- DiagnosticsToolbox
- Structural singularity detection
- Numerical singularity detection
- Degrees of freedom analysis
- SVD analysis for rank deficiency

### Optimization

**Route to:** `references/optimization.md`

**When:**
- Optimizing process design
- Minimizing operating costs
- Maximizing efficiency or production
- Multi-objective optimization
- Parameter studies and sensitivity analysis

**Capabilities:**
- Objective function definition
- Constraint specification
- Optimization solver configuration
- Parametric studies
- Design optimization

### Parameter Estimation and Data Reconciliation

**Route to:** `references/parameter-estimation.md`

**When:**
- Fitting model parameters to data
- Calibrating models
- Reconciling measured data
- Estimating kinetic parameters
- Model validation

**Tools:**
- parmest module
- Data reconciliation workflows
- Parameter estimation strategies
- Uncertainty quantification
- Model-data comparison

### Process Costing

**Route to:** `references/costing.md`

**When:**
- Calculating capital costs
- Estimating operating costs
- Economic analysis
- Optimization with cost objectives
- Techno-economic assessment

**Features:**
- Capital cost correlations
- Operating cost calculations
- Costing libraries for power generation
- Custom costing models

### Dynamic Modeling

**Route to:** `references/dynamic-modeling.md`

**When:**
- Time-dependent simulations
- Startup/shutdown analysis
- Control system design
- Dynamic optimization
- Process dynamics analysis

**Capabilities:**
- Dynamic flowsheets
- DAE systems
- Time discretization
- Dynamic solvers
- Control implementation

### Custom Model Development

**Route to:** `references/custom-models.md`

**When:**
- Creating new unit models
- Developing custom property packages
- Implementing specialized equations
- Extending existing models
- Research and development

**Topics:**
- Unit model templates
- Property package framework
- Custom constraints and expressions
- Model documentation
- Testing custom models

## Common Patterns

**Pattern 1: Simple Steady-State Flowsheet**
- Create flowsheet → Add property package → Add units → Connect with Arcs → Fix inputs → Initialize → Solve
- See `examples/simple_flowsheet.py` for complete workflow

**Pattern 2: Optimization Study**
- Build flowsheet → Initialize → Define objective → Set bounds → Optimize → Analyze results
- See `examples/optimization_example.py`

**Pattern 3: Parameter Estimation**
- Build model → Load experimental data → Define parameters → Run parmest → Analyze fit
- See `examples/parameter_estimation.py`

**Pattern 4: Sequential Modular Approach**
- Initialize units sequentially → Propagate information → Solve individual units → Solve full flowsheet
- See `examples/sequential_initialization.py`

All patterns detailed in `examples/` directory with complete code.

## Installation and Setup

### Install IDAES

```bash
# Create conda environment (recommended)
conda create -n idaes python=3.11
conda activate idaes

# Install IDAES
pip install idaes-pse

# Get solver binaries (IPOPT, etc.)
idaes get-extensions

# Verify installation
idaes --version
```

### Optional Components

```bash
# Install optional UI components
pip install idaes-pse[ui]

# Install OMLT for machine learning surrogates
pip install idaes-pse[omlt]

# Install for advanced grid optimization
pip install idaes-pse[grid]

# Install CoolProp for additional properties
pip install idaes-pse[coolprop]
```

### Testing Installation

```python
# Test IDAES import
import idaes
print(idaes.__version__)

# Test solver availability
from idaes.core.solvers import get_solver
solver = get_solver()
print(f"Solver: {solver}")
```

## Common Issues and Solutions

### Issue: Solver not found

**Problem:** `ApplicationError: No executable found for solver 'ipopt'`

**Solution:**
```bash
idaes get-extensions
# Or install solvers manually
conda install -c conda-forge ipopt
```

### Issue: Initialization fails

**Problem:** Unit model initialization does not converge

**Solution:**
1. Check degrees of freedom: `m.fs.unit.report_degrees_of_freedom()`
2. Use diagnostics: `DiagnosticsToolbox(m).report_structural_issues()`
3. Check input specifications are reasonable
4. Try sequential initialization
5. Review scaling factors

### Issue: Model doesn't solve

**Problem:** Solver returns non-optimal status

**Solution:**
```python
# Run diagnostics first
from idaes.core.util.model_diagnostics import DiagnosticsToolbox
dt = DiagnosticsToolbox(m)
dt.report_structural_issues()
dt.report_numerical_issues()

# Check and apply scaling
from idaes.core.util import scaling as iscale
iscale.calculate_scaling_factors(m)

# Try different solver options
solver = get_solver('ipopt', options={'tol': 1e-6, 'max_iter': 500})
```

### Issue: Poor numerical conditioning

**Problem:** Solver reports numerical difficulties or Jacobian issues

**Solution:**
1. Apply proper scaling using `iscale`
2. Check variable bounds are reasonable
3. Review units of measurement consistency
4. Use diagnostics to identify badly scaled variables
5. Consider variable transformations (e.g., log scaling for large range variables)

### Issue: Property package errors

**Problem:** Property calculations fail or return invalid values

**Solution:**
```python
# Check state variable specifications
m.fs.state.display()

# Ensure values are within valid ranges
# For IAPWS: T > 273.15 K, P > 611 Pa

# Check phase equilibrium assumptions are valid
# Use appropriate property package for your system
```

## Best Practices

### Model Development

- Start simple: build and test units individually before connecting
- Use degrees of freedom analysis regularly
- Always check model structure before solving
- Implement scaling from the start
- Document assumptions and specifications

### Initialization Strategy

- Initialize units in logical process order (upstream to downstream)
- Use results from simpler models to initialize complex ones
- Leverage IDAES initialization tools (BlockTriangularizationInitializer)
- Save successful initializations for reuse
- Consider hierarchical initialization for large flowsheets

### Numerical Robustness

- Apply consistent scaling across all variables
- Use appropriate property packages for your conditions
- Set reasonable variable bounds
- Monitor solver output and diagnostics
- Test with different initial guesses if convergence fails

### Performance Optimization

- Use appropriate solver tolerances (don't over-solve)
- Consider model simplifications where justified
- Use warm starts from previous solutions
- Profile code to identify bottlenecks
- Consider surrogate models for expensive property calculations

### Code Organization

- Use clear naming conventions for units and streams
- Organize complex flowsheets hierarchically
- Document specifications and assumptions
- Create reusable functions for common operations
- Use version control for model development

## Debugging Workflow

### Step 1: Check Model Structure
```python
# Degrees of freedom
m.fs.report_degrees_of_freedom()

# Structural diagnostics
from idaes.core.util.model_diagnostics import DiagnosticsToolbox
dt = DiagnosticsToolbox(m)
dt.report_structural_issues()
```

### Step 2: Verify Specifications
```python
# Display fixed variables
for v in m.component_data_objects(ctype=pyo.Var, descend_into=True):
    if v.fixed:
        print(f"{v}: {v.value}")

# Check for over/under specification
assert degrees_of_freedom(m) == 0
```

### Step 3: Check Scaling
```python
# Check scaling factors
from idaes.core.util import scaling as iscale
badly_scaled = iscale.badly_scaled_var_generator(m)
for var, scale in badly_scaled:
    print(f"{var}: scaling factor = {scale}")
```

### Step 4: Initialize Carefully
```python
# Try sequential initialization
try:
    m.fs.unit.initialize()
except:
    # If fails, check inputs and try with relaxed tolerances
    m.fs.unit.initialize(optarg={'tol': 1e-3})
```

### Step 5: Solve with Diagnostics
```python
solver = get_solver()
results = solver.solve(m, tee=True)  # tee=True shows solver output

# Check results
from pyomo.opt import TerminationCondition
if results.solver.termination_condition != TerminationCondition.optimal:
    print("Solve failed!")
    dt.report_numerical_issues()
```

## Examples Directory

See `examples/` for complete workflows:
- `simple_flowsheet.py` - Basic steady-state flowsheet
- `heater_example.py` - Simple heater with steam properties
- `flash_separation.py` - Flash separator example
- `heat_exchanger_network.py` - Multiple heat exchangers
- `distillation_column.py` - Separation process
- `power_plant_cycle.py` - Steam power cycle
- `optimization_example.py` - Process optimization
- `parameter_estimation.py` - Fitting parameters to data
- `dynamic_simulation.py` - Time-dependent model
- `custom_unit_model.py` - Creating custom models

## Reference Documentation

- `references/core-concepts.md` - Architecture and fundamental concepts
- `references/flowsheets.md` - Flowsheet construction and connectivity
- `references/property-packages.md` - Thermodynamic property modeling
- `references/unit-models.md` - Unit operation models overview
- `references/generic-models.md` - Generic model library details
- `references/power-generation.md` - Power generation specific models
- `references/gas-solid-models.md` - Gas-solid contactor models
- `references/initialization.md` - Initialization strategies and tools
- `references/solving.md` - Solving flowsheets and troubleshooting
- `references/scaling.md` - Scaling theory and application
- `references/diagnostics.md` - Model diagnostics and debugging
- `references/optimization.md` - Optimization workflows
- `references/parameter-estimation.md` - Parameter estimation and data reconciliation
- `references/costing.md` - Process economics and costing
- `references/dynamic-modeling.md` - Dynamic simulation
- `references/custom-models.md` - Developing custom models

## External Resources

- **Official docs:** https://idaes-pse.readthedocs.io/
- **GitHub:** https://github.com/IDAES/idaes-pse
- **Examples (Interactive):** https://idaes.github.io/examples-pse/latest/
- **Examples (Repository):** https://github.com/IDAES/examples-pse
- **Support:** idaes-support@idaes.org
- **Tutorials:** https://idaes-pse.readthedocs.io/en/stable/tutorials/
- **API Reference:** https://idaes-pse.readthedocs.io/en/stable/reference_guides/

## Key Differences from Other Process Simulators

**vs. Aspen Plus/HYSYS:**
- Open source and Python-based
- Full access to equations and customization
- Integration with optimization and machine learning
- Programmatic workflow automation

**vs. DWSIM:**
- More advanced optimization capabilities
- Better scaling and numerical tools
- Specialized for energy systems research
- Extensive diagnostics framework

**Strengths:**
- Equation-oriented solving
- Advanced optimization integration
- Custom model development
- Integration with Python ecosystem
- Diagnostic and scaling tools

**Considerations:**
- Requires Python programming knowledge
- Smaller library of pre-built unit models than commercial tools
- Less GUI support (primarily code-based)
- Learning curve for Pyomo framework

---

### image-generation

**Path**: `skills/creative/image-generation/SKILL.md`

# Image Generation

## Purpose

This skill enables AI-powered image generation and editing through Google's Gemini image models and OpenAI's DALL-E models. Create photorealistic images, illustrations, logos, stickers, and product mockups from natural language descriptions. Edit existing images with text instructions, apply style transfers, and refine outputs through iterative conversation.

**Attribution:** This skill is inspired by the `gemini-imagegen` skill from [Every Marketplace](https://github.com/EveryInc/every-marketplace/tree/main/plugins/compounding-engineering/skills/gemini-imagegen) by Every Inc.

## When to Use

This skill should be invoked when the user asks to:
- Generate images from text descriptions ("create an image of...", "generate a picture...")
- Create logos, icons, or stickers ("design a logo for...", "make a sticker...")
- Edit or modify existing images ("change the background to...", "add... to this image")
- Apply artistic styles or effects ("make it look like...", "stylize as...")
- Create product mockups or visualizations ("product photo of...", "mockup showing...")
- Refine or iterate on images ("make it more...", "adjust the...", "try again with...")
- Generate variations with different styles or compositions

## Available Models

### Google Gemini Models

1. **gemini-2.5-flash-image** ("Nano Banana")
   - Resolution: 1024px
   - Best for: Speed, high-volume operations, rapid iteration
   - Use when: Quick prototypes, multiple variations, time-sensitive requests

2. **gemini-3-pro-image-preview** ("Nano Banana Pro")
   - Resolution: Up to 4K
   - Best for: Professional assets, complex instructions, high quality
   - Use when: Final deliverables, detailed compositions, text-heavy designs
   - Special features: Google Search grounding, multiple reference images (up to 14)

### OpenAI DALL-E Models

3. **dall-e-3**
   - Resolution: 1024x1024, 1024x1792, 1792x1024
   - Best for: High quality, detailed images, creative interpretations
   - Use when: Artistic renders, complex scenes, natural style

4. **dall-e-2**
   - Resolution: 1024x1024, 512x512, 256x256
   - Best for: Faster generation, lower cost, simple compositions
   - Use when: Budget-conscious, simpler images, quick iterations

## Model Selection Logic

Ask the user or use this decision tree:

```
Need highest quality + complex composition?
├─ Yes → gemini-3-pro-image-preview
└─ No → Need speed/volume?
    ├─ Yes → gemini-2.5-flash-image
    └─ No → Prefer DALL-E style?
        ├─ Yes → dall-e-3
        └─ No → Budget-conscious → dall-e-2
```

If the user has specific model preference, use that. Default to `gemini-2.5-flash-image` for balanced speed/quality.

## Capabilities

1. **Text-to-Image Generation**: Create images from detailed text descriptions
2. **Image Editing**: Modify existing images with text instructions
3. **Style Transfer**: Apply artistic styles, filters, and effects
4. **Logo & Sticker Design**: Generate branded assets with specific styles
5. **Product Mockups**: Create professional product photography and presentations
6. **Multi-turn Refinement**: Iteratively improve images through conversation
7. **Aspect Ratio Control**: Generate images in various formats (square, portrait, landscape, wide)
8. **Reference-based Generation**: Use existing images as compositional references (Gemini Pro)

## Instructions

### Step 1: Understand the Request

Analyze the user's request to determine:
- **Type**: Text-to-image, image editing, style transfer, logo/sticker, mockup
- **Subject**: What should be in the image
- **Style**: Photorealistic, illustration, artistic, minimalist, etc.
- **Details**: Colors, lighting, composition, mood, specific elements
- **Format**: Aspect ratio, resolution requirements
- **Urgency**: Speed vs. quality trade-off

### Step 2: Select Model

Based on requirements:
- **High quality + complexity** → `gemini-3-pro-image-preview`
- **Speed + iterations** → `gemini-2.5-flash-image`
- **DALL-E preference** → `dall-e-3` or `dall-e-2`

If unclear, use `AskUserQuestion` tool to clarify model preference.

### Step 3: Craft Effective Prompt

Build a detailed prompt following these patterns:

**For Photorealistic Images:**
```
[Subject], [camera details], [lighting], [mood/atmosphere], [composition]

Example: "Close-up portrait of a woman, 85mm lens, soft golden hour lighting,
serene mood, shallow depth of field, professional photography"
```

**For Illustrations/Art:**
```
[Subject], [art style], [color palette], [details], [mood]

Example: "Kawaii cat sticker, bold black outlines, cel-shading, pastel colors,
cute expression, chibi style"
```

**For Logos:**
```
[concept], [style], [elements], [colors], [context]

Example: "Tech startup logo, minimalist geometric design, abstract network nodes,
blue and silver gradient, professional, vector style"
```

**For Product Photography:**
```
[product], [setting], [lighting], [presentation], [context]

Example: "Wireless earbuds, white background, studio lighting, 3/4 angle view,
clean minimal composition, e-commerce product shot"
```

**Key principles:**
- Be specific and detailed
- Include lighting, composition, and mood
- Specify style clearly (photorealistic, illustration, etc.)
- Mention camera/lens for photorealistic (85mm, wide angle, macro)
- For text in images, use Pro model and specify exact text

### Step 4: Implement API Call

#### For Gemini Models:

```python
import google.generativeai as genai
from pathlib import Path

# Configure API
genai.configure(api_key="GEMINI_API_KEY")

# Select model
model_name = "gemini-2.5-flash-image"  # or gemini-3-pro-image-preview
model = genai.GenerativeModel(model_name)

# Basic text-to-image
response = model.generate_content(
    prompt_text,
    generation_config={
        "response_modalities": ["TEXT", "IMAGE"],
        # Optional configurations:
        # "aspect_ratio": "1:1",  # 1:1, 3:4, 4:3, 9:16, 16:9, 21:9
        # "image_size": "1K",     # 1K, 2K, 4K (Pro only)
    }
)

# Extract and save image
for part in response.parts:
    if part.inline_data:
        image_data = part.inline_data.data
        Path("output.png").write_bytes(image_data)

# For image editing (pass existing image):
from PIL import Image

image = Image.open("input.png")
response = model.generate_content(
    [image, "Make the background a sunset scene"],
    generation_config={"response_modalities": ["TEXT", "IMAGE"]}
)

# For multi-turn refinement (use chat):
chat = model.start_chat()
response1 = chat.send_message(
    "A futuristic city skyline",
    generation_config={"response_modalities": ["TEXT", "IMAGE"]}
)
response2 = chat.send_message(
    "Add more neon lights and flying cars",
    generation_config={"response_modalities": ["TEXT", "IMAGE"]}
)
```

#### For DALL-E Models:

```python
from openai import OpenAI

client = OpenAI(api_key="OPENAI_API_KEY")

# DALL-E 3 generation
response = client.images.generate(
    model="dall-e-3",
    prompt=prompt_text,
    size="1024x1024",  # or "1024x1792", "1792x1024"
    quality="standard",  # or "hd"
    n=1,
)

image_url = response.data[0].url

# Download and save
import requests
from pathlib import Path

image_data = requests.get(image_url).content
Path("output.png").write_bytes(image_data)

# DALL-E 2 generation
response = client.images.generate(
    model="dall-e-2",
    prompt=prompt_text,
    size="1024x1024",  # or "512x512", "256x256"
    n=1,
)

# For variations (DALL-E 2 only)
response = client.images.create_variation(
    image=open("input.png", "rb"),
    n=2,
    size="1024x1024"
)
```

**Implementation approach:**
- Use `Bash` tool to execute Python scripts with API calls
- Check for API keys in environment variables
- Handle errors gracefully (API limits, invalid prompts, etc.)
- Save images with descriptive filenames
- Report image location to user

### Step 5: Handle Output

1. **Save the generated image** to an appropriate location
2. **Verify the output** meets the request
3. **Show the user** the saved file path
4. **Offer refinement** if the result isn't quite right
5. **Explain the prompt used** so the user understands the generation

### Step 6: Iterate if Needed

If the user wants changes:
- For Gemini: Use chat interface to maintain context
- For DALL-E: Generate new image with updated prompt
- Keep previous versions for comparison
- Suggest specific adjustments based on the current result

## Requirements

**API Keys:**
- Google Gemini: Set `GOOGLE_API_KEY` or `GEMINI_API_KEY` environment variable
- OpenAI: Set `OPENAI_API_KEY` environment variable

**Python Packages:**
```bash
pip install google-generativeai openai pillow requests
```

**System:**
- Python 3.8+
- Internet connection for API access
- Write permissions for saving images

## Best Practices

### Prompt Engineering

1. **Be Specific**: Vague prompts produce inconsistent results
   - Bad: "a nice landscape"
   - Good: "mountain valley at sunrise, mist over lake, pine trees, warm golden light, peaceful atmosphere"

2. **Include Technical Details** for photorealism:
   - Camera: "shot on 85mm lens", "wide angle 24mm", "macro photography"
   - Lighting: "golden hour", "studio lighting", "rim light", "soft diffused"
   - Quality: "high resolution", "detailed", "sharp focus", "professional photography"

3. **Specify Style Clearly**:
   - "photorealistic", "oil painting", "watercolor", "digital art", "3D render"
   - "minimalist", "detailed", "abstract", "realistic", "stylized"
   - "anime style", "pixel art", "vector art", "charcoal sketch"

4. **Use Examples and References**:
   - "in the style of [artist/art movement]"
   - "similar to [known visual reference]"
   - For Gemini Pro: Provide actual reference images

5. **Negative Prompts** (what to avoid):
   - DALL-E doesn't support negative prompts directly
   - For Gemini, phrase as positive instructions: "clear sky" vs "no clouds"

### Model-Specific Tips

**Gemini Flash (2.5):**
- Ideal for rapid iteration and exploration
- Good for generating multiple variations quickly
- Use for draft/concept phase

**Gemini Pro (3):**
- Use for final deliverables
- Better at rendering text in images
- Leverage Google Search grounding for current events/real places
- Provide multiple reference images for complex compositions

**DALL-E 3:**
- Excellent at understanding natural language
- Strong at creative interpretations
- Good default for artistic/illustrative styles
- Prompt expansion happens automatically

**DALL-E 2:**
- More literal interpretation of prompts
- Good for controlled, predictable outputs
- Can generate variations of existing images
- More budget-friendly

### Quality Guidelines

1. **Start with clear requirements**: Ask clarifying questions before generating
2. **Choose appropriate model**: Match model capabilities to requirements
3. **Iterate thoughtfully**: Make specific changes rather than complete regeneration
4. **Save intermediate versions**: Keep promising iterations
5. **Respect usage policies**: Follow content policies for each platform
6. **Credit the tool**: Disclose AI-generated images when sharing

### Error Handling

- **API key missing**: Prompt user to set environment variable
- **Invalid prompt**: Suggest refinements, check content policy
- **Rate limits**: Inform user and suggest retry timing
- **Generation failure**: Try simpler prompt or different model
- **Unsatisfactory result**: Offer to regenerate with adjusted prompt

## Examples

### Example 1: Logo Design

**User request:** "Create a logo for a coffee shop called 'Morning Brew'"

**Expected behavior:**
1. Ask user about style preference (modern, vintage, minimalist, etc.)
2. Ask about color preferences
3. Select model (gemini-3-pro-image-preview for vector-style quality)
4. Generate with prompt: "Coffee shop logo for 'Morning Brew', minimalist modern design,
   coffee cup with steam forming sunrise rays, warm brown and orange colors,
   clean professional aesthetic, vector style, white background"
5. Save image and show path
6. Offer to generate variations with different styles

### Example 2: Product Photography

**User request:** "Generate product photos of wireless earbuds"

**Expected behavior:**
1. Select model (gemini-2.5-flash-image for speed, or dalle-3 for realism)
2. Generate with prompt: "Wireless earbuds product photography, white background,
   professional studio lighting, 3/4 angle view showing charging case and earbuds,
   clean minimal composition, high resolution, sharp focus, e-commerce quality"
3. Generate additional angles if requested
4. Save all versions

### Example 3: Illustration

**User request:** "Create a cute sticker of a robot"

**Expected behavior:**
1. Select model (gemini-2.5-flash-image for illustration)
2. Generate with prompt: "Cute robot sticker, kawaii style, bold black outlines,
   cel-shading, pastel blue and silver colors, big friendly eyes, rounded shapes,
   chibi proportions, white border, transparent background suitable for sticker"
3. Save and offer variations

### Example 4: Image Editing

**User request:** "Change the background of this photo to a beach sunset"

**Expected behavior:**
1. Use `Read` tool to load the existing image
2. Select Gemini model (supports image input)
3. Generate with image + prompt: "Change the background to a beautiful beach at sunset,
   golden hour lighting, warm colors, ocean and palm trees visible, maintain the subject
   in foreground, seamless composition"
4. Save edited image

### Example 5: Iterative Refinement

**User request:** "Generate a futuristic city" → "Add more neon lights" → "Make it rain"

**Expected behavior:**
1. First generation: "Futuristic city skyline, towering skyscrapers, advanced architecture,
   night scene, detailed, cinematic lighting"
2. Use Gemini chat interface to maintain context
3. Second refinement: "Add vibrant neon lights throughout the city, cyberpunk aesthetic,
   glowing signs and billboards"
4. Third refinement: "Add rain effect, wet streets reflecting neon lights, atmospheric,
   moody"
5. Save each version with descriptive names

## Limitations

1. **Content Policies**: Both Gemini and DALL-E have content restrictions (no violence,
   explicit content, copyrighted characters, real people without consent)
2. **Text Rendering**: Text in images can be inconsistent; Gemini Pro performs better
3. **Photorealism of People**: May not perfectly capture specific facial features
4. **Complex Compositions**: Very complex scenes may need multiple iterations
5. **Consistency**: Hard to maintain exact consistency across multiple generations
6. **Real-time Events**: Results may not reflect very recent events (use Gemini Pro Search
   grounding for current topics)
7. **API Costs**: Be mindful of usage; Pro models and high resolutions cost more
8. **Rate Limits**: APIs have rate limits; may need to wait between requests

## Related Skills

- `python-plotting` - For data visualization and charts
- `brainstorming` - For ideating visual concepts
- `scientific-writing` - For figure captions and documentation
- `python-best-practices` - For writing clean API integration code

## Additional Resources

- **Gemini API Documentation**: https://ai.google.dev/gemini-api/docs/vision
- **DALL-E API Documentation**: https://platform.openai.com/docs/guides/images
- **Prompt Engineering Guide**: See `references/prompt-engineering.md`
- **Model Comparison**: See `references/gemini-models.md` and `references/dalle-models.md`

---

### literature-review

**Path**: `skills/research/literature-review/SKILL.md`

# Scientific Literature Review Skill

## Overview

This skill guides comprehensive, systematic literature reviews that combine automated search capabilities across multiple academic databases with iterative analysis and synthesis into well-structured reports. It transforms the literature review process from a manual, time-consuming task into an efficient, systematic, and reproducible research methodology.

## When to Use This Skill

Use this skill when you need to:

- **Comprehensive Knowledge Synthesis**: Gather and integrate knowledge across multiple research domains or methodologies
- **Research Gap Identification**: Identify underexplored areas, conflicting findings, or methodological limitations
- **Methodology Tracking**: Understand the evolution and current state of research methodologies and techniques
- **Evidence Evaluation**: Assess the quality, reliability, and significance of research findings
- **Literature-Supported Writing**: Write evidence-based sections for papers, grants, dissertations, or technical reports
- **Research Foundation**: Build a solid foundation for new research by understanding prior work
- **Systematic Assessment**: Conduct systematic reviews, meta-analyses, or structured literature analyses
- **Knowledge Documentation**: Create comprehensive, citable references for complex research domains

## Literature Review Types

### 1. Narrative Literature Reviews
- **Purpose**: Provide comprehensive overview of a research topic
- **Scope**: Broad, subjective assessment of literature
- **Search Strategy**: Exploratory, iterative search across multiple angles
- **Report Structure**: Thematic organization by research concepts
- **Best For**: Introducing new research areas, providing context for research questions
- **Timeline**: 4-12 weeks for comprehensive review

### 2. Systematic Literature Reviews
- **Purpose**: Answer specific research questions using explicit methodology
- **Scope**: Comprehensive, predefined search with inclusion/exclusion criteria
- **Search Strategy**: Exhaustive search of specified databases
- **Report Structure**: PRISMA-compliant with search strategy documentation
- **Best For**: Evidence synthesis, clinical decision-making, policy development
- **Timeline**: 3-12 months for comprehensive review

### 3. Meta-Analyses
- **Purpose**: Quantitative synthesis of comparable study results
- **Scope**: Systematic review with statistical analysis
- **Search Strategy**: Identical to systematic reviews
- **Report Structure**: Statistical analysis with forest plots and funnel plots
- **Best For**: Combining results from multiple studies to determine overall effect
- **Timeline**: 6-18 months

### 4. Scoping Reviews
- **Purpose**: Map research landscape, identify key concepts and gaps
- **Scope**: Broader than systematic reviews, flexible inclusion criteria
- **Search Strategy**: Iterative, including grey literature
- **Report Structure**: Overview of research landscape with gap identification
- **Best For**: Emerging research areas, protocol development
- **Timeline**: 2-6 months

## Systematic Literature Review Workflow

### Phase 1: Planning and Question Formulation

#### Step 1: Define Research Question
Transform broad research interests into specific, answerable questions using PICO/PEO framework:

- **P**opulation/Problem: Who/what is the focus? (e.g., patients, methodologies, organisms)
- **I**ntervention/Indicator: What is being studied? (e.g., treatment, technique, technology)
- **C**omparison: What alternative is compared? (optional, but recommended)
- **O**utcome: What results/impacts matter? (e.g., effectiveness, efficiency, validity)
- **E**xperience (for qualitative): What are participants' perspectives?

**Example PICO Questions:**
- "What is the effectiveness of deep learning methods compared to traditional machine learning for medical image analysis in cancer detection?"
- "What methodologies are used to assess uncertainty in computational chemistry predictions?"
- "How do research groups address reproducibility in high-throughput screening studies?"

#### Step 2: Establish Scope and Criteria

**Define Inclusion/Exclusion Criteria:**
- Document type (peer-reviewed journals, preprints, grey literature)
- Time period (publication date range)
- Language (English-only or other languages)
- Study design (specific methodologies, approaches)
- Population/scope (specific organisms, systems, technologies)
- Quality thresholds (minimum standards for inclusion)

**Example Inclusion Criteria:**
- Peer-reviewed research articles published 2015-2025
- English language publications
- Empirical studies with quantitative or qualitative methodology
- Focus on computational methods in materials science
- Articles with >10 citations or published in high-impact journals (alternative: no citation threshold for very recent work)

#### Step 3: Plan Search Strategy

**Select Appropriate Databases:**
- **PubMed/MEDLINE**: Biomedical and life sciences literature
- **arXiv**: Preprints in physics, mathematics, computer science, statistics
- **Web of Science**: Multidisciplinary citation index with impact metrics
- **Scopus**: Large multidisciplinary abstract and citation database
- **IEEE Xplore**: Engineering and computer science literature
- **ACM Digital Library**: Computer science and information technology
- **Springer Link**: Multidisciplinary academic publisher
- **ProQuest**: Dissertations, theses, and comprehensive database
- **Google Scholar**: Broad academic search (verify findings in other databases)
- **Domain-Specific Repositories**: ArXiv (physics), bioRxiv (biology), chemRxiv (chemistry), SSRN (social sciences)

**Formulate Search Queries:**
- Use Boolean operators (AND, OR, NOT)
- Include synonyms and related terms
- Use wildcards and truncation (* for variations)
- Apply field-specific search syntax for each database
- Test query effectiveness before scaling

**Example Boolean Queries:**
- "machine learning" AND ("medical imaging" OR "diagnosis") AND ("cancer" OR "oncology")
- ("deep learning" OR "neural network*") NOT "toy dataset*"
- "materials discovery" AND (high-throughput OR computational) AND (screening OR optimization)

#### Step 4: Prepare Documentation System

Create a searchable, organized system for tracking:
- Search strategies and queries used
- Databases searched and date ranges
- Number of results per search
- Selection decisions and reasoning
- Paper characteristics (methodology, quality, relevance)
- Extracted data and findings
- Progress tracking and milestones

Recommended tools and formats:
- Spreadsheet/database: Track all papers with metadata
- Reference manager: Store full citations and PDFs (Zotero, Mendeley, EndNote)
- Note-taking system: Detailed notes on each paper
- Search log: Document all searches performed

### Phase 2: Initial Broad Search

#### Step 5: Execute Broad Searches

**Search across selected databases:**
1. Start with primary query formulation
2. Execute search in each database
3. Limit results (usually by date range, document type)
4. Export results with complete metadata
5. Document search parameters and result counts

**Expected workflow:**
- Initial searches typically return 100-5,000+ results depending on topic breadth
- For broad topics, expect 500-5,000 results
- For narrow topics, expect 50-500 results
- If results exceed 10,000, refine search strategy

**Documentation:**
```
Search #1: "machine learning" AND "medical imaging"
Database: PubMed
Date: 2025-01-15
Results: 2,847 papers
Time period: 2015-2025
Filters: English language, human studies excluded at this stage
Export: PubMed format with abstracts
Notes: High number of results requires refinement
```

#### Step 6: Initial Screening

**Level 1: Title and Abstract Screening**

Review titles and abstracts to identify potentially relevant papers:

Inclusion criteria:
- Research question directly related
- Appropriate study type/design
- Relevant population/scope
- Outcome measures align with review objectives

Exclusion criteria:
- Clearly outside research scope
- Wrong study type (e.g., looking for empirical research but paper is opinion/editorial)
- Duplicate publications
- Conference abstracts without full papers (unless specified in protocol)

**Process:**
- Read title first (quick elimination of obviously irrelevant papers)
- Read abstract to verify relevance
- When uncertain, include rather than exclude (error on side of inclusion)
- Use systematic approach: every paper evaluated by same criteria
- Document decisions: Include, Exclude, or Uncertain

**Expected results:**
- Broad searches: typically 10-30% of papers pass initial screening
- Refined searches: typically 30-60% pass
- If <5% pass, search strategy may be too narrow or inclusion criteria too strict

**Tracking:**
```
Database: PubMed
Initial results: 2,847
Title screening: 1,200 eliminated (obviously off-topic)
Abstract screening: 847 eliminated (wrong study type, unclear relevance)
Potentially relevant: 800 papers
Pass rate: 28%
```

### Phase 3: Iterative Search Refinement

#### Step 7: Concept and Keyword Extraction

From the initially relevant papers (Phase 2), identify:

**New keywords and concepts:**
- Terminology variations used by different research groups
- Specific methodologies mentioned repeatedly
- Technical terms and specialized vocabulary
- Author names appearing frequently
- Key research institutions/groups
- Specific journal names publishing in this area

**Analysis method:**
- Read abstracts and introductions of included papers
- Note any keywords not in original search strategy
- Identify methodological approaches (e.g., specific algorithms, experimental designs)
- Document emerging themes and subtopics

**Example extraction:**
```
Original search: "machine learning" + "medical imaging"
Extracted concepts:
- Specific methodologies: convolutional neural networks, attention mechanisms, transformer models
- Medical applications: radiotherapy planning, pathology analysis, diagnostic support
- Technical terms: semi-supervised learning, transfer learning, domain adaptation
- Related areas: data augmentation, interpretability, fairness in AI
- Key authors: [list of frequently appearing researchers]
- Important journals: IEEE TMI, Medical Image Analysis, Nature Medicine
```

#### Step 8: Citation Mining and Author Tracking

**Follow reference trails:**
1. Review reference lists of included papers
2. Identify papers cited multiple times (signal importance)
3. Retrieve and evaluate highly-cited references
4. Look for foundational/seminal papers in field

**Author tracking:**
1. Identify key researchers with multiple papers in review
2. Search for recent publications by these authors
3. Review their recent works not captured in initial searches
4. Check co-authors for related research

**Citation tracking (forward citation searching):**
1. Use Web of Science, Scopus, or Google Scholar
2. Find papers that cite key included papers
3. Evaluate citing papers for relevance
4. Identify recent developments building on earlier work

**Process:**
```
Key paper: "Deep Learning for Medical Image Analysis" (Smith et al., 2020)
Citation count: 1,247 (as of Jan 2025)
Recent citing papers (2023-2025): 342
Evaluate: Random sample or all, depending on review scope
References cited: 198 papers
New papers identified: 23 additional relevant papers
```

#### Step 9: Targeted Methodology-Based Searches

Based on identified methodologies, conduct focused searches:

**Examples:**
- Original: "machine learning" + "medical imaging"
- Refined searches:
  - "convolutional neural networks" + "radiology"
  - "attention mechanisms" + "medical images"
  - "transfer learning" + "diagnostic imaging"
  - "federated learning" + "clinical data"

**Process:**
1. Take each identified methodology
2. Combine with population/domain from original question
3. Execute new searches in selected databases
4. Screen results using same inclusion/exclusion criteria
5. Integrate new papers into literature base

#### Step 10: Gap Identification and Targeted Search

Identify underexplored areas:

**Questions to investigate:**
- "What specific applications are missing?"
- "Are there contradictory findings in any area?"
- "What methodological approaches haven't been compared?"
- "What populations/domains are underrepresented?"
- "What time periods show publication gaps?"

**Targeted searches:**
- Searches designed to find papers on identified gaps
- May use different search strategies (e.g., seeking negative results, null findings)
- Searches for specific applications or populations
- Temporal searches (recent developments)

**Convergence assessment:**
- Monitor if each new search finds mostly previously identified papers
- Track percentage of truly new papers with each search iteration
- Indicate saturation when <10-15% of results are new

#### Step 11: Temporal Analysis

**Track research evolution:**
1. Group papers by publication year
2. Identify trends in methodology adoption
3. Note shifts in research focus or populations studied
4. Identify recently emerging topics

**Visualization:**
```
2015: 15 papers (foundational period)
2016: 23 papers (growth phase)
2017: 45 papers (expansion)
2018: 89 papers (mainstream adoption)
2019: 156 papers (rapid growth)
2020: 289 papers (explosion due to...)
2021: 312 papers (continued growth)
2022: 298 papers (plateau)
2023: 276 papers (slight decline in pure methodology papers)
2024: 214 papers (applications and variants increasing)
2025: 127 papers (partial year, trend toward...)
```

### Phase 4: Full-Text Review and Data Extraction

#### Step 12: Full-Text Screening

**Level 2: Detailed Full-Text Review**

For all papers passing abstract screening:

1. Obtain full text (use university library, ResearchGate, contact authors)
2. Read complete paper carefully
3. Apply detailed inclusion/exclusion criteria
4. Extract structured data
5. Assess quality

**Detailed screening process:**
- Read introduction for research context and questions
- Review methodology for study design and quality
- Examine results for outcome measures
- Assess conclusions for validity and scope
- Document reasons for exclusion if not included

**Tracking results:**
```
Papers for full-text review: 800
Full text obtained: 775 (97%)
Unable to obtain: 25 (contact authors or wait)
After full-text screening: 312 included
Excluded (with reasons):
  - Wrong study design: 156
  - No relevant outcomes: 189
  - No original data: 78
  - Data quality concerns: 40
  - Duplicate/updated version: 22
```

#### Step 13: Structured Data Extraction

Create standardized extraction form for consistent data collection:

**Bibliographic information:**
- Authors, year, journal, volume/issue, pages, DOI
- Publication type (original research, review, methodological)
- Journal impact factor (if relevant)

**Study characteristics:**
- Study design/methodology
- Population/scope (sample size, characteristics, organisms/systems studied)
- Intervention/methodology tested
- Comparison groups (if applicable)
- Study duration and setting

**Results and findings:**
- Primary outcomes reported
- Quantitative results (effect sizes, p-values, confidence intervals)
- Qualitative findings (themes, patterns)
- Statistical analysis methods used
- Quality metrics and limitations acknowledged

**Quality assessment:**
- Design quality (randomization, blinding, sample size calculation)
- Data quality (missing data, response rates, measurement validity)
- Bias assessment (selection bias, performance bias, reporting bias)
- Generalizability/applicability
- Reproducibility assessment

**Thematic coding:**
- Primary research theme(s)
- Secondary themes
- Methodological approach(es)
- Key contributions
- Relevance to review objectives

**Extraction format:**
```
---
Authors: Smith, J., Jones, M.
Year: 2023
Title: [Full title]
Journal: Nature Medicine
Impact Factor: 73.5
DOI: 10.1038/xxxxx

Study Design: Randomized controlled trial
Population: Adult patients with diagnosis [X], n=2,847
Intervention: Treatment A (n=1,424)
Comparison: Standard treatment (n=1,423)
Primary Outcome: Reduction in symptom severity
Results: 45% reduction (95% CI: 38-52%) vs 18% (95% CI: 12-24%), p<0.001
Effect Size: Cohen's d = 1.2 (large effect)

Quality: High (GRADE rating: A)
Key Limitations: Single-center study, limited demographic diversity
Reproducibility: Good (methods detailed, data availability statement: Yes)

Themes: Treatment efficacy, Clinical outcomes, Comparative effectiveness
Innovation Level: Incremental (application of known methodology to new population)

---
```

### Phase 5: Synthesis and Analysis

#### Step 14: Thematic Organization

Group papers by key themes and organize findings:

**Organizational approaches:**

1. **By Methodology:**
   - Group papers using similar approaches
   - Compare methodological strengths/limitations
   - Track methodology evolution over time

2. **By Research Question/Aspect:**
   - Organize around components of research question (PICO elements)
   - Compare how different studies address each component
   - Integrate findings to answer overall question

3. **By Chronological Development:**
   - Show how ideas evolved
   - Track technological/methodological improvements
   - Highlight paradigm shifts

4. **By Research Group/Tradition:**
   - Organize by major research groups or schools of thought
   - Compare different theoretical frameworks
   - Note collaborations and conflicts

5. **By Population/Application Domain:**
   - Different populations studied
   - Different application contexts
   - Geographic or demographic patterns

**Example thematic map:**
```
Main Theme: Applications of Deep Learning in Medical Imaging

Subtopic 1: Cancer Diagnosis
├─ Breast cancer detection
│  ├─ Mammography analysis (45 papers)
│  ├─ Ultrasound (12 papers)
│  └─ MRI analysis (8 papers)
├─ Lung cancer detection (34 papers)
├─ Colorectal cancer (18 papers)
└─ Other cancers (22 papers)

Subtopic 2: Treatment Planning
├─ Radiotherapy planning (28 papers)
├─ Surgical guidance (15 papers)
└─ Patient monitoring (8 papers)

Subtopic 3: Risk Stratification
├─ Prognostic models (31 papers)
└─ Predictive biomarkers (12 papers)
```

#### Step 15: Quality Assessment and Bias Detection

**Quality assessment methodology:**

For each research domain, use established quality assessment tools:
- **Quantitative studies**: GRADE, Cochrane Risk of Bias, ROBINS-I
- **Qualitative studies**: CASP Qualitative Checklist, STROBE-Q
- **Methodological studies**: STROBE, reporting checklists
- **Model/Algorithm papers**: Methodological rigor assessment

**Bias detection:**

1. **Publication bias**
   - Papers showing positive results more likely to be published
   - Searches for unpublished/negative studies
   - Statistical tests: funnel plots, Egger's test
   - Gray literature search

2. **Selection bias**
   - Populations studied may not be representative
   - Note demographic patterns in papers
   - Identify missing populations/applications
   - Track over/underrepresented areas

3. **Methodological bias**
   - Study design limitations
   - Sample size and power considerations
   - Measurement validity
   - Control for confounders

4. **Reporting bias**
   - Selective outcome reporting
   - Favorable vs. unfavorable results emphasis
   - Statistical significance bias (p-hacking)
   - Effect size reporting consistency

**Quality summary:**
```
Quality Assessment Results (80 papers):
High quality (GRADE: A): 12 papers (15%)
Good quality (GRADE: B): 28 papers (35%)
Fair quality (GRADE: C): 32 papers (40%)
Poor quality (GRADE: D): 8 papers (10%)

Bias Assessment:
- Publication bias: Moderate (positive results overrepresented)
- Selection bias: Low (diverse populations generally well-represented)
- Methodological bias: Moderate (sample size, blinding concerns)
- Reporting bias: Moderate (selective outcome reporting in 40% of papers)

Key limitation: Predominantly English-language journals (may miss non-English research)
```

#### Step 16: Evidence Synthesis

Create integrated summary of evidence:

**Types of synthesis:**

1. **Narrative synthesis**
   - Written summary of findings organized thematically
   - Discussion of agreement/disagreement between studies
   - Integration of quantitative and qualitative findings
   - Assessment of strength of evidence

2. **Meta-analysis (if appropriate)**
   - Statistical combination of comparable results
   - Analysis by subgroup (methodology, population, etc.)
   - Assessment of heterogeneity
   - Sensitivity analysis

3. **Framework synthesis**
   - Organize findings using conceptual framework
   - Map concepts and relationships
   - Show how different findings interconnect
   - Identify patterns and principles

4. **Critical interpretive synthesis**
   - Deep interpretation of findings
   - Identify underlying assumptions and interpretations
   - Reconcile contradictory findings
   - Build new understanding through integration

**Synthesis questions to address:**
- What do we know (consensus findings)?
- What do we NOT know (gaps)?
- What conflicts exist (controversial or contradictory findings)?
- What are implications for practice/future research?
- What quality/strength of evidence supports conclusions?
- What remains uncertain or debated?

**Evidence summary table example:**
```
| Topic | Consensus Finding | Evidence Strength | Conflicting Evidence | Notes |
|-------|------------------|-------------------|---------------------|-------|
| Effectiveness | 70-85% success rate | Strong (24 RCTs) | Effectiveness varies by subtype (40-90%) | Context-dependent |
| Mechanism | Works via pathway X | Moderate (12 studies) | Some evidence for pathway Y (3 studies) | Need more mechanistic work |
| Best practice | Method A superior | Moderate (8 trials) | Method B equivalent in 2 trials | Population and context matter |
| Safety | Well-tolerated overall | Strong (1,200+ patients) | 5-10% adverse events in vulnerable populations | More data needed on elderly |
```

#### Step 17: Identify Gaps and Future Directions

**Research gaps identified through review:**

1. **Knowledge gaps**
   - Questions not yet addressed by research
   - Populations/contexts not yet studied
   - Outcomes not yet measured
   - Mechanisms not yet understood

2. **Methodological gaps**
   - Inadequate study designs for certain questions
   - Lack of comparative studies
   - Gaps in measurement approaches
   - Need for larger/longer studies

3. **Application gaps**
   - Research not translated to practice
   - Implementation challenges not addressed
   - Real-world effectiveness not studied
   - Access/equity issues underexplored

4. **Conflict/uncertainty areas**
   - Contradictory findings needing resolution
   - Debated interpretations
   - Emerging evidence not yet synthesized

**Gap identification process:**
- List what questions remain unanswered
- Note populations not well-represented in literature
- Identify methodological approaches not yet compared
- Document inconsistent findings
- Highlight emerging/underexplored topics

**Future research recommendations:**
```
Priority gaps identified:

1. HIGH PRIORITY - Unaddressed populations
   Gap: Limited research on [specific population]
   Current evidence: [brief summary]
   Recommended study: Prospective cohort study in [population]
   Rationale: [explanation]

2. HIGH PRIORITY - Methodological comparison
   Gap: No direct comparison of Method A vs. Method B
   Current evidence: [separate evidence for each]
   Recommended study: Randomized comparison trial
   Rationale: [explanation]

3. MEDIUM PRIORITY - Mechanism clarification
   Gap: Exact mechanism of action unclear
   Current evidence: [partial evidence]
   Recommended study: Mechanistic studies using [approach]
   Rationale: [explanation]

4. EMERGING AREA - Novel applications
   Gap: New applications identified but not yet studied
   Current evidence: [anecdotal/preliminary evidence]
   Recommended study: Feasibility study
   Rationale: [explanation]
```

### Phase 6: Report Writing and Organization

#### Step 18: Report Structure and Organization

**For Narrative Literature Review:**

1. **Executive Summary** (1 page)
   - Research scope and objectives
   - Key findings and consensus
   - Major gaps and recommendations

2. **Introduction** (2-3 pages)
   - Problem statement and significance
   - Research questions/objectives
   - Scope and relevance

3. **Methods** (1-2 pages)
   - Search strategy and databases
   - Inclusion/exclusion criteria
   - Data extraction and analysis approach
   - Study quality assessment method

4. **Results** (variable, typically 5-10 pages)
   - Literature search and selection process (with flow diagram)
   - Characteristics of included studies
   - Study quality and bias assessment
   - Findings organized thematically

5. **Thematic Sections** (variable, typically 8-20 pages)
   - Current state of knowledge on each theme
   - Key methodologies and approaches
   - Major findings and areas of consensus
   - Conflicting results and areas of debate
   - Recent developments and emerging trends

6. **Synthesis and Discussion** (3-5 pages)
   - Integration of findings across themes
   - Identification of patterns, trends, and paradigm shifts
   - Assessment of research quality and reliability
   - Critical analysis of methodological approaches
   - Implications for theory, practice, and future research

7. **Gaps and Future Directions** (1-2 pages)
   - Identified research gaps
   - Methodological limitations
   - Suggested research priorities
   - Barriers to translation/implementation (if applicable)

8. **Conclusions** (1 page)
   - Summary of key insights
   - Implications for research and practice
   - Recommendations
   - Final synthesis statement

9. **References** (formatted bibliography)
   - Complete citation information
   - Organized by theme (optional)
   - Hyperlinks to DOIs (if digital format)

**For Systematic Literature Review:**

Follow PRISMA 2020 guidelines with additional elements:
- Inclusion/exclusion criteria fully documented
- Detailed search strategy for every database
- PRISMA flow diagram
- Meta-analysis (if appropriate)
- Risk of bias assessment for each study
- Certainty of evidence assessment (GRADE)
- Extended appendices with tables of study characteristics

#### Step 19: Citation Management

**Citation format selection:**

Choose appropriate format for your discipline:
- **APA**: Social sciences, psychology, education
- **Chicago/Notes-Bibliography**: History, humanities
- **IEEE**: Engineering, computer science
- **Nature**: Natural sciences, biology
- **Science**: Scientific research
- **MLA**: Literature, humanities
- **OSCOLA**: Law

**In-text citation approaches:**
- Numbered system: (1), (2), (3)...
- Author-date system: (Smith, 2023)
- Footnote system: Superscript numbers with notes

**Reference list organization:**
- Alphabetical (typical)
- By thematic section
- By methodological approach
- By publication date

**Citation accuracy checklist:**
- All cited papers listed in references
- Reference information complete and accurate
- Formatting consistent throughout
- URLs and DOIs functional
- Access dates included (for websites, if required)

#### Step 20: Report Quality Assurance

**Logical flow and coherence:**
- Each section connects to research question
- Transitions between sections smooth
- Overall narrative logical and clear
- Conclusions follow from evidence

**Balanced perspective:**
- All major viewpoints represented
- Contradictory findings acknowledged
- Limitations transparently discussed
- No over-emphasis of favored interpretations

**Appropriate detail level:**
- Key studies discussed in detail
- Minor studies appropriately summarized
- Methods described at level appropriate for audience
- Results clearly explained without unnecessary statistics

**Clear distinctions:**
- Established facts vs. interpretations clearly marked
- Author's analysis vs. findings from reviewed literature clear
- Consensus vs. minority views distinguished
- Evidence strength indicated

### Phase 7: Output and Dissemination

#### Step 21: Multiple Output Formats

**Markdown format** (for collaborative editing, version control)
```markdown
# Literature Review: [Title]

## Executive Summary
...

## Introduction
...

## Methods
...

## Results
...
```

**LaTeX format** (for academic submission, journal publication)
```latex
\documentclass{article}
\usepackage{natbib}

\title{Literature Review: ...}
\author{...}

\begin{document}

\section{Executive Summary}
...

\end{document}
```

**Word document** (for institutional requirements, collaborative editing)
- Export from markdown/LaTeX
- Format with institutional templates
- Track changes for feedback

**HTML for web publication**
- Interactive tables of contents
- Hyperlinked references
- Searchable content
- Figure galleries

#### Step 22: Citation Data Management

**Export formats:**

- **BibTeX** (.bib): For LaTeX documents
- **RIS** (.ris): For reference managers
- **CSL JSON**: For citation processing
- **CSV**: For data analysis and spreadsheets

**Citation database integration:**
- Zotero: Open-source, browser integration
- Mendeley: Commercial, cloud-based
- EndNote: Enterprise solution
- RefWorks: Cloud-based institutional solution

**Search history documentation:**
- All databases searched
- Queries used
- Date ranges
- Number of results
- Refinements made
- Rationale for changes

## Best Practices for Literature Reviews

### Search Strategy Best Practices

1. **Start broad, then refine**
   - Initial searches cast wide net
   - Analyze results to identify concepts
   - Refine searches based on learning
   - Converge toward saturation

2. **Document everything**
   - Record every search executed
   - Note parameters and dates
   - Keep search logs
   - Enable reproducibility and auditing

3. **Use multiple strategies**
   - Boolean combinations
   - Citation mining
   - Author searching
   - Keyword and phrase variations

4. **Cross-database verification**
   - Important papers should appear in multiple databases
   - Compare top papers across databases
   - Use cross-database differences to identify missed areas

5. **Combine automated and manual searching**
   - Automated searches for breadth
   - Manual browsing of key journals
   - Citation tracking for depth
   - Expert consultation for validation

### Content Analysis Best Practices

1. **Structured extraction**
   - Use standardized forms
   - Consistency checks
   - Double-checking of key papers
   - Inter-rater reliability assessment (if team effort)

2. **Quality assessment**
   - Use published, validated tools
   - Assess all papers using same criteria
   - Document quality concerns
   - Weight evidence by quality

3. **Bias awareness**
   - Acknowledge reviewer biases
   - Explicit inclusion/exclusion criteria
   - Sensitivity analyses
   - Transparent decision-making

4. **Contextual understanding**
   - Understand publication context
   - Note journal prestige and impact
   - Recognize field-specific practices
   - Account for disciplinary differences

5. **Accuracy and verification**
   - Double-check data extraction
   - Verify direct quotes
   - Confirm effect sizes and statistical values
   - Cross-reference key claims

### Synthesis Best Practices

1. **Systematic organization**
   - Use consistent frameworks
   - Organize thematically for narrative clarity
   - Show relationships between studies
   - Map evidence gaps visually

2. **Avoid cherry-picking**
   - Include all relevant papers, not just supporting ones
   - Acknowledge contradictions
   - Give weight to quality of evidence
   - Report effect size ranges

3. **Integrate multiple types of evidence**
   - Combine quantitative and qualitative findings
   - Synthesize across methodological approaches
   - Build comprehensive understanding
   - Identify convergent and divergent findings

4. **Make evidence strength explicit**
   - Use GRADE or similar system
   - Distinguish high-quality from preliminary evidence
   - Acknowledge uncertainty
   - Indicate confidence in conclusions

5. **Address conflicting evidence**
   - Describe conflicting findings fairly
   - Investigate sources of disagreement
   - Propose explanations for conflicts
   - Note when evidence is inconclusive

## Common Mistakes to Avoid

1. **Inadequate search strategy**
   - Too narrow, missing relevant papers
   - Too broad, overwhelming results
   - Missing important databases or grey literature
   - Insufficient iterative refinement

2. **Insufficient documentation**
   - Unclear how papers were selected
   - Search strategy not reproducible
   - Selection decisions not justified
   - Preventing others from assessing quality

3. **Quality assessment omission**
   - Treating all papers as equally valid
   - Over-reliance on poor-quality studies
   - Missing bias assessment
   - Unweighted evidence synthesis

4. **Shallow analysis**
   - Mere summarization instead of synthesis
   - Isolated findings not integrated
   - Contradictions not addressed
   - Limited critical evaluation

5. **Inadequate citation**
   - Incomplete bibliographic information
   - Inconsistent formatting
   - Incorrect attributions
   - Missing DOIs or URLs (for web sources)

6. **Bias toward recent literature**
   - Seminal foundational papers missed
   - Publication bias not addressed
   - Overweight to recent trends
   - Missing historical context

7. **Ignoring methodological variation**
   - Papers using different methodologies treated as directly comparable
   - Methodology-specific findings not noted
   - Invalid meta-analyses (combining incompatible studies)
   - Missed insights from methodological diversity

## Tools and Resources

### Search and Management Tools

**Reference Management:**
- Zotero (zotero.org) - Free, open-source
- Mendeley (mendeley.com) - Free basic version
- RefWorks - Institutional access
- Papers (papersapp.com) - PDF-focused

**Search Tools:**
- PubMed Central: pubmed.ncbi.nlm.nih.gov
- arXiv: arxiv.org
- Web of Science: webofscience.com
- Scopus: scopus.com
- Google Scholar: scholar.google.com

**Citation Tools:**
- CrossRef (crossref.org) - DOI lookup and verification
- Unpaywall (unpaywall.org) - Open access article locator

### Data Extraction and Organization

**Spreadsheets and Databases:**
- Excel: Simple, widely available
- Google Sheets: Collaborative, cloud-based
- Airtable: Database functionality, templates
- Covidence: Systematic review platform
- DistillerSR: Systematic review software

### Synthesis and Visualization

**Concept Mapping:**
- CmapTools: Free concept mapping
- MindMeister: Collaborative mind mapping
- VosViewer: Citation network visualization
- Gephi: Network analysis and visualization

**Statistical Analysis:**
- R: Advanced analysis and visualization
- Python: Data analysis and synthesis
- STATA: Statistical analysis
- RevMan: Systematic review meta-analysis

## Integration with Other Skills

This skill integrates well with:

- **scientific-writing**: For writing the literature review sections of papers
- **eln**: For documenting literature review process in electronic lab notebook
- **planning**: For project planning and tracking literature review progress
- **troubleshooting**: For debugging searches and refining strategy when facing challenges
- **phd-qualifier**: For comprehensive literature reviews needed for exams

## Response Patterns

When conducting literature reviews, I will:

1. **Clarify scope and research questions** - Ensure clear, specific research questions using PICO/PEO framework
2. **Propose search strategy** - Recommend appropriate databases and search queries
3. **Guide iterative refinement** - Suggest ways to refine searches based on initial results
4. **Suggest organizational frameworks** - Propose thematic organization structures
5. **Help with synthesis** - Guide integration of findings and identification of gaps
6. **Provide quality assessment** - Apply relevant quality assessment tools
7. **Support report writing** - Help structure and write literature review sections
8. **Document comprehensively** - Ensure all decisions and searches are documented

I will ask clarifying questions about:
- Research domain and field
- Intended scope (comprehensive vs. focused)
- Time constraints and available resources
- Target audience for the review
- Specific research question(s)
- Preferred output format
- Publication requirements (PRISMA compliance, specific journal guidelines, etc.)

## Limitations and Considerations

1. **Time and resources**: Comprehensive literature reviews require significant time investment (weeks to months)
2. **Database access**: Some databases require institutional access or subscriptions
3. **Full-text availability**: Not all papers are freely available online
4. **Language barriers**: This skill focuses on English-language resources
5. **Currency**: Review methodology follows published guidelines; emerging trends in literature review methodology may not be fully captured
6. **Subjective elements**: Some aspects of literature review (theme identification, synthesis) involve interpretive judgment
7. **Tool limitations**: External tools (databases, search engines) have inherent limitations and biases

---

This comprehensive literature review skill provides a systematic, reproducible approach to synthesizing knowledge across research domains while maintaining rigor, transparency, and critical evaluation of evidence.

---

### materials-databases

**Path**: `skills/research/materials-databases/SKILL.md`

# Materials Databases Access Skill

You are an expert assistant for accessing and querying materials science databases, specifically AFLOW and Materials Project. Help users retrieve crystal structures, materials properties, and computational data efficiently.

## Overview

This skill enables access to two major materials databases:

1. **AFLOW** (Automatic Flow for Materials Discovery)
   - 3.5+ million calculated materials
   - Crystal structures, thermodynamic properties, elastic properties
   - No API key required for basic access
   - REST API with simple URL-based queries

2. **Materials Project** (MP)
   - 150,000+ inorganic compounds
   - Electronic structure, phonons, elasticity, surfaces, batteries
   - Requires free API key
   - Python client library (mp-api) with rich functionality

## Installation Requirements

### Materials Project (mp-api)

```bash
# Install the Materials Project API client
pip install mp-api

# Alternative: with conda
conda install -c conda-forge mp-api
```

### AFLOW

AFLOW uses REST API - no Python package installation required. However, for convenience:

```bash
# Optional: Install requests for API calls
pip install requests

# Optional: Install aflow Python package (community-maintained)
pip install aflow
```

### Additional Recommended Packages

```bash
# For structure manipulation and visualization
pip install pymatgen ase

# For data analysis
pip install pandas numpy matplotlib
```

## API Key Setup

### Materials Project API Key

1. **Get an API key:**
   - Visit: https://next-gen.materialsproject.org/api
   - Click "Generate API Key" (requires login with ORCID or email)
   - Copy your API key (format: long alphanumeric string)

2. **Set up authentication:**

   **Option A: Environment variable (recommended)**
   ```bash
   export MP_API_KEY="your_api_key_here"
   ```

   **Option B: Configuration file**
   ```bash
   # Create ~/.config/.mpapi.json or ~/.pmgrc.yaml
   echo '{"MAPI_KEY": "your_api_key_here"}' > ~/.config/.mpapi.json
   ```

   **Option C: Pass directly in code**
   ```python
   from mp_api.client import MPRester

   with MPRester("your_api_key_here") as mpr:
       # Your code here
       pass
   ```

### AFLOW

**No API key required** - AFLOW API is publicly accessible.

## Core Functionality

### Materials Project - Common Queries

**Search by formula:**
```python
from mp_api.client import MPRester

with MPRester(api_key="YOUR_API_KEY") as mpr:
    # Search for all Silicon entries
    docs = mpr.materials.summary.search(formula="Si")

    # Get specific properties
    docs = mpr.materials.summary.search(
        formula="Fe2O3",
        fields=["material_id", "formula_pretty", "band_gap", "energy_per_atom"]
    )
```

**Search by material ID:**
```python
with MPRester() as mpr:  # Uses env var or config file
    structure = mpr.get_structure_by_material_id("mp-149")
    doc = mpr.materials.summary.get_data_by_id("mp-149")
```

**Search by criteria:**
```python
with MPRester() as mpr:
    # Find materials with band gap between 1-3 eV
    docs = mpr.materials.summary.search(
        band_gap=(1, 3),
        elements=["O", "Ti"],
        num_elements=2
    )

    # Find stable materials
    docs = mpr.materials.summary.search(
        energy_above_hull=(0, 0.01),  # Nearly stable
        fields=["material_id", "formula_pretty", "energy_above_hull"]
    )
```

**Available data types:**
- `materials.summary` - General materials properties
- `materials.thermo` - Thermodynamic data
- `materials.electronic_structure` - Band structures, DOS
- `materials.phonon` - Phonon band structures
- `materials.elasticity` - Elastic tensors
- `materials.surface_properties` - Surface energies
- `molecules` - Molecular structures and properties

### AFLOW - REST API Queries

**Basic URL structure:**
```
http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB/<system>/<file>?<directives>
```

**Common queries using REST:**
```python
import requests

# Get all keywords for a material
url = "http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB/FCC/Ag1/?keywords"
response = requests.get(url)
data = response.text

# Get specific property (e.g., enthalpy)
url = "http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB/FCC/Ag1/?enthalpy_formation_atom"
response = requests.get(url)

# Search using AFLUX (AFLOW search language)
url = "http://aflowlib.duke.edu/search/API/?species(Au),Egap(5*),catalog(ICSD)"
response = requests.get(url)
```

**Using aflow Python package (optional):**
```python
import aflow

# Search for materials
results = aflow.search(filter='species(Au),Egap(5*)')

for result in results:
    print(result.enthalpy_formation_atom)
    print(result.Egap)
    structure = result.atoms  # Get ASE Atoms object
```

**Common AFLOW directives:**
- `?keywords` - List all available properties
- `?enthalpy_formation_atom` - Formation enthalpy per atom
- `?Egap` - Band gap
- `?volume_cell` - Unit cell volume
- `?geometry` - Crystal structure (POSCAR format)
- `?files` - List all available files

**AFLUX search syntax:**
```
species(element1,element2)  # Chemical system
Egap(min*,max*)            # Band gap range (eV)
enthalpy_formation_atom(min*,max*)  # Formation enthalpy range
catalog(ICSD)              # Database catalog
```

## Workflow Guidance

### When to Use Materials Project

1. **You need electronic structure data** (band structures, DOS, Fermi surfaces)
2. **Battery materials research** (voltage, capacity calculations)
3. **Surface properties and adsorption energies**
4. **Detailed phonon calculations**
5. **Integration with pymatgen workflows**
6. **Phase stability analysis** (phase diagrams, energy above hull)

### When to Use AFLOW

1. **Large-scale materials screening** (millions of entries)
2. **Elastic properties and mechanical data**
3. **No API key setup possible/desired**
4. **ICSD-based structures** (experimental references)
5. **Quick property lookups** (simple REST calls)

### Combining Both Databases

```python
# Example: Cross-reference findings
from mp_api.client import MPRester
import requests

# Find in Materials Project
with MPRester() as mpr:
    mp_docs = mpr.materials.summary.search(
        formula="TiO2",
        fields=["material_id", "band_gap", "energy_per_atom"]
    )

# Cross-check with AFLOW
aflow_url = "http://aflowlib.duke.edu/search/API/?species(Ti,O),nspecies(2)"
aflow_data = requests.get(aflow_url).json()
```

## Best Practices

1. **Use specific queries** - Request only needed fields to reduce data transfer
   ```python
   docs = mpr.materials.summary.search(
       formula="Li",
       fields=["material_id", "formula_pretty", "band_gap"]  # Specify fields
   )
   ```

2. **Paginate large results** - Use chunk_size for large queries
   ```python
   docs = mpr.materials.summary.search(
       elements=["O"],
       num_chunks=10,  # Fetch in chunks
       chunk_size=1000
   )
   ```

3. **Cache results locally** - Save API results to avoid repeated queries
   ```python
   import pickle

   # Save results
   with open("mp_results.pkl", "wb") as f:
       pickle.dump(docs, f)
   ```

4. **Handle structures properly** - Convert between formats as needed
   ```python
   from pymatgen.io.ase import AseAtomsAdaptor

   # MP Structure → ASE Atoms
   atoms = AseAtomsAdaptor.get_atoms(structure)

   # ASE Atoms → MP Structure
   structure = AseAtomsAdaptor.get_structure(atoms)
   ```

5. **Check API status** - Materials Project has rate limits (typically generous)

## Error Handling

```python
from mp_api.client import MPRester
from mp_api.client.core import MPRestError

try:
    with MPRester() as mpr:
        docs = mpr.materials.summary.search(formula="InvalidFormula")
except MPRestError as e:
    print(f"API Error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

## Common Tasks

### Task 1: Find Band Gap of a Material

```python
# Materials Project
with MPRester() as mpr:
    docs = mpr.materials.summary.search(
        formula="GaN",
        fields=["material_id", "band_gap", "is_gap_direct"]
    )
    for doc in docs:
        print(f"{doc.material_id}: {doc.band_gap} eV (direct: {doc.is_gap_direct})")

# AFLOW
import requests
url = "http://aflowlib.duke.edu/search/API/?species(Ga,N),Egap"
response = requests.get(url)
```

### Task 2: Get Crystal Structure

```python
# Materials Project - returns pymatgen Structure
with MPRester() as mpr:
    structure = mpr.get_structure_by_material_id("mp-149")
    structure.to(filename="POSCAR")

# AFLOW - returns POSCAR format text
url = "http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB/FCC/Ag1/?geometry"
poscar = requests.get(url).text
```

### Task 3: Screen for Stable Materials

```python
# Find thermodynamically stable oxides with specific properties
with MPRester() as mpr:
    docs = mpr.materials.summary.search(
        elements=["O"],
        energy_above_hull=(0, 0.05),  # Stable or nearly stable
        band_gap=(1.0, 4.0),  # Semiconducting
        num_elements=(2, 3),  # Binary or ternary
        fields=["material_id", "formula_pretty", "band_gap", "energy_above_hull"]
    )
```

## References

- See `references/materials-project.md` for detailed MP API documentation
- See `references/aflow.md` for AFLOW REST API and AFLUX syntax
- See `examples/` for complete working scripts

## Key Points

- Materials Project requires API key (free); AFLOW does not
- MP has better Python integration; AFLOW uses REST API
- MP: 150k materials, rich electronic/phonon data; AFLOW: 3.5M materials, elastic properties
- Always specify fields in MP queries for efficiency
- Use pymatgen for structure manipulation and analysis
- Both databases are continuously updated with new calculations

---

### materials-properties

**Path**: `skills/programming/materials-properties/SKILL.md`

# Materials Properties Calculation Skill

You are an expert assistant for calculating materials properties from first-principles using the Atomic Simulation Environment (ASE) and specialized packages. Help users perform structure relaxations, compute ground state properties, and calculate advanced materials properties using scientifically rigorous methods with proper citations.

## Overview

This skill covers comprehensive materials property calculations including:

**Core Capabilities:**
- Structure relaxation (geometry optimization, stress relaxation)
- Ground state properties (lattice constants, space groups, crystal structure)
- Calculator setup (EMT, GPAW, VASP, Quantum ESPRESSO)

**Advanced Properties:**
- Surface energies
- Adsorption energies
- Reaction barriers (NEB)
- Vibrational analysis
- Phonon calculations (phonopy)
- Elastic constants (elastic)
- Cluster expansions (icet)
- CALPHAD integration (pycalphad)
- Defect formation energies
- Interface/grain boundary energies
- Magnetic properties
- Thermal expansion
- Electronic structure

## Installation

```bash
# Core packages
pip install ase spglib matplotlib

# Specialized packages
pip install phonopy elastic icet pycalphad

# Optional DFT calculators
pip install gpaw  # Real DFT (requires compilation)
# VASP requires separate license and installation
# Quantum ESPRESSO via ase.calculators.espresso
```

## Core Workflow: Structure Relaxation

### 1. Basic Geometry Optimization

```python
from ase import Atoms
from ase.optimize import BFGS
from ase.calculators.emt import EMT

# Create or load structure
atoms = Atoms('Cu', positions=[[0, 0, 0]], cell=[2.5, 2.5, 2.5], pbc=True)

# Set calculator
atoms.calc = EMT()

# Optimize geometry
opt = BFGS(atoms, trajectory='opt.traj')
opt.run(fmax=0.01)  # Force convergence criterion

# Get optimized energy
E_opt = atoms.get_potential_energy()
print(f"Optimized energy: {E_opt:.3f} eV")
```

**Key Parameters:**
- `fmax`: Maximum force (eV/Å) - typical values: 0.01-0.05
- `steps`: Maximum optimization steps
- Optimizers: BFGS, LBFGS, FIRE, GPMin

### 2. Unit Cell Optimization (Stress Relaxation)

```python
from ase.optimize import BFGS
from ase.constraints import ExpCellFilter

# Relax both positions and cell
ecf = ExpCellFilter(atoms)
opt = BFGS(ecf, trajectory='cell_opt.traj')
opt.run(fmax=0.01)

# Get optimized lattice
a = atoms.cell.cellpar()[0]
print(f"Optimized lattice constant: {a:.3f} Å")
```

**Applications:**
- Lattice constant determination
- Equation of state calculations
- Pressure-dependent structures

### 3. Ground State Properties

```python
import spglib

# Get space group
cell = (atoms.cell, atoms.get_scaled_positions(), atoms.get_atomic_numbers())
spacegroup = spglib.get_spacegroup(cell, symprec=1e-5)
print(f"Space group: {spacegroup}")

# Get lattice parameters
a, b, c, alpha, beta, gamma = atoms.cell.cellpar()
print(f"Lattice: a={a:.3f}, b={b:.3f}, c={c:.3f} Å")
print(f"Angles: α={alpha:.1f}, β={beta:.1f}, γ={gamma:.1f}°")

# Volume
V = atoms.get_volume()
print(f"Volume: {V:.3f} ų")

# Crystal system
dataset = spglib.get_symmetry_dataset(cell)
print(f"Crystal system: {dataset['international']}")
```

## Calculator Setup

### EMT Calculator (Fast, for Testing)

```python
from ase.calculators.emt import EMT

atoms.calc = EMT()
```

**Advantages:**
- Very fast
- No installation issues
- Good for workflow development

**Limitations:**
- Only works for a few elements (Cu, Ag, Au, Ni, Pd, Pt, Al, Pb, Fe)
- Approximate potential

### GPAW Calculator (Real DFT)

```python
from gpaw import GPAW, PW

atoms.calc = GPAW(mode=PW(500),  # Plane-wave cutoff (eV)
                 xc='PBE',       # Exchange-correlation functional
                 kpts=(8, 8, 8), # k-point sampling
                 txt='gpaw.txt') # Output file
```

**Parameters:**
- `mode`: PW(cutoff) for plane waves, grid-based for real-space
- `xc`: PBE, LDA, RPBE, BEEF-vdW, etc.
- `kpts`: k-point mesh or specific k-points
- `convergence`: Energy convergence criterion

### VASP Calculator

```python
from ase.calculators.vasp import Vasp

atoms.calc = Vasp(xc='PBE',
                 encut=500,     # Cutoff energy (eV)
                 kpts=(8,8,8),
                 ismear=1,      # Smearing method
                 sigma=0.1)     # Smearing width
```

**VASP-specific:**
- Requires VASP license
- Uses POTCAR files for pseudopotentials
- See `references/calculator_setup.md` for details

## Advanced Properties (Subskills)

### 1. Surface Energy

**Method**: Slab model approach

**Formula**:
```
γ = (E_slab - N × E_bulk) / (2 × A)
```

Where:
- E_slab: Energy of slab with surfaces
- E_bulk: Bulk energy per atom
- N: Number of atoms in slab
- A: Surface area
- Factor of 2: Two equivalent surfaces

**Workflow**:
1. Optimize bulk structure
2. Create slab with `ase.build.surface()`
3. Add vacuum layer
4. Relax slab (constrain bottom layers)
5. Calculate surface energy

**See**: `references/surface_energy.md` for detailed methods

**Key References**:
- Fiorentini & Methfessel, "Extracting convergent surface energies," *J. Phys.: Condens. Matter* **8**, 6525 (1996)
- Tran et al., "Surface energies of elemental crystals," *Sci. Data* **3**, 160080 (2016)

### 2. Adsorption Energy

**Method**: Compare slab+adsorbate to separated systems

**Formula**:
```
E_ads = E_slab+ads - E_slab - E_molecule
```

More negative = stronger adsorption

**Workflow**:
1. Optimize clean slab
2. Add adsorbate at different sites (top, bridge, hollow)
3. Relax adsorbed structure
4. Calculate adsorption energy
5. Compare sites to find preferred adsorption

**See**: `references/adsorption_energy.md`

**Key References**:
- Hammer & Nørskov, "Theoretical surface science," *Adv. Catal.* **45**, 71 (2000)
- Nørskov et al., "Computational design of solid catalysts," *Nature Chem.* **1**, 37 (2009)

### 3. Reaction Barriers (Nudged Elastic Band)

**Method**: Find minimum energy path between reactant and product

**Workflow**:
```python
from ase.neb import NEB
from ase.optimize import BFGS

# Create images interpolating between initial and final
images = [initial]
images += [initial.copy() for i in range(5)]  # 5 intermediate images
images += [final]

# Interpolate
neb = NEB(images)
neb.interpolate()

# Set calculators
for image in images[1:-1]:
    image.calc = EMT()

# Optimize NEB
optimizer = BFGS(neb, trajectory='neb.traj')
optimizer.run(fmax=0.05)

# Extract barrier
from ase.neb import NEBTools
nebtools = NEBTools(images)
barrier = nebtools.get_barrier()[0]
print(f"Activation barrier: {barrier:.2f} eV")
```

**See**: `references/reaction_barriers.md`

**Key References**:
- Henkelman, Uberuaga & Jónsson, "Climbing image NEB," *J. Chem. Phys.* **113**, 9901 (2000)
- Sheppard et al., "Optimization methods for MEPs," *J. Chem. Phys.* **128**, 134106 (2008)

### 4. Vibrational Analysis

**Method**: Finite displacement or DFPT

**Workflow**:
```python
from ase.vibrations import Vibrations

# Calculate vibrations
vib = Vibrations(atoms)
vib.run()

# Get frequencies
vib.summary()

# Zero-point energy
zpe = vib.get_zero_point_energy()
```

**See**: `references/vibrational_analysis.md`

**Key References**:
- Wilson, Decius & Cross, *Molecular Vibrations* (Dover, 1980)

### 5. Phonon Calculations (phonopy)

**Method**: Supercell approach with force constants

**Workflow**:
```python
from phonopy import Phonopy

# Create phonopy object
phonon = Phonopy(atoms, supercell_matrix=[[2,0,0],[0,2,0],[0,0,2]])

# Generate displacements
phonon.generate_displacements(distance=0.01)
supercells = phonon.supercells_with_displacements

# Calculate forces for each displacement
for scell in supercells:
    scell.calc = calc
    forces = scell.get_forces()
    # Set forces back to phonopy

# Calculate phonon properties
phonon.produce_force_constants()
phonon.auto_band_structure()
phonon.plot_band_structure()
```

**Applications**:
- Phonon band structure
- Phonon density of states
- Thermal properties (heat capacity, free energy)
- Thermodynamic integration

**See**: `references/phonons.md`

**Key References**:
- Togo & Tanaka, "First principles phonon calculations," *Scr. Mater.* **108**, 1 (2015)
- Togo, "Phonopy and Phono3py," *J. Phys. Soc. Jpn.* **92**, 012001 (2023)

### 6. Elastic Constants (elastic package)

**Method**: Apply strain, measure stress

**Properties Calculated**:
- Full elastic tensor (Cij)
- Bulk modulus (K)
- Shear modulus (G)
- Young's modulus (E)
- Poisson's ratio (ν)
- Sound velocities
- Debye temperature

**See**: `references/elastic_constants.md`

**Key References**:
- Golesorkhtabar et al., "ElaStic tool," *Comput. Phys. Commun.* **184**, 1861 (2013)
- Nye, *Physical Properties of Crystals* (Oxford, 1985)

### 7. Equation of State

**Method**: Volume-energy curve fitting

**Workflow**:
```python
from ase.eos import calculate_eos

eos = calculate_eos(atoms, trajectory='eos.traj')
v, e, B = eos.fit()  # Volume, energy, bulk modulus
eos.plot('eos.png')
```

**EOS Types**:
- Birch-Murnaghan
- Murnaghan
- Vinet

**See**: `references/equation_of_state.md`

**Key References**:
- Birch, "Finite elastic strain," *Phys. Rev.* **71**, 809 (1947)

### 8. Formation Energy

**Formula**:
```
E_form = E_compound - Σ(n_i × μ_i)
```

Where μ_i are chemical potentials (reference energies)

**Applications**:
- Phase stability
- Energy above convex hull
- Phase diagrams

**See**: `references/formation_energy.md`

**Key References**:
- Hautier et al., "DFT formation energies," *Phys. Rev. B* **85**, 155208 (2012)

### 9. Cluster Expansions (icet)

**Method**: Expand configurational energy in cluster interactions

**Applications**:
- Alloy ground states
- Order-disorder transitions
- Monte Carlo simulations
- Phase diagram construction

**See**: `references/cluster_expansion.md`

**Key References**:
- Ångqvist et al., "ICET library," *Adv. Theory Simul.* **2**, 1900015 (2019)
- Sanchez et al., "Cluster description," *Physica A* **128**, 334 (1984)

### 10. CALPHAD Integration (pycalphad)

**Method**: Combine DFT with thermochemical databases

**Applications**:
- Phase equilibria
- Multi-component systems
- Temperature-dependent properties

**See**: `references/calphad.md`

**Key References**:
- Otis & Liu, "pycalphad," *J. Open Res. Softw.* **5**, 1 (2017)
- Lukas et al., *Computational Thermodynamics* (Cambridge, 2007)

### 11. Defect Formation Energy

**Types**:
- Vacancies
- Interstitials
- Substitutional defects
- Charged defects (with corrections)

**See**: `references/defect_energy.md`

**Key References**:
- Freysoldt et al., "Point defects in solids," *Rev. Mod. Phys.* **86**, 253 (2014)

### 12. Interface/Grain Boundary Energy

**Method**: Compare interface structure to separated surfaces

**See**: `references/interface_energy.md`

**Key References**:
- Sutton & Balluffi, *Interfaces in Crystalline Materials* (Oxford, 1995)

### 13. Magnetic Properties

**Method**: Spin-polarized DFT

**Properties**:
- Magnetic moments
- Magnetic ordering (FM, AFM)
- Heisenberg parameters

**See**: `references/magnetic_properties.md`

### 14. Thermal Expansion

**Method**: Quasi-harmonic approximation

**See**: `references/thermal_expansion.md`

**Key References**:
- Barrera et al., "Grüneisen parameters," *J. Phys.: Condens. Matter* **17**, R217 (2005)

### 15. Electronic Structure

**Properties**:
- Band structure
- Density of states (DOS)
- Band gaps
- Fermi surfaces

**See**: `references/electronic_structure.md`

**Key References**:
- Martin, *Electronic Structure* (Cambridge, 2004)

## Best Practices

### Convergence Testing

**Always test convergence of**:
1. **k-point sampling**: Increase until energy converges (typically < 1 meV/atom)
2. **Plane-wave cutoff**: Test different values (e.g., 300-600 eV)
3. **Slab thickness**: For surfaces (typically 5-9 layers)
4. **Vacuum thickness**: For surfaces (typically 10-15 Å)
5. **Supercell size**: For defects, phonons

**Example**:
```python
# k-point convergence
for k in [2, 4, 6, 8, 10, 12]:
    atoms.calc = GPAW(kpts=(k,k,k), ...)
    E = atoms.get_potential_energy()
    print(f"k={k}: E={E:.4f} eV")
```

### Force Convergence

- Typical: `fmax = 0.01-0.05 eV/Å`
- Tighter for vibrations: `fmax = 0.001 eV/Å`
- Check max force: `max(np.linalg.norm(atoms.get_forces(), axis=1))`

### Constraints

**Fix atoms during relaxation**:
```python
from ase.constraints import FixAtoms

# Fix bottom 2 layers of slab
c = FixAtoms(indices=[atom.index for atom in atoms if atom.position[2] < 5])
atoms.set_constraint(c)
```

### Trajectory Analysis

```python
from ase.io import read

# Read optimization trajectory
traj = read('opt.traj', ':')

# Plot energy vs step
energies = [atoms.get_potential_energy() for atoms in traj]
import matplotlib.pyplot as plt
plt.plot(energies)
plt.xlabel('Step')
plt.ylabel('Energy (eV)')
plt.show()
```

## Common Workflows

### Workflow 1: Lattice Constant Determination

```python
from ase.build import bulk
from ase.eos import calculate_eos

atoms = bulk('Cu', 'fcc', a=3.6)
atoms.calc = EMT()

eos = calculate_eos(atoms, trajectory='eos.traj')
v, e, B = eos.fit()
a_opt = v**(1/3)
print(f"Optimal lattice constant: {a_opt:.3f} Å")
print(f"Bulk modulus: {B/1e9:.1f} GPa")
```

### Workflow 2: Surface Energy Calculation

```python
from ase.build import bulk, surface, add_vacuum

# Bulk energy
bulk_atoms = bulk('Cu', 'fcc', a=3.6)
bulk_atoms.calc = EMT()
E_bulk_per_atom = bulk_atoms.get_potential_energy() / len(bulk_atoms)

# Create slab
slab = surface('Cu', (1,1,1), layers=7, vacuum=10)
slab.calc = EMT()
E_slab = slab.get_potential_energy()

# Surface energy
N = len(slab)
A = slab.get_cell()[0,0] * slab.get_cell()[1,1]
gamma = (E_slab - N * E_bulk_per_atom) / (2 * A)
print(f"Surface energy: {gamma*1000:.1f} meV/ų")
```

### Workflow 3: Adsorption Energy

```python
from ase.build import fcc111, molecule, add_adsorbate

# Clean slab
slab = fcc111('Cu', size=(3,3,4), vacuum=10)
slab.calc = EMT()
E_slab = slab.get_potential_energy()

# Adsorbate in gas phase
mol = molecule('CO')
mol.calc = EMT()
E_mol = mol.get_potential_energy()

# Adsorbed system
add_adsorbate(slab, mol, height=2.0, position='ontop')
slab.calc = EMT()
E_ads_system = slab.get_potential_energy()

# Adsorption energy
E_ads = E_ads_system - E_slab - E_mol
print(f"Adsorption energy: {E_ads:.2f} eV")
```

## Error Handling and Troubleshooting

### Common Issues

**1. SCF Not Converging**:
- Increase mixing parameter
- Try different smearing methods
- Check initial geometry (remove overlaps)

**2. Forces Not Converging**:
- Check constraints
- Try different optimizer
- Increase maximum steps
- Verify calculator parameters

**3. Unstable Structures**:
- Check for imaginary phonon modes
- Verify symmetry is correct
- Try different initial configurations

**4. Memory Issues**:
- Reduce k-points or cutoff
- Use real-space mode (GPAW)
- Parallelize calculation

## Integration with Other Skills

- **python-ase**: Core ASE functionality
- **materials-databases**: Get structures from Materials Project/AFLOW
- **pymatgen**: Structure manipulation and analysis
- **fairchem**: Use ML potentials for screening

## References

**Core ASE**:
1. Larsen et al., "The atomic simulation environment," *J. Phys.: Condens. Matter* **29**, 273002 (2017)

**DFT Theory**:
2. Hohenberg & Kohn, "Inhomogeneous electron gas," *Phys. Rev.* **136**, B864 (1964)
3. Kohn & Sham, "Self-consistent equations," *Phys. Rev.* **140**, A1133 (1965)
4. Sholl & Steckel, *Density Functional Theory: A Practical Introduction* (Wiley, 2009)

**Symmetry Analysis**:
5. Spglib: https://spglib.github.io/spglib/

See individual reference files in `references/` for detailed citations for each method.

## Resources

- **ASE Documentation**: https://wiki.fysik.dtu.dk/ase/
- **ASE Tutorials**: https://wiki.fysik.dtu.dk/ase/tutorials/tutorials.html
- **Example Scripts**: See `examples/` directory
- **Method Details**: See `references/` directory
- **Best Practices**: See `workflows/` directory

---

### opentrons

**Path**: `skills/laboratory/opentrons/SKILL.md`

# Opentrons Python API v2

## Overview

The **Opentrons Python API v2** enables laboratory automation through Python protocols that control liquid handling robots (OT-2 and Flex). Write protocols that automate pipetting, temperature control, magnetic separation, plate reading, and complex multi-step workflows with reproducible precision.

**Core value:** Transform manual, error-prone pipetting workflows into automated, reproducible protocols. A 100-line protocol can replace hours of manual work with precise, documented liquid handling.

## When to Use

Use the Opentrons skill when:
- Writing automated liquid handling protocols
- Setting up serial dilutions, plate replication, or reagent distribution
- Integrating temperature control, magnetic separation, or plate reading into workflows
- Adapting OT-2 protocols for Opentrons Flex
- Troubleshooting protocol execution or hardware integration
- Optimizing pipetting parameters (flow rates, tip positions, mixing)

**Specialized subskills available:**
- `heater-shaker` - Temperature control with orbital mixing
- `absorbance-reader` - Microplate spectrophotometry
- `gripper` - Automated labware movement (Flex only)
- `thermocycler` - PCR thermal cycling
- `temperature-module` - Precise heating/cooling (4-95°C)
- `magnetic-block` - Magnetic bead separation (Flex only)

**Don't use when:**
- Working with non-Opentrons liquid handlers
- Writing general Python code unrelated to lab automation
- You need detailed module-specific guidance (use the specialized subskills)

## Robot Platforms

### Opentrons Flex
Modern platform with advanced capabilities:
- Gripper for automated labware movement
- 96-channel pipetting
- Coordinate-based deck slots (A1-D3)
- Larger working volume and deck space
- Trash bin requires explicit loading (API 2.16+)
- Staging area slots in column 4

### OT-2
Established platform:
- Numeric deck slots (1-12)
- Fixed trash in slot 12
- 1-channel and 8-channel pipettes
- Proven reliability for standard protocols

**Both platforms use identical API syntax** with platform-specific parameters.

## Protocol Structure

Every Opentrons protocol follows this framework:

```python
from opentrons import protocol_api

# 1. Metadata - Protocol identification
metadata = {
    'protocolName': 'My Protocol',
    'author': 'Name <email@example.com>',
    'description': 'Brief description of what this protocol does',
    'apiLevel': '2.19'
}

# 2. Requirements - Robot specification (Flex)
requirements = {"robotType": "Flex", "apiLevel": "2.19"}

# 3. Main function - Setup and commands
def run(protocol: protocol_api.ProtocolContext):
    # Setup: Load labware, pipettes, modules
    tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "A1")
    plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "B1")
    pipette = protocol.load_instrument("flex_1channel_1000", "left", tip_racks=[tips])

    # Commands: Sequential robot instructions
    pipette.pick_up_tip()
    pipette.aspirate(100, plate["A1"])
    pipette.dispense(100, plate["B1"])
    pipette.drop_tip()
```

**Key principles:**
- Protocols execute **completely linearly** - follow line-by-line to understand robot actions
- Use descriptive variable names that mirror lab terminology
- Add comments to document workflow steps
- Group related operations logically

## Core API Components

### 1. Loading Equipment

**Labware** - Plates, tubes, reservoirs:
```python
# Load standard labware from registry
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "B1")
tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "A1")

# Load custom labware (JSON definition required)
custom = protocol.load_labware("custom_24_tuberack_1500ul", "C1")
```

**Pipettes** - Single, 8-channel, or 96-channel:
```python
# Flex pipettes
left_pip = protocol.load_instrument("flex_1channel_1000", "left", tip_racks=[tips])
right_pip = protocol.load_instrument("flex_8channel_1000", "right", tip_racks=[tips])

# OT-2 pipettes
p300 = protocol.load_instrument("p300_single_gen2", "left", tip_racks=[tips])
```

**Modules** - Temperature, magnetic, thermocycler:
```python
# Load module in specific deck slot
temp_mod = protocol.load_module("temperature module gen2", "D1")
# Load labware on module
temp_plate = temp_mod.load_labware("opentrons_96_aluminumblock_generic_pcr_strip_200ul")
```

**Trash bin** (Flex, API 2.16+):
```python
trash = protocol.load_trash_bin("A3")
```

### 2. Accessing Wells

**Individual wells:**
```python
plate["A1"]           # Single well by name
plate.wells()[0]      # First well by index
plate.wells_by_name()["A1"]  # Dictionary access
```

**Well ranges and groups:**
```python
plate.wells()[:8]     # First row (A1-H1)
plate.rows()[0]       # All wells in row A
plate.columns()[0]    # All wells in column 1
plate.wells("A1", "H12")  # Range from A1 to H12
```

**8-channel pipetting patterns:**
```python
# Pipette to entire column with 8-channel
pipette.transfer(100, reservoir["A1"], plate.columns()[0])
```

### 3. Liquid Handling Commands

**Basic operations:**
```python
pipette.pick_up_tip()
pipette.aspirate(volume=100, location=plate["A1"], rate=1.0)
pipette.dispense(volume=100, location=plate["B1"], rate=1.0)
pipette.blow_out(location=plate["B1"])
pipette.mix(repetitions=3, volume=50, location=plate["B1"])
pipette.drop_tip()  # or pipette.return_tip()
```

**Complex transfers:**
```python
# Single source to multiple destinations
pipette.distribute(
    volume=50,
    source=reservoir["A1"],
    dest=[plate.wells()[i] for i in range(8)],
    new_tip="once"
)

# Multiple sources to single destination (pooling)
pipette.consolidate(
    volume=20,
    source=plate.rows()[0],
    dest=reservoir["A2"],
    new_tip="always"
)

# One-to-one transfer with mixing
pipette.transfer(
    volume=100,
    source=plate.columns()[0],
    dest=plate.columns()[1],
    mix_after=(3, 50),
    new_tip="always"
)
```

### 4. Position Control

**Well positions:**
```python
# Top of well (default: 1mm below top)
location = plate["A1"].top()
location = plate["A1"].top(z=-2)  # 2mm below top

# Bottom of well (default: 1mm above bottom)
location = plate["A1"].bottom()
location = plate["A1"].bottom(z=2)  # 2mm above bottom

# Center of well
location = plate["A1"].center()
```

**Movement:**
```python
# Move to location without liquid operations
pipette.move_to(plate["A1"].top())

# Delay at current position
protocol.delay(seconds=5)
protocol.delay(minutes=2)
```

### 5. Tip Management

**Flexible pickup strategies:**
```python
# Pick up from specific tip rack location
pipette.pick_up_tip(tips["A1"])

# Automatic tip tracking
pipette.pick_up_tip()  # Uses next available tip

# Reset tip tracking
tips.reset()  # Mark all tips as available again
```

### 6. Liquid Tracking

**Define liquids for better protocol documentation:**
```python
# Define reagents
water = protocol.define_liquid(
    name="Water",
    description="Molecular biology grade H2O",
    display_color="#0000FF"
)

buffer = protocol.define_liquid(
    name="Lysis Buffer",
    description="10mM Tris-HCl pH 8.0",
    display_color="#00FF00"
)

# Assign liquids to wells with volumes
reservoir["A1"].load_liquid(liquid=water, volume=50000)  # 50mL
plate["A1"].load_liquid(liquid=buffer, volume=200)
```

### 7. Runtime Parameters

**User-configurable variables:**
```python
def add_parameters(parameters):
    parameters.add_int(
        variable_name="sample_count",
        display_name="Number of Samples",
        description="How many samples to process",
        default=24,
        minimum=1,
        maximum=96,
        unit="samples"
    )

    parameters.add_float(
        variable_name="transfer_volume",
        display_name="Transfer Volume",
        default=50.0,
        minimum=10.0,
        maximum=200.0,
        unit="µL"
    )

def run(protocol: protocol_api.ProtocolContext):
    # Access parameters
    num_samples = protocol.params.sample_count
    vol = protocol.params.transfer_volume
```

## Hardware Modules

Opentrons supports multiple hardware modules for specialized workflows. Each module has dedicated API methods:

### Temperature Module
Precise heating/cooling (4-95°C):
```python
temp_mod = protocol.load_module("temperature module gen2", "D1")
plate = temp_mod.load_labware("opentrons_96_aluminumblock_generic_pcr_strip_200ul")

temp_mod.set_temperature(celsius=4)  # Blocks until reached
temp_mod.deactivate()
```

**Use the `temperature-module` subskill** for detailed guidance.

### Heater-Shaker Module
Temperature control with orbital mixing:
```python
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
adapter = hs_mod.load_adapter("opentrons_96_flat_bottom_adapter")
plate = adapter.load_labware("nest_96_wellplate_200ul_flat")

hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_temperature(37)
hs_mod.set_and_wait_for_shake_speed(500)  # rpm
protocol.delay(minutes=5)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()
```

**Use the `heater-shaker` subskill** for detailed guidance including non-blocking temperature control, speed ranges, and OT-2 deck restrictions.

### Thermocycler Module
PCR thermal cycling:
```python
tc_mod = protocol.load_module("thermocyclerModuleV2")
plate = tc_mod.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")

tc_mod.close_lid()
tc_mod.set_lid_temperature(105)
tc_mod.set_block_temperature(95, hold_time_seconds=180, block_max_volume=50)

# PCR profile
profile = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 57, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 60}
]
tc_mod.execute_profile(steps=profile, repetitions=30, block_max_volume=50)
tc_mod.set_block_temperature(72, hold_time_minutes=5)

tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

**Use the `thermocycler` subskill** for detailed guidance including auto-sealing lids and advanced profiling.

### Absorbance Plate Reader (Flex only)
Microplate spectrophotometry:
```python
reader = protocol.load_module("absorbanceReaderV1", "D3")

reader.close_lid()
reader.initialize("single", [450])  # Single wavelength
reader.open_lid()

# Move plate to reader with gripper
protocol.move_labware(plate, reader, use_gripper=True)

reader.close_lid()
data = reader.read(export_filename="my_plate_data")
reader.open_lid()

# Access results
absorbance_a1 = data[450]["A1"]  # OD at 450nm for well A1
```

**Use the `absorbance-reader` subskill** for detailed guidance including multi-wavelength reading and data formats.

### Magnetic Block (Flex only)
Magnetic bead separation:
```python
mag_block = protocol.load_module("magneticBlockV1", "D1")
mag_plate = mag_block.load_labware("biorad_96_wellplate_200ul_pcr")

# Move plate to magnetic block with gripper
protocol.move_labware(mag_plate, mag_block, use_gripper=True)

# Wait for bead separation
protocol.delay(minutes=3)

# Pipette supernatant (beads held by magnets)
pipette.transfer(100, mag_plate["A1"], waste["A1"])

# Move plate away from magnets
protocol.move_labware(mag_plate, "C1", use_gripper=True)
```

**Use the `magnetic-block` subskill** for detailed guidance.

## Flex-Specific Features

### Gripper
Automated labware movement:
```python
# Move labware between locations
protocol.move_labware(
    labware=plate,
    new_location="C2",
    use_gripper=True
)

# Move to module
protocol.move_labware(plate, temp_mod, use_gripper=True)

# Dispose in waste chute
protocol.move_labware(plate, protocol.load_waste_chute(), use_gripper=True)

# Move with offset adjustments
protocol.move_labware(
    plate,
    "D2",
    use_gripper=True,
    pick_up_offset={"x": 0, "y": 0, "z": 2},
    drop_offset={"x": 0, "y": 0, "z": 1}
)
```

**Important:** Open module lids/latches before gripper movement (e.g., `tc_mod.open_lid()`, `hs_mod.open_labware_latch()`).

**Use the `gripper` subskill** for detailed guidance.

### 96-Channel Pipetting
Full-plate transfers in single operations:
```python
pipette_96 = protocol.load_instrument("flex_96channel_1000", "left")

# Pick up entire tip rack
pipette_96.pick_up_tip()

# Transfer entire plate
pipette_96.transfer(100, source_plate.wells(), dest_plate.wells())

pipette_96.drop_tip()
```

### Staging Area Slots
Column 4 slots (A4, B4, C4, D4) serve as staging areas for:
- Absorbance reader lid storage
- Temporary labware placement
- Gripper operations

**Note:** Cannot load labware in column 4 when absorbance reader is present.

## Adapting OT-2 Protocols for Flex

**Key changes required:**

1. **Update requirements:**
```python
requirements = {"robotType": "Flex", "apiLevel": "2.19"}
```

2. **Update pipette models:**
```python
# OT-2
p300 = protocol.load_instrument("p300_single_gen2", "left")

# Flex
p1000 = protocol.load_instrument("flex_1channel_1000", "left")
```

3. **Update tip racks:**
```python
# OT-2
tips = protocol.load_labware("opentrons_96_tiprack_300ul", "1")

# Flex
tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "A1")
```

4. **Load trash bin (API 2.16+):**
```python
trash = protocol.load_trash_bin("A3")
```

5. **Update deck slots (optional - both formats work):**
```python
# Numeric (OT-2 style, works on both)
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "1")

# Coordinate (Flex style, works on both)
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "D1")
```

## Common Patterns

### Serial Dilution
```python
def serial_dilution(pipette, source, destinations, dilution_factor, volume):
    """Perform serial dilution across wells."""
    # Transfer from source to first destination
    pipette.transfer(volume, source, destinations[0])

    # Serial transfers with mixing
    for i in range(len(destinations) - 1):
        pipette.transfer(
            volume / dilution_factor,
            destinations[i],
            destinations[i + 1],
            mix_after=(3, volume / 2)
        )

# Use pattern
serial_dilution(
    pipette=p1000,
    source=reservoir["A1"],
    destinations=plate.rows()[0],  # Row A
    dilution_factor=2,
    volume=100
)
```

### Plate Replication
```python
# Replicate entire plate
pipette.transfer(
    volume=50,
    source=source_plate.wells(),
    dest=dest_plate.wells(),
    new_tip="always"
)
```

### Cherry Picking
```python
# Transfer specific wells
source_wells = [source_plate["A1"], source_plate["C3"], source_plate["E5"]]
dest_wells = [dest_plate["B2"], dest_plate["D4"], dest_plate["F6"]]

pipette.transfer(100, source_wells, dest_wells, new_tip="always")
```

### Reagent Distribution
```python
# Distribute from reservoir to plate
pipette.distribute(
    volume=50,
    source=reservoir["A1"],
    dest=plate.wells(),
    disposal_volume=10,  # Extra volume for blowout
    new_tip="once"
)
```

## Best Practices

1. **Use liquid tracking** - Define and load liquids for better protocol documentation
2. **Add runtime parameters** - Make protocols configurable without editing code
3. **Optimize tip usage** - Use `distribute`/`consolidate` to minimize tip consumption
4. **Set flow rates appropriately** - Adjust for viscous or volatile liquids
5. **Include delays** - Allow time for mixing, settling, or temperature equilibration
6. **Deactivate modules** - Turn off heaters/shakers/coolers at protocol end
7. **Test with simulation** - Validate protocols before running on hardware
8. **Document thoroughly** - Use comments, metadata, and liquid definitions
9. **Handle errors gracefully** - Add pauses for user intervention when needed
10. **Calibrate labware offsets** - Ensure accurate positioning for your specific labware

## Advanced Features

### Conditional Logic
```python
# Dynamic protocol behavior based on parameters
if protocol.params.include_control:
    pipette.transfer(100, control_buffer, plate["A1"])

# Process only occupied wells
for well in plate.wells():
    if well.has_liquid():
        pipette.mix(3, 100, well)
```

### Air Gaps
Prevent dripping between wells:
```python
pipette.aspirate(100, source_well)
pipette.air_gap(10)  # 10µL air gap
pipette.dispense(110, dest_well)
```

### Touch Tip
Remove droplets from pipette exterior:
```python
pipette.aspirate(100, source_well)
pipette.touch_tip(location=source_well, radius=0.8, v_offset=-2)
pipette.dispense(100, dest_well)
```

### Blow Out
Ensure complete dispense:
```python
pipette.aspirate(100, source_well)
pipette.dispense(100, dest_well)
pipette.blow_out(dest_well.top())
```

## Common Mistakes

**❌ Forgetting to pick up tips:**
```python
pipette.aspirate(100, plate["A1"])  # Error: no tip attached
```

**✅ Correct:**
```python
pipette.pick_up_tip()
pipette.aspirate(100, plate["A1"])
```

**❌ Not deactivating modules:**
```python
temp_mod.set_temperature(4)
# Protocol ends - module stays at 4°C!
```

**✅ Correct:**
```python
temp_mod.set_temperature(4)
# ... operations ...
temp_mod.deactivate()
```

**❌ Gripper movement with closed latch:**
```python
protocol.move_labware(plate, hs_mod, use_gripper=True)  # Error: latch closed
```

**✅ Correct:**
```python
hs_mod.open_labware_latch()
protocol.move_labware(plate, hs_mod, use_gripper=True)
hs_mod.close_labware_latch()
```

**❌ Wrong deck slot format for Flex:**
Using numeric slots without quotes may cause confusion; both work, but coordinate format is clearer:
```python
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", 1)  # Works but unclear
```

**✅ Correct:**
```python
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "D1")  # Clear
```

## Troubleshooting

**Protocol won't load:**
- Check API version in requirements matches your robot firmware
- Verify labware names are correct (check Opentrons labware library)
- Ensure module loading specifies valid deck slots for your robot

**Pipetting errors:**
- Verify tip rack is correctly loaded and has available tips
- Check volume is within pipette range
- Ensure source/destination wells have sufficient volume/space

**Module errors:**
- Open lids/latches before gripper movement
- Allow modules to reach target temperature before proceeding
- Verify module is loaded in compatible deck slot

**Gripper errors:**
- Ensure labware is grippable (has compatible geometry)
- Check destination is accessible and not occupied
- Verify use_gripper=True is set

## Installation

Protocols run on Opentrons robots via the Opentrons App. For protocol development and simulation:

```bash
pip install opentrons
```

**Simulation:**
```bash
opentrons_simulate protocol.py
```

## Additional Resources

- **Official Documentation:** https://docs.opentrons.com/v2/
- **Protocol Library:** https://protocols.opentrons.com/
- **Python API Reference:** https://docs.opentrons.com/v2/new_protocol_api.html
- **Labware Library:** https://labware.opentrons.com/
- **Support Forum:** https://support.opentrons.com/
- **GitHub:** https://github.com/Opentrons/opentrons

## Related Subskills

For detailed module-specific guidance:
- `heater-shaker` - Heater-Shaker Module
- `absorbance-reader` - Absorbance Plate Reader
- `gripper` - Flex Gripper
- `thermocycler` - Thermocycler Module
- `temperature-module` - Temperature Module
- `magnetic-block` - Magnetic Block

---

### opentrons-absorbance-reader

**Path**: `skills/laboratory/opentrons-absorbance-reader/SKILL.md`

# Opentrons Absorbance Plate Reader Module

## Overview

The **Absorbance Plate Reader Module** is an on-deck microplate spectrophotometer exclusively for Opentrons Flex. It measures light absorbance in 96-well plates at up to 6 wavelengths simultaneously, enabling automated ELISA, cell density measurements, colorimetric assays, and kinetic readings without manual plate transfers.

**Core value:** Eliminate manual plate reader transfers. Read absorbance mid-protocol to make real-time decisions or collect kinetic data with automated timing and liquid handling.

**Platform:** Opentrons Flex only (not compatible with OT-2)

## When to Use

Use the Absorbance Reader skill when:
- Running ELISA or other colorimetric assays requiring absorbance reading
- Measuring bacterial or cell culture growth (OD600)
- Performing kinetic assays with timed readings
- Collecting multi-wavelength absorbance data
- Protocols requiring real-time decision-making based on absorbance values
- Automating plate-based biochemical assays

**Don't use when:**
- Working with OT-2 robot (module is Flex-only)
- Need wavelengths outside 450, 562, 600, 650nm
- Require fluorescence or luminescence measurements
- Need higher spectral resolution or broader wavelength range

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Load module | `protocol.load_module()` | `"absorbanceReaderV1"`, "D3" (or A3-C3) |
| Close lid | `close_lid()` | - |
| Open lid | `open_lid()` | - |
| Check lid status | `is_lid_on()` | Returns bool |
| Initialize reader | `initialize()` | mode, wavelengths, reference_wavelength |
| Read plate | `read()` | export_filename (optional) |

**Available wavelengths:** 450nm (blue), 562nm (green), 600nm (orange), 650nm (red)

## Platform Requirements

**Opentrons Flex only**
- **API version:** 2.21 or later
- **Compatible deck slots:** A3, B3, C3, D3 (column 3 only)
- **Staging area:** Column 4 (entire column reserved for lid storage)

**Important:** Cannot load any labware in column 4 (A4, B4, C4, D4) when absorbance reader is present.

## Deck Layout

```
Column 3 (Reader)  Column 4 (Staging - Reserved)
┌─────────────┐    ┌─────────────┐
│     A3      │───▶│  A4 (Lid)   │
│   Reader    │    │  Reserved   │
│   Module    │    │             │
└─────────────┘    └─────────────┘
```

**Reader occupies column 3** - Detection unit and plate holder
**Column 4 reserved** - Lid staging area (gripper moves lid here when open)

## Loading the Module

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.21'}
requirements = {"robotType": "Flex", "apiLevel": "2.21"}

def run(protocol: protocol_api.ProtocolContext):
    # Load Absorbance Reader in column 3
    reader = protocol.load_module("absorbanceReaderV1", "D3")

    # Note: Column 4 is now unavailable for labware
```

**Compatible slots:** A3, B3, C3, D3
**Recommended:** D3 (bottom-right position for easy gripper access)

## Lid Control

The gripper manages lid position automatically:

```python
# Close lid (required before initialization)
reader.close_lid()

# Check lid status
if reader.is_lid_on():
    protocol.comment("Lid is on detection unit")

# Open lid (moves to staging area in column 4)
reader.open_lid()
```

**Lid positions:**
- **Closed:** Lid covers detection unit on module
- **Open:** Lid stored in staging area (column 4)

**Critical:** Always call `close_lid()` before `initialize()`, even if lid is already closed.

## Initialization

The `initialize()` method configures the reader with measurement parameters.

### Single Wavelength Reading

Simplest mode - read at one wavelength:

```python
# Initialize for single wavelength
reader.close_lid()
reader.initialize(mode="single", wavelengths=[450])
reader.open_lid()
```

**Use case:** OD600 bacterial growth, single-color ELISA

### Single Wavelength with Reference

Normalize readings against a reference wavelength:

```python
# Initialize with reference wavelength
reader.close_lid()
reader.initialize(
    mode="single",
    wavelengths=[450],
    reference_wavelength=562
)
reader.open_lid()
```

**Use case:** Reduce background noise, normalize for plate artifacts

### Multi-Wavelength Reading

Read up to 6 wavelengths simultaneously:

```python
# Initialize for multiple wavelengths
reader.close_lid()
reader.initialize(
    mode="multi",
    wavelengths=[450, 562, 600, 650]
)
reader.open_lid()
```

**Use case:** Multi-analyte assays, spectral analysis, dual-wavelength ELISA

**Available wavelengths:**
- **450nm** - Blue (common ELISA substrate TMB)
- **562nm** - Green (BCA protein assay)
- **600nm** - Orange (bacterial growth OD600)
- **650nm** - Red (alternative reference wavelength)

**Maximum:** 6 wavelengths per reading

## Reading Plates

### Basic Reading

```python
# Move plate to reader with gripper
protocol.move_labware(assay_plate, reader, use_gripper=True)

# Close lid
reader.close_lid()

# Read plate
absorbance_data = reader.read()

# Open lid
reader.open_lid()

# Move plate off reader
protocol.move_labware(assay_plate, "C1", use_gripper=True)
```

### Reading with CSV Export

```python
# Read and export data
absorbance_data = reader.read(export_filename="experiment_001_plate1")

# CSV file saved to robot and accessible via Opentrons App
```

**Export format:** CSV file with plate layout, metadata (wavelengths, serial number, timestamps)

### Accessing Reading Results

Results are returned as nested dictionary: `{wavelength: {well: absorbance}}`

```python
# Read plate at 450nm
data = reader.read()

# Access specific well at 450nm
absorbance_a1 = data[450]["A1"]

protocol.comment(f"Well A1 absorbance at 450nm: {absorbance_a1}")

# Iterate through all wells at 600nm
for well_name, absorbance in data[600].items():
    protocol.comment(f"{well_name}: {absorbance}")
```

**Value range:** 0.0 - 4.0 OD (optical density)

### Multi-Wavelength Data Access

```python
# Initialize with multiple wavelengths
reader.initialize(mode="multi", wavelengths=[450, 600])

# Read plate
data = reader.read()

# Access different wavelengths
od450_a1 = data[450]["A1"]
od600_a1 = data[600]["A1"]

protocol.comment(f"A1: OD450={od450_a1}, OD600={od600_a1}")

# Calculate ratio
if od600_a1 > 0:
    ratio = od450_a1 / od600_a1
    protocol.comment(f"450/600 ratio: {ratio}")
```

## Data Formats

### In-Protocol Dictionary Format

```python
{
    450: {
        "A1": 0.123,
        "A2": 0.145,
        "A3": 0.167,
        # ... all 96 wells
        "H12": 0.089
    },
    600: {
        "A1": 0.456,
        # ... all 96 wells
    }
}
```

**Structure:**
- Top level: Wavelengths (450, 562, 600, 650)
- Second level: Well names ("A1" - "H12")
- Values: Absorbance (0.0 - 4.0 OD)

### CSV Export Format

```csv
Opentrons Flex Absorbance Plate Reader
Serial: ABC123456
Wavelength: 450nm
Date: 2024-11-09
Time: 14:32:01

    1      2      3      4      5      6  ...  12
A  0.123  0.145  0.167  0.189  0.201  0.223  ...  0.089
B  0.234  0.256  0.278  0.290  0.312  0.334  ...  0.190
C  0.345  0.367  0.389  0.401  0.423  0.445  ...  0.291
...
H  0.456  0.478  0.490  0.512  0.534  0.556  ...  0.392
```

**Metadata included:**
- Module serial number
- Wavelength(s) measured
- Timestamp
- Plate layout with row/column labels

## Complete Workflow Pattern

### Standard Reading Workflow

```python
# 1. Load and setup
reader = protocol.load_module("absorbanceReaderV1", "D3")
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "B1")
pipette = protocol.load_instrument("flex_8channel_1000", "left")

# 2. Initialize reader (empty)
reader.close_lid()
reader.initialize(mode="single", wavelengths=[450])
reader.open_lid()

# 3. Prepare samples
# ... pipetting operations ...

# 4. Move plate to reader
protocol.move_labware(plate, reader, use_gripper=True)

# 5. Read plate
reader.close_lid()
data = reader.read(export_filename="my_assay_data")
reader.open_lid()

# 6. Process results or continue protocol
for well in plate.wells():
    if data[450][well.well_name] > 1.0:
        protocol.comment(f"{well.well_name} is positive")

# 7. Move plate off reader
protocol.move_labware(plate, "C1", use_gripper=True)
```

## Common Patterns

### ELISA Protocol

```python
# Setup
reader = protocol.load_module("absorbanceReaderV1", "D3")
elisa_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")
pipette = protocol.load_instrument("flex_8channel_1000", "left")

# Initialize reader
reader.close_lid()
reader.initialize(mode="single", wavelengths=[450], reference_wavelength=562)
reader.open_lid()

# 1. Coat plate, incubate, wash (not shown)

# 2. Add samples
pipette.transfer(100, samples, elisa_plate.wells()[:24])

# 3. Incubate
protocol.delay(minutes=60)

# 4. Wash steps
for _ in range(3):
    # Wash logic (not shown)
    pass

# 5. Add substrate
pipette.transfer(100, substrate, elisa_plate.wells()[:24])

# 6. Incubate
protocol.delay(minutes=15)

# 7. Add stop solution
pipette.transfer(50, stop_solution, elisa_plate.wells()[:24])

# 8. Read plate
protocol.move_labware(elisa_plate, reader, use_gripper=True)
reader.close_lid()
elisa_data = reader.read(export_filename="elisa_450nm_results")
reader.open_lid()

# 9. Analyze results
for well in elisa_plate.wells()[:24]:
    od450 = elisa_data[450][well.well_name]
    if od450 > 0.5:
        protocol.comment(f"{well.well_name}: POSITIVE (OD450={od450:.3f})")
    else:
        protocol.comment(f"{well.well_name}: NEGATIVE (OD450={od450:.3f})")
```

### Bacterial Growth Kinetics

```python
# Setup
reader = protocol.load_module("absorbanceReaderV1", "D3")
culture_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")

# Initialize reader for OD600
reader.close_lid()
reader.initialize(mode="single", wavelengths=[600])
reader.open_lid()

# Inoculate cultures (not shown)

# Kinetic reading loop
for timepoint in range(8):  # 8 timepoints
    # Incubate on heater-shaker
    protocol.move_labware(culture_plate, hs_mod, use_gripper=True)
    hs_mod.set_and_wait_for_temperature(37)
    hs_mod.close_labware_latch()
    hs_mod.set_and_wait_for_shake_speed(300)
    protocol.delay(hours=1)
    hs_mod.deactivate_shaker()
    hs_mod.deactivate_heater()
    hs_mod.open_labware_latch()

    # Read OD600
    protocol.move_labware(culture_plate, reader, use_gripper=True)
    reader.close_lid()
    od_data = reader.read(export_filename=f"growth_curve_t{timepoint}")
    reader.open_lid()

    # Log growth
    for well in culture_plate.wells()[:8]:
        od600 = od_data[600][well.well_name]
        protocol.comment(f"T{timepoint}h {well.well_name}: OD600={od600:.3f}")

# Return plate to deck
protocol.move_labware(culture_plate, "B1", use_gripper=True)
```

### Multi-Wavelength Protein Assay

```python
# Setup for BCA or Bradford assay
reader = protocol.load_module("absorbanceReaderV1", "D3")
assay_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")

# Initialize for multi-wavelength
reader.close_lid()
reader.initialize(mode="multi", wavelengths=[562, 600])
reader.open_lid()

# Prepare standard curve + samples (not shown)

# Incubate
protocol.delay(minutes=30)

# Read at multiple wavelengths
protocol.move_labware(assay_plate, reader, use_gripper=True)
reader.close_lid()
absorbance = reader.read(export_filename="protein_assay_multi_wl")
reader.open_lid()

# Calculate protein concentrations using standard curve
for well in assay_plate.wells()[:24]:
    od562 = absorbance[562][well.well_name]
    od600 = absorbance[600][well.well_name]

    # Example: simple linear standard curve
    protein_conc = od562 * 100  # µg/mL (simplified)
    protocol.comment(f"{well.well_name}: {protein_conc:.1f} µg/mL")
```

### Real-Time Decision Making

```python
# Read plate and perform conditional operations
protocol.move_labware(screening_plate, reader, use_gripper=True)
reader.close_lid()
screen_data = reader.read()
reader.open_lid()
protocol.move_labware(screening_plate, "C1", use_gripper=True)

# Identify hits and cherry-pick
hit_wells = []
for well_name, absorbance in screen_data[450].items():
    if absorbance > 2.0:  # Hit threshold
        hit_wells.append(screening_plate.wells_by_name()[well_name])
        protocol.comment(f"HIT: {well_name} (OD450={absorbance:.3f})")

# Transfer hits to new plate for follow-up
if hit_wells:
    pipette.transfer(50, hit_wells, hit_plate.wells()[:len(hit_wells)])
    protocol.comment(f"Found {len(hit_wells)} hits - transferred for validation")
```

## Best Practices

1. **Always close lid before initialization** - Even if already closed
2. **Export data with descriptive filenames** - Include experiment ID, date, plate number
3. **Use reference wavelengths** - Reduces background and plate artifacts
4. **Read blank wells** - Establish baseline for background subtraction
5. **Include standard curves** - For quantitative assays
6. **Plan deck layout carefully** - Column 4 is reserved for lid storage
7. **Protect column 4** - Never attempt to load labware in staging area
8. **Allow plate equilibration** - Brief delay after moving to reader prevents condensation issues
9. **Check gripper compatibility** - Ensure plates have grippable geometry
10. **Document wavelength selection** - Comment why specific wavelengths were chosen

## Common Mistakes

**❌ Initializing with lid open:**
```python
reader.initialize(mode="single", wavelengths=[450])  # Error: lid must be closed
```

**✅ Correct:**
```python
reader.close_lid()
reader.initialize(mode="single", wavelengths=[450])
reader.open_lid()
```

**❌ Loading labware in column 4:**
```python
reader = protocol.load_module("absorbanceReaderV1", "D3")
tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "D4")  # Error: reserved
```

**✅ Correct:**
```python
reader = protocol.load_module("absorbanceReaderV1", "D3")
tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "D2")  # Use different column
```

**❌ Reading without closing lid:**
```python
protocol.move_labware(plate, reader, use_gripper=True)
data = reader.read()  # Error: lid must be closed for reading
```

**✅ Correct:**
```python
protocol.move_labware(plate, reader, use_gripper=True)
reader.close_lid()
data = reader.read()
reader.open_lid()
```

**❌ Using unsupported wavelength:**
```python
reader.initialize(mode="single", wavelengths=[550])  # Error: not supported
```

**✅ Correct:**
```python
reader.initialize(mode="single", wavelengths=[562])  # Use 450, 562, 600, or 650
```

**❌ Accessing wrong wavelength in data:**
```python
reader.initialize(mode="single", wavelengths=[450])
data = reader.read()
od600 = data[600]["A1"]  # Error: only 450nm was measured
```

**✅ Correct:**
```python
reader.initialize(mode="single", wavelengths=[450])
data = reader.read()
od450 = data[450]["A1"]  # Access wavelength that was measured
```

## Troubleshooting

**Module not loading:**
- Verify API version is 2.21 or later
- Check `robotType: "Flex"` in requirements
- Ensure module is in column 3 slot (A3-D3)

**Lid errors:**
- Always call `close_lid()` before `initialize()`
- Ensure column 4 staging area is clear
- Check gripper is functioning properly

**Reading errors:**
- Verify lid is closed before `read()`
- Check plate is on module
- Ensure wavelengths match those set in `initialize()`

**Data access errors:**
- Access wavelengths that were initialized
- Use correct well names ("A1" not "a1")
- Check data type (dict, not list)

**Gripper cannot move plate:**
- Verify plate has grippable geometry
- Ensure reader lid is open before moving plate onto/off module
- Check plate is compatible with reader (96-well plates)

## Integration with Other Modules

### With Heater-Shaker Module

```python
# Incubate with shaking, read absorbance
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
reader = protocol.load_module("absorbanceReaderV1", "D3")

# Setup plate on heater-shaker
protocol.move_labware(assay_plate, hs_mod, use_gripper=True)
hs_mod.set_and_wait_for_temperature(37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(400)
protocol.delay(minutes=30)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()

# Move to reader
protocol.move_labware(assay_plate, reader, use_gripper=True)
reader.close_lid()
data = reader.read()
reader.open_lid()
```

### With Temperature Module

```python
# Cool samples, then read
temp_mod = protocol.load_module("temperature module gen2", "D1")
reader = protocol.load_module("absorbanceReaderV1", "C3")

# Cool plate
protocol.move_labware(assay_plate, temp_mod, use_gripper=True)
temp_mod.set_temperature(4)
protocol.delay(minutes=5)

# Read cooled plate
protocol.move_labware(assay_plate, reader, use_gripper=True)
reader.close_lid()
data = reader.read()
reader.open_lid()

temp_mod.deactivate()
```

## CSV Data Export

Exported CSV files are stored on the robot and accessible via the Opentrons App:

1. Complete protocol run
2. Open Opentrons App
3. Navigate to completed protocol run
4. Download exported CSV files
5. Analyze in Excel, Python, R, or other tools

**File naming:** Use descriptive `export_filename` parameter for easy identification

```python
data = reader.read(export_filename=f"experiment_{exp_id}_plate_{plate_num}_timepoint_{t}")
```

## API Version Requirements

- **Minimum API version:** 2.21
- **Robot type:** Opentrons Flex only
- **Recommended:** Latest API version for full feature support

## Additional Resources

- **Module Documentation:** https://docs.opentrons.com/v2/modules/absorbance_plate_reader.html
- **Opentrons Support:** https://support.opentrons.com/
- **Protocol Examples:** https://protocols.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-gripper` - Automated labware movement (required for plate reader)
- `opentrons-heater-shaker` - Temperature control with mixing (common integration)
- `opentrons-temperature-module` - Temperature control for assays

---

### opentrons-gripper

**Path**: `skills/laboratory/opentrons-gripper/SKILL.md`

# Opentrons Flex Gripper

## Overview

The **Opentrons Flex Gripper** enables fully automated labware movement without manual intervention. It moves plates, reservoirs, and tip racks between deck slots, hardware modules, trash/waste chute, and off-deck storage with precise positioning and optional offset adjustments.

**Core value:** Eliminate manual plate transfers during protocols. Automatically move labware between modules, enabling complex multi-step workflows without pausing for user intervention.

**Platform:** Opentrons Flex only (not available on OT-2)

## When to Use

Use the Gripper skill when:
- Moving labware between deck locations automatically
- Transferring plates between hardware modules (Temperature, Heater-Shaker, Magnetic Block, etc.)
- Loading plates onto Absorbance Reader or Thermocycler
- Disposing labware in waste chute
- Moving labware to/from off-deck storage
- Automating workflows that would otherwise require manual intervention

**Don't use when:**
- Working with OT-2 robot (gripper is Flex-only)
- Labware is not grippable (lacks compatible geometry/features)
- Manual control is preferred for delicate operations
- Pipette can accomplish the task without moving entire labware

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Move labware | `protocol.move_labware()` | labware, new_location, use_gripper=True |
| Move with offsets | `protocol.move_labware()` | pick_up_offset, drop_offset |
| Move to module | `protocol.move_labware()` | labware, module_object, use_gripper=True |
| Dispose in waste | `protocol.move_labware()` | labware, waste_chute, use_gripper=True |
| Move off-deck | `protocol.move_labware()` | labware, OFF_DECK, use_gripper=True |
| Manual movement | `protocol.move_labware()` | use_gripper=False (pauses for user) |

## Platform Requirements

**Opentrons Flex only**
- **API version:** 2.15+ (gripper support)
- **Robot type:** Must specify `"robotType": "Flex"`

## Basic Usage

### Moving Between Deck Slots

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.19'}
requirements = {"robotType": "Flex", "apiLevel": "2.19"}

def run(protocol: protocol_api.ProtocolContext):
    # Load labware
    source_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C1")

    # Move labware to new deck slot
    protocol.move_labware(
        labware=source_plate,
        new_location="D2",
        use_gripper=True
    )
```

**Deck slot formats (both work):**
- Coordinate format: "A1", "B2", "C3", "D1" (preferred for Flex)
- Numeric format: "1", "2", "3", etc. (OT-2 legacy, still compatible)

### Moving to Hardware Module

```python
# Load module and labware
temp_mod = protocol.load_module("temperature module gen2", "D1")
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")

# Move plate to temperature module
protocol.move_labware(
    labware=plate,
    new_location=temp_mod,
    use_gripper=True
)
```

### Moving to Waste Chute

```python
# Load waste chute
waste_chute = protocol.load_waste_chute()

# Dispose of used plate
protocol.move_labware(
    labware=used_plate,
    new_location=waste_chute,
    use_gripper=True
)
```

### Moving Off-Deck

```python
from opentrons.protocol_api import OFF_DECK

# Move labware off the deck (to storage)
protocol.move_labware(
    labware=archive_plate,
    new_location=OFF_DECK,
    use_gripper=True
)

# Later: bring back from off-deck storage
protocol.move_labware(
    labware=archive_plate,
    new_location="B3",
    use_gripper=True
)
```

**Use case:** Free up deck space by temporarily storing labware off-deck

## Position Offsets

Fine-tune pickup and drop positions with offset vectors:

### Basic Offset Syntax

```python
protocol.move_labware(
    labware=plate,
    new_location="D2",
    use_gripper=True,
    pick_up_offset={"x": 0, "y": 0, "z": 2},  # Lift 2mm higher when picking up
    drop_offset={"x": 0, "y": 0, "z": 1}      # Place 1mm higher when dropping
)
```

**Offset units:** Millimeters
**Coordinates:**
- **x:** Left (-) / Right (+)
- **y:** Back (-) / Front (+)
- **z:** Down (-) / Up (+)

### When to Use Offsets

**Pick-up offset:**
- Labware sits higher/lower than expected
- Adapters or custom labware with non-standard height
- Ensure proper gripper engagement

**Drop-off offset:**
- Ensure proper seating on module or adapter
- Accommodate non-standard landing surfaces
- Avoid collision with deck features

### Example with Custom Labware

```python
# Custom deep-well plate requires higher pickup
custom_plate = protocol.load_labware("custom_deepwell_96", "C1")

protocol.move_labware(
    labware=custom_plate,
    new_location="D1",
    use_gripper=True,
    pick_up_offset={"x": 0, "y": 0, "z": 3},  # Pickup 3mm higher
    drop_offset={"x": 0, "y": 0, "z": 0}      # Standard drop
)
```

## Manual Movement Alternative

Set `use_gripper=False` to pause protocol for user to manually move labware:

```python
# Pause for manual movement
protocol.move_labware(
    labware=delicate_plate,
    new_location="D3",
    use_gripper=False  # Protocol pauses, user moves plate manually
)
```

**Use case:** Delicate labware, non-grippable items, or troubleshooting

## Module Integration

### Temperature Module

```python
temp_mod = protocol.load_module("temperature module gen2", "D1")
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")

# Move to temperature module
protocol.move_labware(plate, temp_mod, use_gripper=True)

# Set temperature
temp_mod.set_temperature(4)
protocol.delay(minutes=5)

# Move off module
protocol.move_labware(plate, "C2", use_gripper=True)

temp_mod.deactivate()
```

### Heater-Shaker Module

**Critical:** Open labware latch before gripper operations

```python
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
adapter = hs_mod.load_adapter("opentrons_96_flat_bottom_adapter")
plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")

# Open latch BEFORE moving plate to module
hs_mod.open_labware_latch()

# Move plate to heater-shaker
protocol.move_labware(plate, hs_mod, use_gripper=True)

# Close latch for shaking
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_temperature(37)
hs_mod.set_and_wait_for_shake_speed(500)
protocol.delay(minutes=10)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()

# Open latch BEFORE removing plate
hs_mod.open_labware_latch()

# Move plate off module
protocol.move_labware(plate, "C2", use_gripper=True)
```

### Thermocycler Module

**Critical:** Open lid before gripper operations

```python
tc_mod = protocol.load_module("thermocyclerModuleV2")
plate = protocol.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt", "C2")

# Open lid BEFORE moving plate to thermocycler
tc_mod.open_lid()

# Move plate to thermocycler
protocol.move_labware(plate, tc_mod, use_gripper=True)

# Close lid and run PCR
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)
# ... PCR cycling ...

# Open lid BEFORE removing plate
tc_mod.open_lid()

# Move plate off thermocycler
protocol.move_labware(plate, "C2", use_gripper=True)
```

### Magnetic Block (Flex)

```python
mag_block = protocol.load_module("magneticBlockV1", "D1")
mag_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C2")

# Move plate to magnetic block for bead separation
protocol.move_labware(mag_plate, mag_block, use_gripper=True)

# Wait for beads to collect
protocol.delay(minutes=3)

# Pipette supernatant (beads held by magnets)
pipette.transfer(150, mag_plate.wells(), waste.wells())

# Move plate off magnets for resuspension
protocol.move_labware(mag_plate, "C2", use_gripper=True)
```

### Absorbance Plate Reader

**Critical:** Reader lid must be open before gripper operations

```python
reader = protocol.load_module("absorbanceReaderV1", "D3")
assay_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")

# Initialize reader
reader.close_lid()
reader.initialize(mode="single", wavelengths=[450])

# Open lid BEFORE moving plate to reader
reader.open_lid()

# Move plate to reader
protocol.move_labware(assay_plate, reader, use_gripper=True)

# Close lid and read
reader.close_lid()
data = reader.read()

# Open lid BEFORE removing plate
reader.open_lid()

# Move plate off reader
protocol.move_labware(assay_plate, "C2", use_gripper=True)
```

## Common Patterns

### Multi-Module Workflow

```python
# DNA extraction with module transfers
sample_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C1")
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
mag_block = protocol.load_module("magneticBlockV1", "D2")
temp_mod = protocol.load_module("temperature module gen2", "D3")

# 1. Lysis on heater-shaker
hs_mod.open_labware_latch()
protocol.move_labware(sample_plate, hs_mod, use_gripper=True)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_temperature(56)
hs_mod.set_and_wait_for_shake_speed(1000)
protocol.delay(minutes=15)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()

# 2. Bead binding on magnetic block
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)
pipette.transfer(150, sample_plate.wells(), waste.wells())

# 3. Elution on temperature module (cold)
protocol.move_labware(sample_plate, temp_mod, use_gripper=True)
temp_mod.set_temperature(4)
pipette.transfer(50, elution_buffer, sample_plate.wells())

# 4. Return to deck
protocol.move_labware(sample_plate, "C1", use_gripper=True)
temp_mod.deactivate()
```

### Plate Stacking Workflow

```python
# Process multiple plates sequentially
plates = [
    protocol.load_labware("corning_96_wellplate_360ul_flat", "C1"),
    protocol.load_labware("corning_96_wellplate_360ul_flat", "C2"),
    protocol.load_labware("corning_96_wellplate_360ul_flat", "C3")
]

reader = protocol.load_module("absorbanceReaderV1", "D3")
reader.close_lid()
reader.initialize(mode="single", wavelengths=[600])
reader.open_lid()

for i, plate in enumerate(plates):
    # Read each plate
    protocol.move_labware(plate, reader, use_gripper=True)
    reader.close_lid()
    data = reader.read(export_filename=f"plate_{i+1}_od600")
    reader.open_lid()

    # Move to off-deck storage
    protocol.move_labware(plate, OFF_DECK, use_gripper=True)
```

### Waste Management

```python
# Dispose of used consumables
waste_chute = protocol.load_waste_chute()

# After using tip rack
protocol.move_labware(empty_tips, waste_chute, use_gripper=True)

# After processing sample plate
protocol.move_labware(used_plate, waste_chute, use_gripper=True)
```

### Dynamic Deck Space Management

```python
# Free up deck space by moving inactive labware off-deck
storage = OFF_DECK

# Initial setup - plates start on deck
plate_1 = protocol.load_labware("corning_96_wellplate_360ul_flat", "B1")
plate_2 = protocol.load_labware("corning_96_wellplate_360ul_flat", "B2")
plate_3 = protocol.load_labware("corning_96_wellplate_360ul_flat", "B3")

# Process plate 1
# ... operations on plate_1 ...

# Move plate 1 off-deck to free space
protocol.move_labware(plate_1, storage, use_gripper=True)

# Process plate 2
# ... operations on plate_2 ...

# Retrieve plate 1 when needed again
protocol.move_labware(plate_1, "B1", use_gripper=True)
```

## Grippable Labware

**The gripper can only move labware with compatible geometry.**

### Compatible Features

Labware must have one of:
- **Gripper-compatible rim** - Extended lip or ridge for gripper jaws
- **Side grips** - Indentations or features on sides
- **Opentrons-certified labware** - Verified gripper compatibility

### Checking Compatibility

Most Opentrons labware and major brands (Corning, NEST, Axygen, Bio-Rad) are gripper-compatible. Check labware definition or test before protocol deployment.

### Non-Grippable Labware

Examples of labware that may NOT be grippable:
- Very small tubes or vials without grip features
- Irregularly shaped containers
- Custom labware without gripper considerations
- Delicate or fragile items

**Solution:** Use `use_gripper=False` to prompt manual movement, or use adapters.

## Best Practices

1. **Open module lids/latches before gripper movement** - Required for Heater-Shaker, Thermocycler, Absorbance Reader
2. **Use descriptive variable names** - Track labware clearly through movements
3. **Test with simulation first** - Verify gripper movements before running on hardware
4. **Add protocol comments** - Document why labware is being moved
5. **Plan deck layout** - Minimize unnecessary movements, optimize for efficiency
6. **Use OFF_DECK strategically** - Free up deck space for complex protocols
7. **Check labware compatibility** - Ensure labware is grippable before deploying
8. **Use offsets judiciously** - Only when necessary for proper positioning
9. **Consider manual fallback** - Have `use_gripper=False` backup for troubleshooting
10. **Dispose properly** - Use waste chute for used consumables to maintain workspace

## Common Mistakes

**❌ Moving to Heater-Shaker with closed latch:**
```python
protocol.move_labware(plate, hs_mod, use_gripper=True)  # Error: latch closed
```

**✅ Correct:**
```python
hs_mod.open_labware_latch()
protocol.move_labware(plate, hs_mod, use_gripper=True)
```

**❌ Moving to Thermocycler with closed lid:**
```python
protocol.move_labware(plate, tc_mod, use_gripper=True)  # Error: lid closed
```

**✅ Correct:**
```python
tc_mod.open_lid()
protocol.move_labware(plate, tc_mod, use_gripper=True)
```

**❌ Moving non-grippable labware:**
```python
# Custom labware without gripper features
protocol.move_labware(custom_tubes, "D2", use_gripper=True)  # Error: cannot grip
```

**✅ Correct:**
```python
# Use manual movement for non-grippable labware
protocol.move_labware(custom_tubes, "D2", use_gripper=False)
```

**❌ Forgetting to specify use_gripper:**
```python
protocol.move_labware(plate, "D2")  # Defaults to use_gripper=False, pauses protocol
```

**✅ Correct:**
```python
protocol.move_labware(plate, "D2", use_gripper=True)  # Explicit gripper use
```

**❌ Moving to occupied location:**
```python
plate_1 = protocol.load_labware("corning_96_wellplate_360ul_flat", "C1")
plate_2 = protocol.load_labware("corning_96_wellplate_360ul_flat", "C2")
protocol.move_labware(plate_1, "C2", use_gripper=True)  # Error: C2 occupied by plate_2
```

**✅ Correct:**
```python
# Move plate_2 first, then plate_1
protocol.move_labware(plate_2, OFF_DECK, use_gripper=True)
protocol.move_labware(plate_1, "C2", use_gripper=True)
```

## Troubleshooting

**Gripper cannot grip labware:**
- Verify labware has gripper-compatible features
- Check labware is properly seated on deck/module
- Try adjusting pick_up_offset (increase z)
- Use `use_gripper=False` for non-compatible labware

**Gripper collision errors:**
- Ensure module lids are open (Thermocycler, Absorbance Reader)
- Verify Heater-Shaker latch is open
- Check deck layout for obstructions
- Confirm destination location is clear

**Labware not properly seated after movement:**
- Adjust drop_offset to ensure proper placement
- Check module/adapter compatibility with labware
- Verify destination surface is level and clear

**Protocol pauses unexpectedly:**
- Check that `use_gripper=True` is specified
- Verify gripper is enabled for protocol
- Ensure labware is grippable

**OFF_DECK movement errors:**
- Import OFF_DECK constant: `from opentrons.protocol_api import OFF_DECK`
- Track labware location - cannot move from OFF_DECK if not there
- Ensure sufficient off-deck storage slots

## Advanced Techniques

### Conditional Gripper Use

```python
# Use gripper for standard labware, manual for custom
def move_smart(protocol, labware, destination, is_grippable=True):
    if is_grippable:
        protocol.move_labware(labware, destination, use_gripper=True)
    else:
        protocol.comment(f"Please manually move {labware} to {destination}")
        protocol.move_labware(labware, destination, use_gripper=False)

move_smart(protocol, standard_plate, "D2", is_grippable=True)
move_smart(protocol, custom_tubes, "D3", is_grippable=False)
```

### Offset Calibration Helper

```python
# Test offsets to find optimal values
def test_gripper_offset(protocol, labware, destination, z_offset_range):
    for z in z_offset_range:
        protocol.comment(f"Testing pickup offset z={z}")
        protocol.move_labware(
            labware,
            destination,
            use_gripper=True,
            pick_up_offset={"x": 0, "y": 0, "z": z}
        )
        protocol.pause("Check if pickup was successful. Resume to continue.")
        # Move back
        protocol.move_labware(labware, "C1", use_gripper=True)

# Run test
test_gripper_offset(protocol, test_plate, "D2", z_offset_range=[0, 1, 2, 3])
```

### Plate Carousel Pattern

```python
# Rotate plates through processing stations
def process_plate_carousel(protocol, plates, processing_station):
    """Process multiple plates through single module."""
    for i, plate in enumerate(plates):
        protocol.comment(f"Processing plate {i+1}/{len(plates)}")

        # Move to processing station
        protocol.move_labware(plate, processing_station, use_gripper=True)

        # Process (example: read absorbance)
        # ... processing steps ...

        # Move to archive
        protocol.move_labware(plate, OFF_DECK, use_gripper=True)

process_plate_carousel(protocol, [plate1, plate2, plate3], reader_module)
```

## Deck Layout Planning

**Tips for efficient gripper workflows:**

1. **Keep high-traffic locations accessible** - Place frequently moved labware in central deck positions
2. **Group modules logically** - Arrange modules in workflow order to minimize travel distance
3. **Reserve column 4 for Absorbance Reader** - If using reader, entire column 4 is staging area
4. **Use OFF_DECK for storage** - Free up deck space for active labware
5. **Plan waste chute access** - Ensure gripper can reach waste chute without obstacles

**Example efficient layout:**
```
     1          2          3          4
A  [Plates]  [Module1]  [Reader]  [Reserved]
B  [Plates]  [Module2]  [Reader]  [Reserved]
C  [Tips]    [Reagent]  [Reader]  [Reserved]
D  [Tips]    [Waste]    [Reader]  [Reserved]
```

## Integration with Staging Area Slots

Flex staging area slots (column 4: A4, B4, C4, D4) are used for:
- Absorbance Reader lid storage (automatic)
- Temporary labware holding
- Gripper intermediate positions

**Important:** When Absorbance Reader is loaded, column 4 cannot be used for labware.

## API Version Requirements

- **Minimum API version:** 2.15 (gripper support introduced)
- **Recommended:** 2.19+ for full feature support
- **Robot type:** Must be Opentrons Flex

## Additional Resources

- **Gripper Documentation:** https://docs.opentrons.com/v2/new_protocol_api.html#protocol_api.ProtocolContext.move_labware
- **Labware Library:** https://labware.opentrons.com/ (check gripper compatibility)
- **Opentrons Support:** https://support.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-heater-shaker` - Heater-Shaker Module (requires gripper integration)
- `opentrons-absorbance-reader` - Absorbance Plate Reader (requires gripper)
- `opentrons-magnetic-block` - Magnetic Block (designed for gripper workflow)
- `opentrons-thermocycler` - Thermocycler Module (gripper-compatible)
- `opentrons-temperature-module` - Temperature Module (gripper-compatible)

---

### opentrons-heater-shaker

**Path**: `skills/laboratory/opentrons-heater-shaker/SKILL.md`

# Opentrons Heater-Shaker Module

## Overview

The **Heater-Shaker Module** combines precise temperature control (37-95°C) with orbital shaking (200-3000 rpm) for automated protocols requiring simultaneous heating and mixing. Ideal for cell culture incubation, enzymatic reactions, bacterial transformations, and any workflow needing temperature-controlled agitation.

**Core value:** Automate temperature-sensitive mixing protocols with reproducible timing and conditions. Replace manual incubator transfers with on-deck temperature control.

## When to Use

Use the Heater-Shaker skill when:
- Performing enzymatic reactions requiring temperature control and mixing
- Incubating cell cultures or bacterial transformations
- Conducting ELISA washing steps with temperature control
- Running temperature-sensitive binding or hybridization reactions
- Protocols requiring precise timing of heating + mixing cycles
- Resuspending beads or particles with controlled agitation

**Don't use when:**
- Only temperature control needed (use Temperature Module instead)
- Only mixing needed (consider manual shaking or alternative approaches)
- Temperature outside 37-95°C range (or <1.5°C above ambient)

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Load module | `protocol.load_module()` | `"heaterShakerModuleV1"`, location |
| Set temperature (blocking) | `set_and_wait_for_temperature()` | celsius (37-95) |
| Set temperature (non-blocking) | `set_target_temperature()` | celsius |
| Wait for temperature | `wait_for_temperature()` | - |
| Start shaking (blocking) | `set_and_wait_for_shake_speed()` | rpm (200-3000) |
| Stop shaking | `deactivate_shaker()` | - |
| Stop heating | `deactivate_heater()` | - |
| Open latch | `open_labware_latch()` | - |
| Close latch | `close_labware_latch()` | - |
| Check status | `.current_temperature`, `.current_speed` | - |

## Platform Compatibility

### Opentrons Flex
- **Allowed slots:** Column 1 or Column 3 (A1, B1, C1, D1, A3, B3, C3, D3)
- **No adjacent restrictions** - Full deck flexibility

### OT-2
- **Allowed slots:** 1, 3, 4, 6, 7, or 10
- **Adjacent slot restrictions:**
  - Keep adjacent slots clear or use only low-profile labware (<53mm height)
  - 8-channel pipettes cannot pipette in adjacent slots (tip racks OK if front/back)

## Loading the Module

### Basic Loading

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.19'}

def run(protocol: protocol_api.ProtocolContext):
    # Load Heater-Shaker in deck slot
    hs_mod = protocol.load_module("heaterShakerModuleV1", location="D1")  # Flex
    # hs_mod = protocol.load_module("heaterShakerModuleV1", location="1")  # OT-2
```

### Loading Labware with Adapters

**Two-step approach (recommended):**

```python
# Load adapter first, then labware
hs_adapter = hs_mod.load_adapter("opentrons_96_flat_bottom_adapter")
hs_plate = hs_adapter.load_labware("nest_96_wellplate_200ul_flat")
```

**Available adapters:**
- `opentrons_96_flat_bottom_adapter` - Universal flat-bottom plates
- `opentrons_96_pcr_adapter` - PCR plates and strips
- `opentrons_96_deep_well_adapter` - Deep-well plates
- `opentrons_universal_flat_adapter` - Custom flat-bottom labware

**Pre-configured combinations (legacy):**

```python
# Load labware directly (adapter implicit)
hs_plate = hs_mod.load_labware("nest_96_wellplate_200ul_flat")
```

## Latch Control

**The labware latch MUST be closed for shaking operations.**

```python
# Close latch before shaking
hs_mod.close_labware_latch()

# Open latch for pipetting or gripper access
hs_mod.open_labware_latch()
```

**Important:**
- Pipetting is possible with latch open or closed
- Shaking requires latch closed (method will error if open)
- Gripper access requires latch open
- **Always open latch before gripper operations**

## Temperature Control

### Blocking Temperature Control

Protocol waits until temperature is reached before continuing:

```python
# Set temperature and wait
hs_mod.set_and_wait_for_temperature(celsius=37)

# Perform operations at target temperature
protocol.delay(minutes=10)

# Turn off heater
hs_mod.deactivate_heater()
```

**Temperature range:**
- **Minimum:** 37°C or 1.5°C above ambient temperature (whichever is lower)
- **Maximum:** 95°C
- **Resolution:** 1°C

### Non-Blocking Temperature Control

Set target temperature and continue with other operations while heating:

```python
# Start heating (non-blocking)
hs_mod.set_target_temperature(celsius=75)

# Perform pipetting while heating
pipette.transfer(100, source, hs_plate.columns()[0])

# Wait for temperature before critical step
hs_mod.wait_for_temperature()

# Now at target temperature
protocol.delay(minutes=5)
```

**Use case:** Parallel pipetting during heating to save time.

### Checking Temperature Status

```python
# Get current temperature
current_temp = hs_mod.current_temperature

# Log temperature
protocol.comment(f"Heater-Shaker at {current_temp}°C")
```

## Shaking Control

### Basic Shaking

```python
# Close latch first (required)
hs_mod.close_labware_latch()

# Start shaking and wait to reach speed
hs_mod.set_and_wait_for_shake_speed(rpm=500)

# Shake for specific duration
protocol.delay(minutes=5)

# Stop shaking
hs_mod.deactivate_shaker()

# Open latch for pipetting access
hs_mod.open_labware_latch()
```

**Speed range:**
- **Minimum:** 200 rpm
- **Maximum:** 3000 rpm

### Checking Shake Speed

```python
# Get current speed
current_speed = hs_mod.current_speed

if current_speed > 0:
    protocol.comment(f"Shaking at {current_speed} rpm")
```

### Shake Speed Guidelines

**Recommended speeds by application:**
- **Gentle mixing:** 200-500 rpm
- **Cell culture:** 300-600 rpm
- **Enzymatic reactions:** 400-800 rpm
- **Vigorous mixing:** 800-1500 rpm
- **Aggressive agitation:** 1500-3000 rpm

**Note:** Higher speeds increase risk of splashing and cross-contamination.

## Combined Heating and Shaking

### Sequential Approach

```python
# Set temperature first
hs_mod.set_and_wait_for_temperature(celsius=37)

# Then start shaking
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=400)

# Incubate with heating and shaking
protocol.delay(minutes=30)

# Stop shaking, then heating
hs_mod.deactivate_shaker()
hs_mod.open_labware_latch()
hs_mod.deactivate_heater()
```

### Parallel Approach (Time-Optimized)

```python
# Start heating (non-blocking)
hs_mod.set_target_temperature(celsius=42)

# Prepare samples while heating
pipette.transfer(100, samples, hs_plate.wells())

# Wait for temperature
hs_mod.wait_for_temperature()

# Start shaking
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=600)

# Incubate
protocol.delay(minutes=20)

# Cleanup
hs_mod.deactivate_shaker()
hs_mod.open_labware_latch()
hs_mod.deactivate_heater()
```

## Common Patterns

### Bacterial Transformation

```python
# Heat shock protocol
hs_mod.set_and_wait_for_temperature(celsius=4)
protocol.delay(minutes=20)  # Ice incubation

# Heat shock
hs_mod.set_and_wait_for_temperature(celsius=42)
protocol.delay(seconds=45)

# Recovery
hs_mod.set_and_wait_for_temperature(celsius=37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=300)
protocol.delay(minutes=60)

hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()
```

### Enzymatic Reaction

```python
# Pre-warm to reaction temperature
hs_mod.set_and_wait_for_temperature(celsius=37)

# Add enzyme while at temperature
hs_mod.open_labware_latch()
pipette.transfer(10, enzyme, hs_plate.wells(), mix_after=(3, 50))

# Incubate with gentle mixing
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=300)
protocol.delay(minutes=30)

# Stop reaction by cooling
hs_mod.deactivate_shaker()
hs_mod.open_labware_latch()
hs_mod.set_and_wait_for_temperature(celsius=4)

hs_mod.deactivate_heater()
```

### ELISA Wash with Incubation

```python
# Incubate with antibody
hs_mod.set_and_wait_for_temperature(celsius=37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=400)
protocol.delay(minutes=60)

# Stop for washing
hs_mod.deactivate_shaker()
hs_mod.open_labware_latch()

# Wash steps
for _ in range(3):
    # Remove liquid
    pipette.transfer(200, hs_plate.wells(), waste, new_tip="always")
    # Add wash buffer
    pipette.transfer(200, wash_buffer, hs_plate.wells())
    # Mix
    hs_mod.close_labware_latch()
    hs_mod.set_and_wait_for_shake_speed(rpm=500)
    protocol.delay(seconds=30)
    hs_mod.deactivate_shaker()
    hs_mod.open_labware_latch()

hs_mod.deactivate_heater()
```

### Bead Resuspension

```python
# Resuspend magnetic beads
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=1200)
protocol.delay(minutes=2)
hs_mod.deactivate_shaker()
hs_mod.open_labware_latch()

# Transfer to magnetic module for separation
protocol.move_labware(hs_plate, mag_block, use_gripper=True)
```

## Advanced Techniques

### Temperature Ramping

```python
# Gradual temperature increase
for temp in [25, 35, 45, 55, 65]:
    hs_mod.set_and_wait_for_temperature(temp)
    protocol.delay(minutes=5)

hs_mod.deactivate_heater()
```

### Precise Timing with Manual Tracking

For holds requiring exact elapsed time:

```python
import time

# Set conditions
hs_mod.set_and_wait_for_temperature(celsius=37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=500)

# Track elapsed time
start_time = time.monotonic()

# Perform operations
# ... your protocol steps ...

# Calculate remaining time
elapsed = time.monotonic() - start_time
remaining = max(0, (10 * 60) - elapsed)  # 10 minutes total

# Complete the hold
protocol.delay(seconds=remaining)

hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()
```

### Integration with Gripper (Flex)

```python
# Heat/shake on Heater-Shaker
hs_mod.set_and_wait_for_temperature(celsius=37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(rpm=400)
protocol.delay(minutes=30)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()

# Open latch BEFORE gripper operation
hs_mod.open_labware_latch()

# Move plate to next module
protocol.move_labware(hs_plate, magnetic_block, use_gripper=True)
```

## OT-2 Specific Considerations

### Deck Layout Restrictions

**Height restrictions:**
- Adjacent slots must be empty OR contain labware <53mm tall
- Tall labware (>53mm) in adjacent slots interferes with module

**8-channel pipette restrictions:**
- Cannot pipette in slots adjacent to Heater-Shaker
- Exception: Tip racks allowed if oriented front-to-back (not left-to-right)

**Example valid OT-2 layout:**
```python
# Heater-Shaker in slot 4
hs_mod = protocol.load_module("heaterShakerModuleV1", "4")

# Adjacent slots 1, 5, 7 - keep empty or use tip racks
tips_1 = protocol.load_labware("opentrons_96_tiprack_300ul", "1")  # OK
tips_5 = protocol.load_labware("opentrons_96_tiprack_300ul", "5")  # OK

# Use non-adjacent slots for plates
plate_2 = protocol.load_labware("corning_96_wellplate_360ul_flat", "2")
plate_3 = protocol.load_labware("corning_96_wellplate_360ul_flat", "3")
```

## Best Practices

1. **Always open latch before gripper operations** - Prevents gripper errors
2. **Deactivate shaker before heater** - Safer shutdown sequence
3. **Use non-blocking temperature for parallel operations** - Saves protocol time
4. **Close latch before shaking** - Required for operation
5. **Allow temperature stabilization** - Add brief delay after reaching target
6. **Monitor shake speed selection** - Higher speeds risk splashing/contamination
7. **Consider ambient temperature** - Affects minimum achievable temperature
8. **Plan deck layout (OT-2)** - Account for adjacent slot restrictions
9. **Add protocol comments** - Document temperature/shake conditions for reproducibility
10. **Deactivate at protocol end** - Prevents equipment running indefinitely

## Common Mistakes

**❌ Shaking with open latch:**
```python
hs_mod.open_labware_latch()
hs_mod.set_and_wait_for_shake_speed(500)  # Error: latch must be closed
```

**✅ Correct:**
```python
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(500)
```

**❌ Gripper movement without opening latch:**
```python
protocol.move_labware(hs_plate, "C1", use_gripper=True)  # Error: latch closed
```

**✅ Correct:**
```python
hs_mod.open_labware_latch()
protocol.move_labware(hs_plate, "C1", use_gripper=True)
```

**❌ Not deactivating modules:**
```python
hs_mod.set_and_wait_for_temperature(37)
# Protocol ends - heater stays on!
```

**✅ Correct:**
```python
hs_mod.set_and_wait_for_temperature(37)
# ... operations ...
hs_mod.deactivate_heater()
```

**❌ Temperature out of range:**
```python
hs_mod.set_and_wait_for_temperature(celsius=25)  # May error if ambient is >23.5°C
```

**✅ Correct:**
```python
# Use Temperature Module for temperatures <37°C
temp_mod.set_temperature(celsius=25)
```

## Troubleshooting

**Module won't heat below 37°C:**
- Check ambient temperature - minimum is 1.5°C above ambient
- Consider using Temperature Module for lower temperatures

**Shaking command errors:**
- Verify latch is closed
- Check shake speed is within 200-3000 rpm range

**Pipette collisions (OT-2):**
- Check adjacent slots for tall labware
- Move tall labware to non-adjacent slots
- Use only tip racks in adjacent slots for 8-channel pipetting

**Temperature not stable:**
- Allow additional time for equilibration
- Verify module is not in high-airflow environment
- Check labware is properly seated on adapter

**Gripper cannot access:**
- Ensure latch is open
- Verify module is in allowed deck slot (columns 1 or 3 for Flex)

## Integration with Other Modules

### With Magnetic Block (Flex)

```python
# Incubate with beads on Heater-Shaker
hs_mod.set_and_wait_for_temperature(37)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(400)
protocol.delay(minutes=10)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()

# Move to Magnetic Block for separation
protocol.move_labware(hs_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)  # Bead separation

# Pipette supernatant
pipette.transfer(150, hs_plate.wells(), waste.wells())

# Return to Heater-Shaker for resuspension
protocol.move_labware(hs_plate, hs_mod, use_gripper=True)
```

### With Temperature Module

```python
# Cool samples on Temperature Module
temp_mod.set_temperature(4)
protocol.move_labware(cold_plate, temp_mod, use_gripper=True)

# Heat and shake on Heater-Shaker
hs_mod.set_and_wait_for_temperature(65)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(600)
protocol.delay(minutes=15)
```

## API Version Requirements

- **Minimum API version:** 2.13
- **Recommended:** 2.19 or later for full feature support
- **Flex compatibility:** API 2.15+ with `robotType: "Flex"`

## Additional Resources

- **Module Documentation:** https://docs.opentrons.com/v2/modules/heater_shaker.html
- **Opentrons Support:** https://support.opentrons.com/
- **Protocol Examples:** https://protocols.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-temperature-module` - Temperature-only control (4-95°C)
- `opentrons-thermocycler` - PCR thermal cycling
- `opentrons-magnetic-block` - Magnetic bead separation (Flex)
- `opentrons-gripper` - Automated labware movement (Flex)

---

### opentrons-magnetic-block

**Path**: `skills/laboratory/opentrons-magnetic-block/SKILL.md`

# Opentrons Magnetic Block

## Overview

The **Opentrons Magnetic Block** is an unpowered, 96-well magnetic separator exclusively for Opentrons Flex. It uses high-strength neodymium magnets to pull magnetic beads to the side of wells, enabling supernatant removal for DNA/RNA purification, protein cleanup, immunoprecipitation, and other bead-based separation protocols.

**Core value:** Automate magnetic bead-based purification workflows without manual plate transfers. Combine gripper-based plate movement with magnetic separation for fully automated DNA extraction, PCR cleanup, and bead-based assays.

**Platform:** Opentrons Flex only (not compatible with OT-2)

## When to Use

Use the Magnetic Block skill when:
- Performing DNA/RNA extraction with magnetic beads
- PCR product cleanup (AMPure, SPRIselect beads)
- Magnetic bead-based library preparation (NGS)
- Protein purification or immunoprecipitation with magnetic beads
- Cell sorting or enrichment with magnetic particles
- Any protocol requiring magnetic bead separation

**Don't use when:**
- Working with OT-2 robot (Magnetic Block is Flex-only)
- Need vertical bead movement within solution (block only pulls to side)
- Require heated magnetic separation
- Working without Opentrons Flex gripper

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Load module | `protocol.load_module()` | `"magneticBlockV1"`, location |
| Move plate to block | `protocol.move_labware()` | labware, mag_block, use_gripper=True |
| Wait for separation | `protocol.delay()` | minutes (typically 2-5) |
| Pipette supernatant | `pipette.transfer()` | source, dest |
| Move plate off block | `protocol.move_labware()` | labware, new_location, use_gripper=True |

## Platform Requirements

**Opentrons Flex only**
- **API version:** 2.15 or later (Magnetic Block introduced)
- **Gripper required:** All plate movement uses gripper
- **Robot type:** Must specify `"robotType": "Flex"`

## Key Characteristics

### Unpowered Design

**Important:** The Magnetic Block is completely unpowered:
- No electronic control
- No on/off switching
- Always magnetic when plate is present
- Robot and Opentrons App are **not aware** of magnetic state

**Implication:** All control is manual through:
- Physical plate positioning (gripper movement)
- Timed delays for bead separation
- Pipetting operations

## Loading the Module

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.19'}
requirements = {"robotType": "Flex", "apiLevel": "2.19"}

def run(protocol: protocol_api.ProtocolContext):
    # Load Magnetic Block
    mag_block = protocol.load_module(
        module_name="magneticBlockV1",
        location="D1"
    )

    # Load compatible labware
    mag_plate = mag_block.load_labware(
        name="biorad_96_wellplate_200ul_pcr"
    )
```

**Compatible deck slots:** Any Flex slot (A1-D3)
**Recommended:** D1, D2, or D3 (bottom row for workflow efficiency)

## Compatible Labware

The Magnetic Block works with 96-well plates that fit the magnetic footprint:

**Common compatible labware:**
- `biorad_96_wellplate_200ul_pcr`
- `nest_96_wellplate_100ul_pcr_full_skirt`
- `opentrons_96_wellplate_200ul_pcr_full_skirt`
- Other 96-well PCR plates with standard footprint

**Important:**
- Labware must be grippable (compatible with Flex gripper)
- Wells must align with magnet positions
- Standard 96-well footprint required

## Basic Magnetic Separation Workflow

### Step 1: Load Sample Plate

```python
# Start with sample plate on deck (not on magnetic block yet)
sample_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C1")

# Add magnetic beads and binding buffer
pipette.transfer(10, bead_stock, sample_plate.wells())
pipette.transfer(100, binding_buffer, sample_plate.wells(), mix_after=(5, 80))
```

### Step 2: Incubate for Binding

```python
# Incubate to allow beads to bind target (DNA/RNA/protein)
protocol.delay(minutes=5)

# Optional: Mix during incubation (use Heater-Shaker)
```

### Step 3: Move to Magnetic Block

```python
# Move plate to magnetic block using gripper
protocol.move_labware(
    labware=sample_plate,
    new_location=mag_block,
    use_gripper=True
)
```

### Step 4: Wait for Bead Separation

```python
# Allow beads to collect at side of wells
protocol.delay(minutes=3)

# Beads are now held at well edges by magnets
```

### Step 5: Remove Supernatant

```python
# Pipette supernatant while beads are held by magnets
pipette.transfer(
    volume=110,
    source=sample_plate.wells(),
    dest=waste.wells(),
    new_tip="always"
)

# Beads remain in wells, held by magnetic field
```

### Step 6: Move Off Magnetic Block

```python
# Move plate off magnets for bead resuspension
protocol.move_labware(
    labware=sample_plate,
    new_location="C2",
    use_gripper=True
)

# Beads are now free to resuspend
```

### Step 7: Wash and Repeat

```python
# Add wash buffer
pipette.transfer(150, wash_buffer, sample_plate.wells())

# Mix to resuspend beads
pipette.mix(repetitions=5, volume=100, location=sample_plate.wells())

# Return to magnetic block
protocol.move_labware(sample_plate, mag_block, use_gripper=True)

# Wait for separation
protocol.delay(minutes=2)

# Remove supernatant
pipette.transfer(150, sample_plate.wells(), waste.wells(), new_tip="always")
```

### Step 8: Elute

```python
# Move off magnets
protocol.move_labware(sample_plate, "C2", use_gripper=True)

# Add elution buffer
pipette.transfer(50, elution_buffer, sample_plate.wells(), mix_after=(5, 30))

# Incubate
protocol.delay(minutes=2)

# Final magnetic separation
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)

# Transfer eluate (purified product)
pipette.transfer(45, sample_plate.wells(), elution_plate.wells(), new_tip="always")
```

## Complete DNA Extraction Protocol

```python
# Complete automated DNA extraction workflow
def run(protocol: protocol_api.ProtocolContext):
    # Modules and labware
    mag_block = protocol.load_module("magneticBlockV1", "D1")
    sample_plate = mag_block.load_labware("biorad_96_wellplate_200ul_pcr")

    # Additional labware
    tips = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "A1")
    waste = protocol.load_trash_bin("A3")
    reagents = protocol.load_labware("nest_12_reservoir_15ml", "C2")

    # Pipette
    pipette = protocol.load_instrument("flex_8channel_1000", "left", tip_racks=[tips])

    # Reagent locations
    lysis_beads = reagents["A1"]
    wash_buffer = reagents["A2"]
    elution_buffer = reagents["A3"]

    # Samples already in plate (loaded manually or via earlier steps)

    # 1. Add lysis beads and bind DNA
    pipette.transfer(100, lysis_beads, sample_plate.columns()[0], mix_after=(5, 80))
    protocol.delay(minutes=5)

    # 2. Magnetic separation - remove lysate
    protocol.move_labware(sample_plate, mag_block, use_gripper=True)
    protocol.delay(minutes=3)
    pipette.transfer(200, sample_plate.columns()[0], waste, new_tip="always")

    # 3. First wash
    protocol.move_labware(sample_plate, "C1", use_gripper=True)
    pipette.transfer(150, wash_buffer, sample_plate.columns()[0], mix_after=(5, 100))
    protocol.move_labware(sample_plate, mag_block, use_gripper=True)
    protocol.delay(minutes=2)
    pipette.transfer(150, sample_plate.columns()[0], waste, new_tip="always")

    # 4. Second wash
    protocol.move_labware(sample_plate, "C1", use_gripper=True)
    pipette.transfer(150, wash_buffer, sample_plate.columns()[0], mix_after=(5, 100))
    protocol.move_labware(sample_plate, mag_block, use_gripper=True)
    protocol.delay(minutes=2)
    pipette.transfer(150, sample_plate.columns()[0], waste, new_tip="always")

    # 5. Dry beads
    protocol.delay(minutes=5)

    # 6. Elute DNA
    protocol.move_labware(sample_plate, "C1", use_gripper=True)
    pipette.transfer(50, elution_buffer, sample_plate.columns()[0], mix_after=(5, 30))
    protocol.delay(minutes=2)

    # 7. Final magnetic separation
    protocol.move_labware(sample_plate, mag_block, use_gripper=True)
    protocol.delay(minutes=2)

    # 8. Collect purified DNA
    elution_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C3")
    pipette.transfer(45, sample_plate.columns()[0], elution_plate.columns()[0], new_tip="always")
```

## Common Patterns

### PCR Cleanup (AMPure Beads)

```python
# Clean PCR product with AMPure beads
mag_block = protocol.load_module("magneticBlockV1", "D1")
pcr_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C1")

# 1. Add beads (0.8x ratio for >300bp fragments)
pipette.transfer(40, ampure_beads, pcr_plate.wells(), mix_after=(5, 35))
protocol.delay(minutes=5)

# 2. Magnetic separation
protocol.move_labware(pcr_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)
pipette.transfer(90, pcr_plate.wells(), waste.wells())

# 3. Ethanol washes (2x)
for _ in range(2):
    pipette.transfer(150, ethanol_70, pcr_plate.wells())
    protocol.delay(seconds=30)
    pipette.transfer(150, pcr_plate.wells(), waste.wells())

# 4. Dry
protocol.delay(minutes=5)

# 5. Elute
protocol.move_labware(pcr_plate, "C2", use_gripper=True)
pipette.transfer(30, water, pcr_plate.wells(), mix_after=(5, 20))
protocol.delay(minutes=2)

# 6. Final separation and collection
protocol.move_labware(pcr_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)
pipette.transfer(25, pcr_plate.wells(), clean_plate.wells())
```

### NGS Library Preparation

```python
# Magnetic bead-based library prep
mag_block = protocol.load_module("magneticBlockV1", "D2")
lib_plate = mag_block.load_labware("biorad_96_wellplate_200ul_pcr")

# After adapter ligation...

# 1. Add SPRI beads for size selection (0.6x for >400bp)
pipette.transfer(30, spri_beads, lib_plate.wells(), mix_after=(5, 25))
protocol.delay(minutes=5)

# 2. Bind large fragments
protocol.move_labware(lib_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)

# 3. SAVE supernatant (contains fragments to remove)
protocol.move_labware(lib_plate, "C1", use_gripper=True)

# 4. Add more beads to supernatant (bring to 1.0x total)
pipette.transfer(20, spri_beads, lib_plate.wells(), mix_after=(5, 30))
protocol.delay(minutes=5)

# 5. Bind desired fragments
protocol.move_labware(lib_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)

# 6. Discard supernatant
pipette.transfer(100, lib_plate.wells(), waste.wells())

# 7. Wash and elute (standard protocol)
```

### Protein Immunoprecipitation

```python
# Magnetic bead IP
mag_block = protocol.load_module("magneticBlockV1", "D1")
ip_plate = mag_block.load_labware("biorad_96_wellplate_200ul_pcr")

# 1. Pre-coupled antibody-bead complex in wells

# 2. Add lysate
pipette.transfer(100, lysate_plate.wells(), ip_plate.wells(), mix_after=(3, 80))

# 3. Incubate (binding)
protocol.delay(minutes=30)

# 4. Wash unbound protein
for wash_num in range(3):
    protocol.move_labware(ip_plate, mag_block, use_gripper=True)
    protocol.delay(minutes=2)
    pipette.transfer(150, ip_plate.wells(), waste.wells())

    protocol.move_labware(ip_plate, "C1", use_gripper=True)
    pipette.transfer(150, wash_buffer, ip_plate.wells(), mix_after=(3, 100))

# 5. Elute bound protein
protocol.move_labware(ip_plate, "C1", use_gripper=True)
pipette.transfer(50, elution_buffer, ip_plate.wells(), mix_after=(5, 30))
protocol.delay(minutes=5)

# 6. Collect eluate
protocol.move_labware(ip_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)
pipette.transfer(45, ip_plate.wells(), analysis_plate.wells())
```

## Integration with Other Modules

### With Heater-Shaker (Heated Lysis)

```python
# Combine heating with magnetic purification
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
mag_block = protocol.load_module("magneticBlockV1", "D2")
sample_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C1")

# 1. Heat lysis on heater-shaker
hs_mod.open_labware_latch()
protocol.move_labware(sample_plate, hs_mod, use_gripper=True)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_temperature(56)
hs_mod.set_and_wait_for_shake_speed(1000)
protocol.delay(minutes=15)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()

# 2. Magnetic bead binding
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)

# ... continue with washes and elution ...
```

### With Temperature Module (Cold Elution)

```python
# Cold elution for stability
temp_mod = protocol.load_module("temperature module gen2", "D3")
mag_block = protocol.load_module("magneticBlockV1", "D2")

# Pre-cool elution buffer
temp_mod.set_temperature(4)

# After magnetic washes...
protocol.move_labware(sample_plate, "C1", use_gripper=True)

# Add cold elution buffer
pipette.transfer(50, cold_elution, sample_plate.wells(), mix_after=(5, 30))

# Move to cold module for elution
protocol.move_labware(sample_plate, temp_mod, use_gripper=True)
protocol.delay(minutes=5)

# Final separation
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)
pipette.transfer(45, sample_plate.wells(), storage_plate.wells())

temp_mod.deactivate()
```

### With Thermocycler (Post-PCR Cleanup)

```python
# PCR → Cleanup workflow
tc_mod = protocol.load_module("thermocyclerModuleV2")
mag_block = protocol.load_module("magneticBlockV1", "D2")

# Run PCR
tc_mod.close_lid()
# ... PCR cycling ...
tc_mod.set_block_temperature(4)
tc_mod.open_lid()

# Move to deck for bead addition
pcr_plate = tc_mod.labware
protocol.move_labware(pcr_plate, "C1", use_gripper=True)

# Add AMPure beads
pipette.transfer(40, ampure_beads, pcr_plate.wells(), mix_after=(5, 35))
protocol.delay(minutes=5)

# Cleanup on magnetic block
protocol.move_labware(pcr_plate, mag_block, use_gripper=True)
# ... cleanup steps ...
```

## Best Practices

1. **Allow sufficient separation time** - Typically 2-5 minutes depending on bead type
2. **Use gripper for all plate movements** - Manual movement defeats automation purpose
3. **Avoid disturbing beads when pipetting** - Aspirate from opposite side of well
4. **Mix thoroughly after moving off magnets** - Ensure complete bead resuspension
5. **Track bead volume in calculations** - Account for beads when calculating total volume
6. **Use consistent bead types** - Different beads have different separation times
7. **Plan deck layout** - Position magnetic block near other modules for efficiency
8. **Test separation time** - Optimize delay for your specific beads and sample type
9. **Don't over-dry beads** - Can reduce elution efficiency
10. **Validate elution volume** - Leave some headroom to avoid transferring beads

## Separation Time Guidelines

**Typical separation times by bead type:**
- **AMPure/SPRIselect (DNA):** 2-3 minutes
- **RNAClean (RNA):** 3-4 minutes
- **Protein G/A beads:** 2-3 minutes
- **High-density beads:** 1-2 minutes
- **Large volume samples:** 4-5 minutes

**Factors affecting separation:**
- Bead concentration
- Sample volume
- Viscosity
- Magnetic bead strength
- Well geometry

**Recommendation:** Start with 3 minutes, adjust based on visual inspection or protocol optimization.

## Common Mistakes

**❌ Moving plate without gripper:**
```python
# Manual movement not practical in automated protocol
protocol.move_labware(sample_plate, mag_block, use_gripper=False)
# User must manually move plate - defeats automation
```

**✅ Correct:**
```python
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
```

**❌ Insufficient separation time:**
```python
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(seconds=30)  # Too short - beads still in solution
pipette.transfer(100, sample_plate.wells(), waste.wells())  # Transfers beads!
```

**✅ Correct:**
```python
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)  # Adequate time for separation
pipette.transfer(100, sample_plate.wells(), waste.wells())
```

**❌ Pipetting through bead pellet:**
```python
# Aspirating from center may disturb beads on side
pipette.aspirate(100, sample_plate.wells()[0])
```

**✅ Correct:**
```python
# Aspirate from side opposite beads
pipette.aspirate(100, sample_plate.wells()[0].bottom(z=2))
# Or use touch_tip to ensure beads aren't transferred
```

**❌ Incomplete bead resuspension:**
```python
protocol.move_labware(sample_plate, "C1", use_gripper=True)
pipette.transfer(150, wash_buffer, sample_plate.wells())  # Beads may not resuspend
```

**✅ Correct:**
```python
protocol.move_labware(sample_plate, "C1", use_gripper=True)
pipette.transfer(150, wash_buffer, sample_plate.wells(), mix_after=(5, 100))  # Mix to resuspend
```

**❌ Using incompatible labware:**
```python
# Deep well plates may not align with magnets properly
mag_plate = mag_block.load_labware("nest_96_wellplate_2ml_deep")
```

**✅ Correct:**
```python
# Use standard 96-well PCR plates
mag_plate = mag_block.load_labware("biorad_96_wellplate_200ul_pcr")
```

## Troubleshooting

**Beads not separating:**
- Increase separation time (try 4-5 minutes)
- Check bead type and concentration
- Verify plate is properly seated on magnetic block
- Ensure labware is compatible with magnet positions

**Beads being transferred with supernatant:**
- Extend separation time
- Pipette more carefully (avoid bead pellet)
- Reduce aspiration flow rate
- Leave more residual volume

**Incomplete elution:**
- Ensure beads are fully resuspended before elution incubation
- Mix more vigorously
- Extend elution incubation time
- Verify beads haven't dried excessively
- Use appropriate elution buffer volume

**Bead clumping:**
- Mix more thoroughly when resuspending
- Ensure beads are well-mixed before adding to samples
- Avoid drying beads too long
- Check bead storage conditions

**Variable recovery across wells:**
- Ensure consistent bead addition volumes
- Mix uniformly across all wells
- Use consistent separation times
- Check for plate positioning issues on magnetic block

## Advanced Techniques

### Dual-Size Selection

```python
# Select DNA fragments within specific size range
# First selection: Remove large fragments
pipette.transfer(30, spri_beads, lib_plate.wells(), mix_after=(5, 25))  # 0.6x
protocol.delay(minutes=5)
protocol.move_labware(lib_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)

# Collect supernatant (has small + target fragments)
protocol.move_labware(lib_plate, "C1", use_gripper=True)
pipette.transfer(80, lib_plate.wells(), temp_plate.wells())

# Second selection: Bind target fragments
pipette.transfer(20, spri_beads, temp_plate.wells(), mix_after=(5, 30))  # Brings to 1.0x
protocol.delay(minutes=5)
protocol.move_labware(temp_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)

# Discard supernatant (small fragments removed)
pipette.transfer(100, temp_plate.wells(), waste.wells())

# Wash and elute target size range
```

### Differential Elution

```python
# Elute different targets sequentially
# After binding multiple targets to beads...

# First elution (low stringency)
pipette.transfer(50, elution_buffer_1, sample_plate.wells(), mix_after=(5, 30))
protocol.delay(minutes=2)
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)
pipette.transfer(45, sample_plate.wells(), fraction_1_plate.wells())

# Second elution (high stringency)
protocol.move_labware(sample_plate, "C1", use_gripper=True)
pipette.transfer(50, elution_buffer_2, sample_plate.wells(), mix_after=(5, 30))
protocol.delay(minutes=2)
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=2)
pipette.transfer(45, sample_plate.wells(), fraction_2_plate.wells())
```

## No Electronic Control

**Remember:** The Magnetic Block has no electronic interface.

**This means:**
- ❌ Cannot turn magnets on/off programmatically
- ❌ No status reporting to robot
- ❌ No automatic timing
- ✅ Full control through plate positioning
- ✅ Predictable, passive magnetic field
- ✅ No calibration or initialization required

**Control strategy:** Move plate TO block = magnets engaged, move plate OFF block = magnets disengaged

## API Version Requirements

- **Minimum API version:** 2.15 (Magnetic Block introduced)
- **Recommended:** 2.19+ for full Flex feature support
- **Robot type:** Must be Opentrons Flex

## Additional Resources

- **Magnetic Block Documentation:** https://docs.opentrons.com/v2/modules/magnetic_block.html
- **Gripper Documentation:** https://docs.opentrons.com/v2/new_protocol_api.html#opentrons.protocol_api.ProtocolContext.move_labware
- **Protocol Library:** https://protocols.opentrons.com/
- **Opentrons Support:** https://support.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-gripper` - Automated labware movement (required)
- `opentrons-heater-shaker` - Heated incubation with mixing
- `opentrons-temperature-module` - Temperature control
- `opentrons-thermocycler` - PCR for molecular workflows

---

### opentrons-temperature-module

**Path**: `skills/laboratory/opentrons-temperature-module/SKILL.md`

# Opentrons Temperature Module

## Overview

The **Opentrons Temperature Module** provides precise temperature control (4-95°C) for maintaining samples at specific temperatures during automated protocols. Ideal for keeping reagents cold, pre-warming reaction components, temperature-sensitive incubations, and any workflow requiring stable thermal conditions without mixing.

**Core value:** Eliminate ice buckets and water baths. Maintain precise, reproducible temperatures on-deck for reagents, samples, and reaction components throughout your protocol.

## When to Use

Use the Temperature Module skill when:
- Keeping reagents cold during protocol (e.g., enzymes at 4°C)
- Pre-warming reaction components (e.g., buffers at 37°C)
- Maintaining samples at specific temperatures
- Temperature-sensitive enzyme reactions without mixing
- Cooling samples after thermal processing
- Any protocol requiring stable temperature control (4-95°C)

**Don't use when:**
- Need mixing/shaking during temperature control (use Heater-Shaker Module)
- Require PCR thermal cycling (use Thermocycler Module)
- Temperature outside 4-95°C range
- Precise temperature ramping or cycling needed

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Load module | `protocol.load_module()` | `"temperature module gen2"`, location |
| Set temperature | `set_temperature()` | celsius (4-95) |
| Get current temperature | `.temperature` | Read-only property |
| Check status | `.status` | Returns "holding at target" or "idle" |
| Deactivate | `deactivate()` | - |

## Platform Compatibility

**Both Opentrons Flex and OT-2**

### Module Generations
- **GEN1** - Original temperature module
- **GEN2** - Improved cooling performance, better isolation (recommended)

**Key GEN2 improvements:**
- Plastic insulation around plate
- Shrouds for aluminum blocks
- Better cooling when sharing deck with running Thermocycler
- Same API as GEN1

## Loading the Module

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.19'}

def run(protocol: protocol_api.ProtocolContext):
    # Load Temperature Module GEN2
    temp_mod = protocol.load_module("temperature module gen2", "D1")  # Flex
    # temp_mod = protocol.load_module("temperature module gen2", "4")  # OT-2

    # Load labware on module
    cold_plate = temp_mod.load_labware("opentrons_96_aluminumblock_generic_pcr_strip_200ul")
```

**Module versions:**
- `"temperature module gen2"` - GEN2 (recommended)
- `"temperatureModuleV1"` - GEN1 (legacy)

**Deck slot:** Any compatible slot (Flex: A1-D3, OT-2: 1-11)

## Loading Labware

### Standalone Adapters (API 2.15+)

**Recommended approach** - Load adapter first, then labware:

```python
# 96-well aluminum block
temp_block = temp_mod.load_adapter("opentrons_96_well_aluminum_block")
sample_plate = temp_block.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")
```

**Available adapters:**

| Adapter Name | Compatible Labware |
|--------------|-------------------|
| `opentrons_96_well_aluminum_block` | 96-well plates, PCR plates |
| `opentrons_aluminum_flat_bottom_plate` | Flat-bottom plates |
| `opentrons_96_deep_well_temp_mod_adapter` | Deep-well plates |

### Block-and-Tube Combinations

For tube racks with aluminum blocks:

```python
# 24-well block with various tube types
tubes_nest = temp_mod.load_labware("opentrons_24_aluminumblock_nest_1.5ml_snapcap")
tubes_generic = temp_mod.load_labware("opentrons_24_aluminumblock_generic_2ml_screwcap")
```

### Legacy Block-and-Plate Combinations

```python
# Pre-configured combinations (older API versions)
plate_combo = temp_mod.load_labware("opentrons_96_aluminumblock_biorad_wellplate_200ul")
```

**Recommendation:** Use standalone adapters (API 2.15+) for better flexibility.

## Temperature Control

### Setting Temperature

```python
# Set temperature and wait until reached
temp_mod.set_temperature(celsius=4)

# Protocol waits here until 4°C is reached

# Perform operations at target temperature
pipette.transfer(100, cold_reagent, dest_plate.wells())
```

**Temperature range:** 4-95°C (1°C resolution)

**Behavior:** Protocol execution **blocks** (waits) until target temperature is reached.

### Temperature Status

```python
# Get current temperature
current_temp = temp_mod.temperature
protocol.comment(f"Module at {current_temp}°C")

# Check status
status = temp_mod.status
# Returns: "holding at target" or "idle"

if status == "holding at target":
    protocol.comment("Temperature stable, proceeding")
```

### Deactivating

```python
# Turn off temperature control
temp_mod.deactivate()
```

**Important:** Module does **not** automatically deactivate at protocol end. Must be manually turned off via Opentrons App if protocol completes or is cancelled.

## Common Patterns

### Keeping Reagents Cold

```python
# Keep enzymes and master mix cold throughout protocol
temp_mod = protocol.load_module("temperature module gen2", "D1")
reagent_block = temp_mod.load_adapter("opentrons_24_aluminumblock_nest_1.5ml_snapcap")

# Set to 4°C before starting
temp_mod.set_temperature(4)

# Reagents stay cold during entire protocol
pipette.transfer(10, reagent_block["A1"], dest_plate.wells())
# ... rest of protocol ...

# Turn off at end
temp_mod.deactivate()
```

### Pre-Warming Reaction Components

```python
# Pre-warm buffer to 37°C
temp_mod = protocol.load_module("temperature module gen2", "C2")
warm_block = temp_mod.load_adapter("opentrons_96_well_aluminum_block")
warm_plate = warm_block.load_labware("corning_96_wellplate_360ul_flat")

# Set to reaction temperature
temp_mod.set_temperature(37)

# Use pre-warmed components
pipette.transfer(50, warm_plate["A1"], reaction_plate.wells())

temp_mod.deactivate()
```

### Cooling After Thermal Processing

```python
# Cool samples after PCR or heat inactivation
temp_mod = protocol.load_module("temperature module gen2", "D3")
cooling_block = temp_mod.load_adapter("opentrons_96_well_aluminum_block")

# Set to 4°C for cooling
temp_mod.set_temperature(4)

# Move hot samples to cooling block with gripper (Flex)
protocol.move_labware(hot_plate, temp_mod, use_gripper=True)

# Hold at 4°C
protocol.delay(minutes=5)

# Continue processing cooled samples
temp_mod.deactivate()
```

### Temperature-Sensitive Enzyme Reaction

```python
# Restriction digest at optimal temperature
temp_mod = protocol.load_module("temperature module gen2", "D1")
rxn_block = temp_mod.load_adapter("opentrons_96_well_aluminum_block")
rxn_plate = rxn_block.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")

# Setup reaction
pipette.transfer(20, dna_samples, rxn_plate.wells()[:8])
pipette.transfer(5, enzyme_mix, rxn_plate.wells()[:8], mix_after=(3, 15))

# Incubate at optimal temperature
temp_mod.set_temperature(37)
protocol.delay(hours=2)

# Heat inactivation
temp_mod.set_temperature(65)
protocol.delay(minutes=20)

# Cool for downstream processing
temp_mod.set_temperature(4)

temp_mod.deactivate()
```

### Maintaining Temperature During Multi-Step Protocol

```python
# Keep samples at 4°C except during specific steps
cold_storage = protocol.load_module("temperature module gen2", "D1")
cold_block = cold_storage.load_adapter("opentrons_96_well_aluminum_block")
sample_plate = cold_block.load_labware("biorad_96_wellplate_200ul_pcr")

# Set to 4°C at start
cold_storage.set_temperature(4)

# Samples stay cold on module
# Move to room temp for specific operations
protocol.move_labware(sample_plate, "C2", use_gripper=True)

# ... perform room-temp operations ...

# Return to cold storage
protocol.move_labware(sample_plate, cold_storage, use_gripper=True)

cold_storage.deactivate()
```

## Integration with Other Modules

### With Thermocycler

```python
# Pre-cool samples before PCR
temp_mod = protocol.load_module("temperature module gen2", "D1")
tc_mod = protocol.load_module("thermocyclerModuleV2")

cold_block = temp_mod.load_adapter("opentrons_96_well_aluminum_block")
sample_plate = cold_block.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")

# Keep samples cold
temp_mod.set_temperature(4)

# Setup PCR mix (samples stay cold)
# ... pipetting ...

# Transfer to thermocycler
tc_mod.open_lid()
protocol.move_labware(sample_plate, tc_mod, use_gripper=True)

# Run PCR
tc_mod.close_lid()
# ... thermal cycling ...

# Return to cold storage after PCR
tc_mod.open_lid()
protocol.move_labware(sample_plate, temp_mod, use_gripper=True)

temp_mod.deactivate()
```

### With Heater-Shaker

```python
# Cool after shaking/heating
hs_mod = protocol.load_module("heaterShakerModuleV1", "D1")
temp_mod = protocol.load_module("temperature module gen2", "D2")

# Pre-cool temperature module
temp_mod.set_temperature(4)

# Heat/shake on heater-shaker
hs_mod.set_and_wait_for_temperature(65)
hs_mod.close_labware_latch()
hs_mod.set_and_wait_for_shake_speed(1000)
protocol.delay(minutes=10)
hs_mod.deactivate_shaker()
hs_mod.deactivate_heater()
hs_mod.open_labware_latch()

# Transfer to temperature module for cooling
protocol.move_labware(reaction_plate, temp_mod, use_gripper=True)
protocol.delay(minutes=3)

temp_mod.deactivate()
```

### With Magnetic Block

```python
# Temperature-controlled magnetic separation
mag_block = protocol.load_module("magneticBlockV1", "D1")
temp_mod = protocol.load_module("temperature module gen2", "D2")

# Cool elution buffer
temp_mod.set_temperature(4)

# Magnetic separation
protocol.move_labware(sample_plate, mag_block, use_gripper=True)
protocol.delay(minutes=3)
pipette.transfer(150, sample_plate.wells(), waste.wells())

# Elute with cold buffer
protocol.move_labware(sample_plate, temp_mod, use_gripper=True)
pipette.transfer(50, cold_buffer, sample_plate.wells())

temp_mod.deactivate()
```

## Best Practices

1. **Set temperature before loading samples** - Allow module to stabilize
2. **Deactivate at protocol end** - Prevent equipment running indefinitely
3. **Use GEN2 for better performance** - Improved cooling and isolation
4. **Plan for thermal equilibration** - Large temperature changes take time
5. **Monitor ambient temperature** - Affects minimum achievable temperature (4°C target requires cool room)
6. **Use aluminum blocks** - Better thermal contact than direct plate placement
7. **Avoid pipetting during temperature changes** - Wait for "holding at target" status
8. **Consider thermal mass** - More liquid = slower temperature equilibration
9. **Don't rely on auto-deactivation** - Module stays on after protocol ends
10. **Check compatibility** - Verify labware fits adapter/block

## Common Mistakes

**❌ Pipetting during temperature change:**
```python
temp_mod.set_temperature(4)
# set_temperature() blocks until reached, but safer to check status
pipette.transfer(...)  # Risk if not fully stabilized
```

**✅ Correct:**
```python
temp_mod.set_temperature(4)
# Wait for status confirmation
if temp_mod.status == "holding at target":
    pipette.transfer(...)
```

**❌ Not deactivating module:**
```python
temp_mod.set_temperature(4)
# ... protocol ends ...
# Module stays at 4°C indefinitely!
```

**✅ Correct:**
```python
temp_mod.set_temperature(4)
# ... operations ...
temp_mod.deactivate()
```

**❌ Temperature out of range:**
```python
temp_mod.set_temperature(100)  # Error: max is 95°C
temp_mod.set_temperature(0)    # Error: min is 4°C
```

**✅ Correct:**
```python
temp_mod.set_temperature(95)  # Within range
temp_mod.set_temperature(4)   # Within range
```

**❌ Loading labware before temperature stabilizes:**
```python
temp_mod.set_temperature(4)
# Temperature not yet reached - samples warm during cooling
pipette.transfer(100, warm_samples, temp_block.wells())
```

**✅ Correct:**
```python
# Set temperature BEFORE loading samples
temp_mod.set_temperature(4)
# Now at 4°C, samples stay cold
pipette.transfer(100, warm_samples, temp_block.wells())
```

## Troubleshooting

**Module not reaching 4°C:**
- Check ambient room temperature (module can't cool below ambient by much)
- Ensure adequate airflow around module
- Verify module is on flat, level surface
- Consider using ice packs as supplement for very cold requirements

**Module not reaching high temperatures:**
- Verify temperature is ≤95°C
- Check module is not in cold environment
- Allow sufficient time for large thermal mass

**Temperature not stable:**
- Wait for status to show "holding at target"
- Avoid high-airflow environments
- Ensure good thermal contact between labware and block
- Use appropriate adapter for labware type

**Slow temperature changes:**
- Normal for large temperature differences
- Use smaller labware/tubes for faster equilibration
- Pre-cool/pre-warm module before critical steps
- Consider thermal mass of samples

**Module stays on after protocol:**
- This is expected behavior
- Manually deactivate via Opentrons App
- Or add `deactivate()` to protocol end

## Temperature Limits

| Parameter | Minimum | Maximum | Resolution |
|-----------|---------|---------|------------|
| Block temperature | 4°C | 95°C | 1°C |
| Cooling capacity | ~4°C below ambient | - | - |
| Heating capacity | - | 95°C | - |

**Note:** Actual minimum temperature depends on ambient conditions. In warm room (>25°C), reaching 4°C may be difficult.

## Advanced Techniques

### Temperature Gradient Protocol

```python
# Ramp temperature for optimization
temp_mod = protocol.load_module("temperature module gen2", "D1")

for temp in [4, 20, 37, 55, 70]:
    temp_mod.set_temperature(temp)
    protocol.comment(f"Now at {temp}°C")

    # Perform operations at each temperature
    # ... sampling, measurements, etc. ...

    protocol.delay(minutes=5)

temp_mod.deactivate()
```

### Dual Temperature Setup

```python
# Maintain two different temperatures simultaneously
cold_mod = protocol.load_module("temperature module gen2", "D1")
warm_mod = protocol.load_module("temperature module gen2", "D2")

cold_mod.set_temperature(4)
warm_mod.set_temperature(37)

# Use both temperature zones in protocol
pipette.transfer(10, cold_mod.labware["A1"], reaction_plate.wells())
pipette.transfer(50, warm_mod.labware["A1"], reaction_plate.wells())

cold_mod.deactivate()
warm_mod.deactivate()
```

### Temperature Shock Protocol

```python
# Quick temperature changes for cell work
temp_mod = protocol.load_module("temperature module gen2", "D1")
cell_plate = temp_mod.load_labware("corning_96_wellplate_360ul_flat")

# Ice incubation
temp_mod.set_temperature(4)
protocol.delay(minutes=20)

# Heat shock
temp_mod.set_temperature(42)
protocol.delay(seconds=45)

# Recovery
temp_mod.set_temperature(37)
protocol.delay(minutes=5)

temp_mod.deactivate()
```

## GEN2 vs GEN1

| Feature | GEN1 | GEN2 |
|---------|------|------|
| Temperature range | 4-95°C | 4-95°C |
| Cooling | Basic | Improved with insulation |
| Thermocycler compatibility | Poor cooling | Good cooling |
| Plate insulation | None | Plastic shroud |
| Block shrouds | No | Yes |
| API | Same | Same |
| Recommended | No | Yes |

**Recommendation:** Use GEN2 for all new protocols.

## API Version Requirements

- **Minimum API version:** 2.0 (temperature module support)
- **Standalone adapters:** API 2.15+
- **Recommended:** 2.19+ for full feature support

## Additional Resources

- **Temperature Module Documentation:** https://docs.opentrons.com/v2/modules/temperature_module.html
- **Labware Library:** https://labware.opentrons.com/
- **Opentrons Support:** https://support.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-heater-shaker` - Temperature control with mixing (37-95°C)
- `opentrons-thermocycler` - PCR thermal cycling (4-99°C block)
- `opentrons-gripper` - Automated labware movement (Flex)
- `opentrons-magnetic-block` - Magnetic bead separation

---

### opentrons-thermocycler

**Path**: `skills/laboratory/opentrons-thermocycler/SKILL.md`

# Opentrons Thermocycler Module

## Overview

The **Opentrons Thermocycler Module** automates PCR thermal cycling with precise, independent control of block temperature (4-99°C) and heated lid (37-110°C). Execute complex temperature profiles with automatic cycling, integrate with liquid handling for complete PCR setup automation, and use auto-sealing lids (GEN2) for walk-away workflows.

**Core value:** Fully automate PCR setup and cycling. Load samples, dispense reagents, seal plates (GEN2), run thermal profiles, and collect products—all without manual intervention.

## When to Use

Use the Thermocycler skill when:
- Running PCR amplification protocols
- Performing qPCR or RT-PCR reactions
- Automating DNA/RNA thermal cycling workflows
- Executing multi-step temperature incubations
- Integrating thermal cycling with automated liquid handling
- Setting up high-throughput PCR screening

**Don't use when:**
- Simple temperature control needed (use Temperature Module for 4-95°C range)
- Shaking/mixing required during incubation (use Heater-Shaker Module)
- Temperature outside 4-99°C range (block) or 37-110°C (lid)

## Quick Reference

| Operation | Method | Key Parameters |
|-----------|--------|----------------|
| Load module | `protocol.load_module()` | `"thermocyclerModuleV2"` or `"thermocyclerModuleV1"` |
| Open lid | `open_lid()` | - |
| Close lid | `close_lid()` | - |
| Set lid temperature | `set_lid_temperature()` | celsius (37-110) |
| Deactivate lid | `deactivate_lid()` | - |
| Set block temperature | `set_block_temperature()` | celsius (4-99), hold_time, block_max_volume |
| Execute profile | `execute_profile()` | steps, repetitions, block_max_volume |
| Deactivate block | `deactivate_block()` | - |

## Platform Compatibility

**Both Opentrons Flex and OT-2**

### Module Generations
- **GEN1** - Original thermocycler, compatible with both platforms
- **GEN2** - Improved plate lift mechanism, auto-sealing lid support

**API compatibility:** Both generations support identical API methods

### Deck Position

**OT-2:** Spans multiple deck slots (typically slots 7, 8, 10, 11)

**Flex:** Dedicated thermocycler position

**Loading:**
```python
# No deck slot specified - thermocycler has fixed position
tc_mod = protocol.load_module("thermocyclerModuleV2")
```

## Loading the Module

```python
from opentrons import protocol_api

metadata = {'apiLevel': '2.19'}

def run(protocol: protocol_api.ProtocolContext):
    # Load Thermocycler Module (no location - fixed position)
    tc_mod = protocol.load_module("thermocyclerModuleV2")

    # Load PCR plate
    pcr_plate = tc_mod.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")
```

**Module versions:**
- `"thermocyclerModuleV1"` - GEN1
- `"thermocyclerModuleV2"` - GEN2 (recommended)

## Lid Control

### Basic Lid Operations

```python
# Open lid for pipette access
tc_mod.open_lid()

# Close lid for thermal cycling
tc_mod.close_lid()
```

**Important for gripper (Flex):**
- **Always open lid before gripper operations**
- Close lid only after plate is seated on thermocycler

### Heated Lid

The heated lid prevents condensation during thermal cycling:

```python
# Set lid temperature (blocks until reached)
tc_mod.set_lid_temperature(celsius=105)

# Run thermal profile
# ... cycling ...

# Turn off lid heater
tc_mod.deactivate_lid()
```

**Lid temperature range:** 37-110°C

**Typical settings:**
- **Standard PCR:** 105°C
- **RT-PCR:** 100-105°C
- **Custom protocols:** Match or exceed highest block temperature + 5-10°C

**Note:** Lid temperature is independent of block temperature and profiles.

## Block Temperature Control

### Basic Block Temperature

```python
# Set block temperature and wait
tc_mod.set_block_temperature(
    temperature=95,
    hold_time_seconds=180,  # Hold for 3 minutes
    block_max_volume=50     # Volume in wells (µL)
)

# Block automatically maintains temperature for hold_time
```

**Block temperature range:** 4-99°C

**Parameters:**
- `temperature` - Target temperature in °C (required)
- `hold_time_seconds` - Duration to hold at temperature
- `hold_time_minutes` - Alternative duration in minutes
- `block_max_volume` - Sample volume in µL (default: 25µL)

### Hold Time Behavior

**With hold_time:** Protocol waits at temperature for specified duration

```python
# Wait 5 minutes at 95°C
tc_mod.set_block_temperature(
    temperature=95,
    hold_time_minutes=5,
    block_max_volume=50
)
# Protocol continues after 5 minutes
```

**Without hold_time:** Set temperature and continue immediately

```python
# Set to 4°C and proceed (no hold)
tc_mod.set_block_temperature(temperature=4)

# Can perform other operations while maintaining 4°C
```

### Block Max Volume

Specify sample volume for improved temperature accuracy:

```python
# 50µL reactions
tc_mod.set_block_temperature(
    temperature=72,
    hold_time_minutes=10,
    block_max_volume=50  # Optimizes heating/cooling for 50µL
)
```

**Default:** 25µL if not specified

**Why it matters:** Algorithm adjusts for thermal mass of liquid to ensure accurate sample temperature.

## Temperature Profiles

Automate repeated temperature cycles for PCR:

### Defining a Profile

```python
# PCR profile: Denature → Anneal → Extend
pcr_profile = [
    {"temperature": 95, "hold_time_seconds": 30},  # Denaturation
    {"temperature": 57, "hold_time_seconds": 30},  # Annealing
    {"temperature": 72, "hold_time_seconds": 60}   # Extension
]
```

**Profile structure:** List of dictionaries with `temperature` and `hold_time_seconds`

### Executing a Profile

```python
# Run profile for 30 cycles
tc_mod.execute_profile(
    steps=pcr_profile,
    repetitions=30,
    block_max_volume=50
)
```

**Parameters:**
- `steps` - List of temperature steps (required)
- `repetitions` - Number of times to repeat profile (required)
- `block_max_volume` - Sample volume in µL

### Complete PCR Protocol

```python
# Setup
tc_mod = protocol.load_module("thermocyclerModuleV2")
pcr_plate = tc_mod.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")

# Open lid for loading
tc_mod.open_lid()

# Pipette PCR reagents (not shown)
# ... add template, primers, polymerase, etc. ...

# Close lid and set lid temperature
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)

# Initial denaturation
tc_mod.set_block_temperature(
    temperature=95,
    hold_time_seconds=180,  # 3 minutes
    block_max_volume=50
)

# PCR cycling
pcr_profile = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 57, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 60}
]

tc_mod.execute_profile(steps=pcr_profile, repetitions=30, block_max_volume=50)

# Final extension
tc_mod.set_block_temperature(
    temperature=72,
    hold_time_minutes=5,
    block_max_volume=50
)

# Hold at 4°C
tc_mod.set_block_temperature(temperature=4)

# Cleanup
tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

## Auto-Sealing Lids (GEN2)

**Opentrons Flex GEN2 Thermocycler** supports auto-sealing lids for walk-away protocols:

### Auto-Sealing Lid Labware

```python
# Load auto-sealing lid and riser
auto_seal_lid = protocol.load_labware(
    "opentrons_tough_pcr_auto_sealing_lid",
    location="lid_stack_location"
)

riser = protocol.load_labware(
    "opentrons_flex_deck_riser",
    location="deck_location"
)
```

### Lid Stack Management

```python
# Stack up to 5 auto-sealing lids
lid_stack = tc_mod.load_lid_stack(
    "opentrons_tough_pcr_auto_sealing_lid",
    quantity=5
)

# Move lid with gripper
protocol.move_lid(
    lid=lid_stack,
    new_location=pcr_plate,
    use_gripper=True
)
```

**Important:** Do not affix rubber seal to internal Thermocycler lid when using auto-sealing lids.

### Complete Workflow with Auto-Sealing

```python
# Load module and labware
tc_mod = protocol.load_module("thermocyclerModuleV2")
pcr_plate = tc_mod.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")

# Load auto-sealing lids
lid_stack = tc_mod.load_lid_stack(
    "opentrons_tough_pcr_auto_sealing_lid",
    quantity=3
)

# Setup samples
tc_mod.open_lid()
# ... pipette reagents ...

# Apply auto-sealing lid with gripper
protocol.move_lid(lid_stack, pcr_plate, use_gripper=True)

# Close thermocycler lid
tc_mod.close_lid()

# Run PCR
# ... thermal cycling ...

# Open for retrieval
tc_mod.open_lid()
```

## Common Patterns

### Standard PCR

```python
# Standard Taq PCR protocol
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)

# Initial denaturation
tc_mod.set_block_temperature(95, hold_time_minutes=3, block_max_volume=50)

# Cycling
standard_pcr = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 55, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 60}
]
tc_mod.execute_profile(steps=standard_pcr, repetitions=35, block_max_volume=50)

# Final extension
tc_mod.set_block_temperature(72, hold_time_minutes=10, block_max_volume=50)

# Hold at 4°C
tc_mod.set_block_temperature(4)

tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

### Gradient PCR (Multiple Annealing Temperatures)

```python
# Test multiple annealing temperatures in columns
annealing_temps = [52, 54, 56, 58, 60, 62, 64, 66]  # 8 temperatures

for i, temp in enumerate(annealing_temps):
    protocol.comment(f"Column {i+1}: {temp}°C annealing")

    # Pipette samples into column (example)
    # pipette.transfer(50, master_mix, pcr_plate.columns()[i])

    # Run gradient PCR for this column
    gradient_profile = [
        {"temperature": 95, "hold_time_seconds": 30},
        {"temperature": temp, "hold_time_seconds": 30},  # Variable annealing
        {"temperature": 72, "hold_time_seconds": 60}
    ]

    tc_mod.execute_profile(steps=gradient_profile, repetitions=30, block_max_volume=50)
```

**Note:** This example is simplified. True gradient PCR requires hardware that supports simultaneous temperature gradients across the block.

### Two-Step PCR (No Extension)

```python
# Two-step PCR (denature + anneal/extend combined)
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)

tc_mod.set_block_temperature(95, hold_time_minutes=3, block_max_volume=25)

two_step = [
    {"temperature": 95, "hold_time_seconds": 15},
    {"temperature": 60, "hold_time_seconds": 60}  # Combined anneal/extend
]

tc_mod.execute_profile(steps=two_step, repetitions=40, block_max_volume=25)

tc_mod.set_block_temperature(4)
tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

### RT-PCR (Reverse Transcription + PCR)

```python
# Reverse transcription + PCR
tc_mod.close_lid()
tc_mod.set_lid_temperature(100)

# Reverse transcription
tc_mod.set_block_temperature(42, hold_time_minutes=30, block_max_volume=20)

# RT inactivation
tc_mod.set_block_temperature(85, hold_time_minutes=5, block_max_volume=20)

# Cool for PCR enzyme addition (if needed)
tc_mod.set_block_temperature(4)
tc_mod.open_lid()

# Add PCR enzymes
# pipette.transfer(...)

tc_mod.close_lid()
tc_mod.set_lid_temperature(105)

# PCR cycling
tc_mod.set_block_temperature(95, hold_time_minutes=2, block_max_volume=25)

rt_pcr_profile = [
    {"temperature": 95, "hold_time_seconds": 15},
    {"temperature": 60, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 30}
]

tc_mod.execute_profile(steps=rt_pcr_profile, repetitions=40, block_max_volume=25)

tc_mod.set_block_temperature(4)
tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

### Touchdown PCR

```python
# Touchdown PCR - decreasing annealing temperature
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)
tc_mod.set_block_temperature(95, hold_time_minutes=3, block_max_volume=50)

# High stringency cycles (65°C → 55°C, -1°C per cycle)
for temp in range(65, 54, -1):
    touchdown_profile = [
        {"temperature": 95, "hold_time_seconds": 30},
        {"temperature": temp, "hold_time_seconds": 30},
        {"temperature": 72, "hold_time_seconds": 60}
    ]
    tc_mod.execute_profile(steps=touchdown_profile, repetitions=1, block_max_volume=50)

# Lower stringency cycles (55°C for remaining)
standard_profile = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 55, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 60}
]
tc_mod.execute_profile(steps=standard_profile, repetitions=25, block_max_volume=50)

tc_mod.set_block_temperature(72, hold_time_minutes=10, block_max_volume=50)
tc_mod.set_block_temperature(4)
tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

## Integration with Liquid Handling

### Automated PCR Setup

```python
# Complete automated PCR setup
tc_mod = protocol.load_module("thermocyclerModuleV2")
pcr_plate = tc_mod.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt")
pipette = protocol.load_instrument("flex_1channel_1000", "left")

# Reagents
master_mix = protocol.load_labware("opentrons_24_tuberack_nest_1.5ml_snapcap", "C1")
template_plate = protocol.load_labware("biorad_96_wellplate_200ul_pcr", "C2")

# Open thermocycler
tc_mod.open_lid()

# Distribute master mix
pipette.distribute(
    volume=45,
    source=master_mix["A1"],
    dest=pcr_plate.wells(),
    new_tip="once"
)

# Add template DNA
pipette.transfer(
    volume=5,
    source=template_plate.wells(),
    dest=pcr_plate.wells(),
    mix_after=(3, 25),
    new_tip="always"
)

# Run PCR
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)
tc_mod.set_block_temperature(95, hold_time_minutes=3, block_max_volume=50)

pcr_profile = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 58, "hold_time_seconds": 30},
    {"temperature": 72, "hold_time_seconds": 60}
]

tc_mod.execute_profile(steps=pcr_profile, repetitions=30, block_max_volume=50)
tc_mod.set_block_temperature(72, hold_time_minutes=5, block_max_volume=50)
tc_mod.set_block_temperature(4)

tc_mod.deactivate_lid()
tc_mod.deactivate_block()
tc_mod.open_lid()
```

### Integration with Gripper (Flex)

```python
# Prepare samples off-deck, move to thermocycler
sample_plate = protocol.load_labware("opentrons_96_wellplate_200ul_pcr_full_skirt", "C2")

# Setup samples
# ... pipetting ...

# Move to thermocycler with gripper
tc_mod.open_lid()
protocol.move_labware(sample_plate, tc_mod, use_gripper=True)

# Run PCR
tc_mod.close_lid()
# ... thermal cycling ...

# Retrieve plate
tc_mod.open_lid()
protocol.move_labware(sample_plate, "C2", use_gripper=True)
```

## GEN2 Improvements

**Plate lift mechanism:**
- Press button for 3 seconds with lid open to activate
- Raises plate for easier manual or gripper removal
- Improves plate access and prevents damage

**Auto-sealing lid support:**
- Use Opentrons Tough PCR Auto-sealing Lids
- Stack up to 5 lids
- Automated lid placement with gripper

## Best Practices

1. **Set lid temperature before cycling** - Prevents condensation
2. **Use block_max_volume parameter** - Improves temperature accuracy
3. **Deactivate block and lid at protocol end** - Prevents equipment running indefinitely
4. **Open lid before gripper operations** - Required for plate movement
5. **Use profiles for repeated cycles** - More efficient than individual set_block_temperature calls
6. **Cool to 4°C at end** - Preserves samples until retrieval
7. **Plan for thermal equilibration** - Allow extra time for large temperature changes
8. **Test profiles with simulation** - Verify timing before running on hardware
9. **Document profile parameters** - Include reasoning in protocol comments
10. **Consider enzyme specifications** - Match temperatures to polymerase requirements

## Common Mistakes

**❌ Not setting lid temperature:**
```python
tc_mod.close_lid()
tc_mod.execute_profile(...)  # Condensation risk - no lid heating
```

**✅ Correct:**
```python
tc_mod.close_lid()
tc_mod.set_lid_temperature(105)
tc_mod.execute_profile(...)
```

**❌ Gripper movement with closed lid:**
```python
protocol.move_labware(plate, tc_mod, use_gripper=True)  # Error: lid closed
```

**✅ Correct:**
```python
tc_mod.open_lid()
protocol.move_labware(plate, tc_mod, use_gripper=True)
```

**❌ Not deactivating modules:**
```python
tc_mod.execute_profile(...)
# Protocol ends - thermocycler still heating!
```

**✅ Correct:**
```python
tc_mod.execute_profile(...)
tc_mod.deactivate_lid()
tc_mod.deactivate_block()
```

**❌ Wrong profile structure:**
```python
# Missing hold_time_seconds
bad_profile = [
    {"temperature": 95},
    {"temperature": 55}
]
tc_mod.execute_profile(steps=bad_profile, repetitions=30)  # Error
```

**✅ Correct:**
```python
good_profile = [
    {"temperature": 95, "hold_time_seconds": 30},
    {"temperature": 55, "hold_time_seconds": 30}
]
tc_mod.execute_profile(steps=good_profile, repetitions=30)
```

## Troubleshooting

**Module not reaching temperature:**
- Verify temperature is within range (4-99°C block, 37-110°C lid)
- Check ambient temperature for low-temp targets
- Allow sufficient time for thermal equilibration

**Condensation in wells:**
- Ensure lid temperature is set and reached before cycling
- Set lid temperature ≥ highest block temperature + 5-10°C
- Verify lid is properly closed

**Profile not executing:**
- Check profile structure (list of dicts with temperature and hold_time_seconds)
- Verify repetitions parameter is provided
- Ensure block_max_volume matches sample volume

**Plate removal difficulty (GEN1):**
- Wait for block to cool below 60°C
- Use plate lift mechanism (GEN2)
- Ensure lid is fully open

**Gripper errors:**
- Verify lid is open before movement
- Check plate is compatible PCR labware
- Ensure thermocycler position is clear

## Integration with Other Modules

### With Temperature Module (Pre-cooling)

```python
temp_mod = protocol.load_module("temperature module gen2", "D1")
tc_mod = protocol.load_module("thermocyclerModuleV2")

# Pre-cool samples on temperature module
temp_mod.set_temperature(4)
protocol.move_labware(sample_plate, temp_mod, use_gripper=True)
protocol.delay(minutes=5)

# Transfer to thermocycler
tc_mod.open_lid()
protocol.move_labware(sample_plate, tc_mod, use_gripper=True)

# Run PCR
tc_mod.close_lid()
# ... thermal cycling ...
```

### With Magnetic Module (PCR Cleanup)

```python
# After PCR, perform magnetic bead cleanup
tc_mod.set_block_temperature(4)
tc_mod.open_lid()

mag_block = protocol.load_module("magneticBlockV1", "D2")

# Move PCR product to magnetic block
protocol.move_labware(pcr_plate, mag_block, use_gripper=True)

# Cleanup workflow
# ... magnetic bead purification ...
```

## API Version Requirements

- **Minimum API version:** 2.0 (thermocycler support)
- **GEN2 features:** API 2.13+
- **Auto-sealing lids:** API 2.15+
- **Recommended:** 2.19+ for full feature support

## Additional Resources

- **Thermocycler Documentation:** https://docs.opentrons.com/v2/modules/thermocycler.html
- **PCR Protocol Library:** https://protocols.opentrons.com/
- **Opentrons Support:** https://support.opentrons.com/

## Related Skills

- `opentrons` - Main Opentrons Python API skill
- `opentrons-temperature-module` - Simple temperature control (4-95°C)
- `opentrons-gripper` - Automated labware movement (Flex)
- `opentrons-magnetic-block` - Magnetic bead separation for PCR cleanup

---

### phd-qualifier

**Path**: `skills/academic/phd-qualifier/SKILL.md`

# PhD Qualifier Evaluation Skill

You are an experienced Chemical Engineering professor with decades of experience evaluating first-year PhD students in their qualifying examinations. Your role is to provide thorough, constructive evaluation to help students succeed while maintaining rigorous academic standards.

## CRITICAL INSTRUCTION: Document-Based Evaluation

**When a student provides a file (PDF, Word document, PowerPoint, Keynote, etc.):**

1. **YOU MUST READ THE ACTUAL FILE** - Use the Read tool to access and analyze the complete document
2. **ALL FEEDBACK MUST BE BASED ON ACTUAL CONTENT** - Quote specific text, reference specific page numbers, cite actual figures and tables
3. **DO NOT MAKE UP OR ASSUME CONTENT** - Never provide hypothetical examples or assume what might be in the document
4. **DO NOT PROVIDE GENERIC FEEDBACK** - All comments must be specific to what is actually written in the provided file
5. **VERIFY FORMAT COMPLIANCE FROM ACTUAL DOCUMENT** - Count actual pages, measure actual margins, check actual font sizes, count actual words in abstract

**If you cannot read the file or access its contents, explicitly state this and ask for the file to be provided in a readable format.**

**Never provide feedback on a document you have not actually read. All evaluation must be grounded in the actual document content.**

## When to Use This Skill

Use this skill when:
- Reviewing PhD qualifier written reports (must read actual file if provided)
- Evaluating qualifying exam presentations (must read actual slides if provided)
- Preparing questions for oral examinations (must review actual report/presentation if provided)
- Assessing student readiness for PhD research
- Providing feedback on scientific writing and presentation
- Identifying gaps in knowledge or understanding
- Evaluating technical competence and research potential

## Qualifying Exam Overview

The PhD qualifying exam tests **research potential**, **communication skills**, and **general knowledge of chemical engineering**. It consists of two parts:

### Part 1: Written Report

The report documents research accomplished and projected. It consists of three sections only:

**1. Title and Abstract Section (1 page total)**
- Abstract limited to 300 words
- No other material allowed

**2. Body Section (maximum 10 pages total)**
- Includes ALL figures and tables within the 10-page limit
- Any material that is not Title/Abstract or Literature Cited counts as Body
- Typically organized as: Introduction, Methods, Results, Discussion, Outlook

**3. Literature Cited Section (unlimited length, typically 1-2 pages)**
- Full citations required: all authors, full title, full journal name, inclusive page numbers
- Single-spaced (only section that can be)

**Format Requirements (STRICT - non-compliant reports will NOT be accepted):**
- **Margins**: 1 inch on all sides (top, bottom, left, right)
- **Font**: 12-point Times New Roman (or close equivalent) for all text
- **Figures/Tables**: Must fit within 1-inch margins; text must be at least 10-point font
- **Spacing**: Double-spacing for Title/Abstract and Body sections
  - Maximum 23 lines per page (2.56 lines per inch)
- **Literature Cited**: May be single-spaced

**Note**: The student's advisor may provide editorial comments and participate in practice talks.

### Part 2: Examination Day

**Oral Presentation (20 minutes)**
- Formal presentation to Qualifying Exam Committee
- Clear story of research
- Professional graphics and delivery
- Demonstrates mastery of subject

**Question Period (30-60 minutes)**
- Tests depth of knowledge
- Probes understanding of fundamentals
- Explores connections to broader field
- Assesses critical thinking ability

**Note**: The research advisor may attend as a silent observer to provide feedback on performance.

## Official Evaluation Criteria

The following six criteria are used to evaluate the qualifying exam:

### 1. Definition of the Research Problem
- Is the problem clearly stated and well-motivated?
- Is the significance and broader impact articulated?
- Are the research objectives specific and achievable?
- Is the scope appropriate for PhD research?

### 2. Knowledge of Fundamental Principles Involved
- Does the student understand the underlying chemical engineering principles?
- Can they connect their work to thermodynamics, transport phenomena, kinetics, etc.?
- Do they demonstrate grasp of relevant theory?
- Can they explain phenomena at multiple scales (molecular, macro)?

### 3. Knowledge of the Appropriate Literature
- Is the literature review comprehensive and current?
- Are key papers in the field properly cited and discussed?
- Does the student demonstrate awareness of related work?
- Can they place their work in the context of the field?

### 4. Approach to Solution and Quality of Preliminary Results
- Are the methods appropriate for addressing the research problem?
- Is the experimental or computational design sound?
- Are preliminary results compelling and properly analyzed?
- Does the work demonstrate technical competence?

### 5. Ability to Critically Evaluate Preliminary Results and Define Direction of Future Work
- Can the student interpret their results correctly?
- Do they acknowledge limitations and uncertainties?
- Are alternative explanations considered?
- Is the proposed future work logical and realistic?

### 6. Quality of the Written and Oral Presentations
- Is the writing clear, professional, and well-organized?
- Do figures and tables meet quality standards?
- Is the oral presentation coherent and well-delivered?
- Can the student communicate effectively under questioning?

## Part 1: Written Report Evaluation

### Format Compliance Check (REQUIRED FIRST STEP)

**Before evaluating content, verify format compliance:**

- [ ] Title and Abstract: Exactly 1 page
- [ ] Abstract: 300 words or fewer
- [ ] Body: 10 pages or fewer (including ALL figures and tables)
- [ ] Margins: 1 inch on all sides
- [ ] Font: 12-point Times New Roman (or equivalent)
- [ ] Figure/table text: At least 10-point font
- [ ] Spacing: Double-spaced (23 lines per page maximum)
- [ ] Literature Cited: Full citations with all required elements
- [ ] No extraneous material (appendices, supplementary, etc.)

**If format is non-compliant, the report cannot be accepted and must be revised before evaluation.**

### Content Evaluation Framework

When reviewing content, assess against the six official criteria:

### Detailed Format Checking Guide

#### Page Count Verification

**Title and Abstract (Page 1):**
- Must be exactly 1 page
- Title at top
- Abstract below (300 words maximum)
- Count abstract words carefully
- No other content allowed

**Body (Pages 2-11, maximum 10 pages):**
- Everything except Title/Abstract and Literature Cited is Body
- Count ALL pages with figures and tables
- Figures/tables embedded in text count toward 10-page limit
- Introduction, Methods, Results, Discussion, Outlook all count as Body
- If Body exceeds 10 pages, report is NON-COMPLIANT

**Literature Cited (Pages 12+):**
- Unlimited length
- Typically 1-2 pages
- Must contain full citations

#### Margin Check
Use ruler or document properties:
- Top margin: exactly 1 inch
- Bottom margin: exactly 1 inch
- Left margin: exactly 1 inch
- Right margin: exactly 1 inch
- ALL figures and tables must fit within margins

#### Font and Spacing Check
- Body text: 12-point Times New Roman
- Acceptable alternatives: Times, Liberation Serif (close equivalents)
- Figure/table text: Minimum 10-point
- Line spacing: Double (2.0)
- Lines per page: Count - should not exceed 23 lines
- Literature Cited: Single spacing allowed

#### Common Format Violations

**REJECT if present:**
- Abstract exceeds 300 words
- Body exceeds 10 pages
- Margins less than 1 inch
- Font smaller than 12pt (body) or 10pt (figures)
- Single-spacing in Body section
- More than 23 lines per page
- Appendices or supplementary material included
- Incomplete Literature Cited entries

### Section-by-Section Evaluation

Map each section to the official evaluation criteria:

#### Title and Abstract
**Evaluates: Criteria 1 (Problem Definition) and 6 (Presentation Quality)**

**What to Look For:**
- Clear, informative title
- Abstract within 300-word limit
- Problem clearly stated (Criterion 1)
- Significance articulated (Criterion 1)
- Main results summarized
- Key conclusions stated
- Self-contained (understandable without reading full report)

**Format Check:**
- Exactly 1 page total
- 300 words or fewer in abstract
- 12-point font, double-spaced
- 1-inch margins

#### Introduction (Part of Body)
**Evaluates: Criteria 1 (Problem Definition) and 3 (Literature Knowledge)**

**What to Look For:**
- Clear problem statement and motivation (Criterion 1)
- Comprehensive, current literature review (Criterion 3)
- Identification of knowledge gaps (Criterion 1)
- Clear, achievable research objectives (Criterion 1)
- Appropriate scope for first-year work
- Connection to fundamental ChE principles (Criterion 2)

**Common Issues:**
- Too broad or too narrow scope
- Inadequate literature coverage
- Missing key references
- Unclear connection between background and objectives
- Over-claiming novelty

**Evaluation Questions:**
- Is the problem clearly stated and well-motivated?
- Is the literature review current (last 3-5 years emphasized)?
- Are key papers in the field cited?
- Is the gap in knowledge clearly identified?
- Are the objectives specific and achievable?

**Feedback Framework:**
```
Strengths:
- [Specific positive aspects]

Areas for Improvement:
- [Specific gaps or weaknesses]

Critical Issues:
- [Must-address problems]

Suggestions:
- [Concrete recommendations]
```

#### Methods (Part of Body)
**Evaluates: Criteria 2 (Fundamental Principles), 4 (Approach to Solution), 6 (Presentation Quality)**

**What to Look For:**
- Sufficient detail for reproducibility (Criterion 6)
- Appropriate techniques for research problem (Criterion 4)
- Connection to fundamental principles (Criterion 2)
- Understanding of method limitations (Criterion 2)
- Proper controls and validation (Criterion 4)
- Clear experimental or computational design (Criterion 4)
- Sound scientific approach (Criterion 4)

**Common Issues:**
- Insufficient procedural detail
- Missing key parameters
- No discussion of method limitations
- Inadequate controls
- Unclear data analysis procedures

**Evaluation Questions:**
- Could another researcher reproduce this work?
- Are the methods appropriate for the objectives?
- Does the student demonstrate understanding of the techniques?
- Are limitations acknowledged?
- Is the statistical or uncertainty analysis appropriate?

#### Results (Part of Body)
**Evaluates: Criteria 4 (Quality of Preliminary Results), 6 (Presentation Quality)**

**What to Look For:**
- Clear presentation of data (Criterion 6)
- Appropriate figures and tables (Criterion 6)
- Quality of preliminary results (Criterion 4)
- Logical organization (Criterion 6)
- Objective reporting - save interpretation for Discussion (Criterion 6)
- Evidence of successful execution (Criterion 4)
- Reproducibility demonstrated (Criterion 4)
- Proper statistical analysis (Criterion 4)

**Common Issues:**
- Mixing results and discussion
- Poor figure quality or labeling
- Missing error bars or statistical analysis
- Cherry-picking data
- Insufficient replicates
- Over-interpretation in Results section

**Evaluation Questions:**
- Are the results clearly presented?
- Are figures publication-quality?
- Is there sufficient data to support conclusions?
- Are controls included?
- Are uncertainties quantified?

#### Discussion (Part of Body)
**Evaluates: Criteria 2 (Fundamental Principles), 3 (Literature), 5 (Critical Evaluation), 6 (Presentation Quality)**

**What to Look For:**
- Critical evaluation of results (Criterion 5)
- Interpretation connected to literature (Criterion 3)
- Connection to fundamental principles (Criterion 2)
- Acknowledgment of limitations and uncertainties (Criterion 5)
- Alternative explanations considered (Criterion 5)
- Implications identified (Criterion 5)
- Clear, logical conclusions (Criterion 6)

**Common Issues:**
- Simply restating results
- Ignoring contradictory data
- Over-interpretation
- Missing connections to literature
- Not addressing limitations

**Evaluation Questions:**
- Does the discussion interpret, not just repeat, results?
- Are results placed in context of existing literature?
- Are limitations honestly addressed?
- Are alternative explanations considered?
- Do conclusions follow logically from results?

#### Outlook / Future Work (Part of Body)
**Evaluates: Criteria 5 (Define Direction of Future Work), 1 (Problem Definition)**

**What to Look For:**
- Logical next steps that build on current results (Criterion 5)
- Realistic and well-defined future direction (Criterion 5)
- Addresses limitations identified (Criterion 5)
- Identifies key remaining questions (Criterion 1)
- Realistic timeline and scope for PhD work (Criterion 5)
- Shows independent thinking (Criterion 5)
- Clear connection to broader research goals (Criterion 1)

**Common Issues:**
- Vague future plans
- Unrealistic scope
- No connection to current results
- Missing key experiments
- Just listing random ideas

**Evaluation Questions:**
- Do proposed next steps logically follow from results?
- Are they appropriately scoped for PhD timeline?
- Do they address identified limitations?
- Is there evidence of independent thinking?

#### Literature Cited
**Evaluates: Criteria 3 (Literature Knowledge), 6 (Presentation Quality)**

**Format Requirements (STRICT):**
- **Full citations required**: ALL authors listed (no "et al.")
- **Full article title**: Complete, not abbreviated
- **Full journal name**: Not abbreviated (e.g., "Journal of Chemical Engineering" not "J. Chem. Eng.")
- **Inclusive page numbers**: First and last page (e.g., "123-135")
- **Proper citation format**: Consistent style throughout
- **Single-spaced**: Only section that may be single-spaced

**Content Evaluation:**
- Are key papers in the field cited? (Criterion 3)
- Is literature current (emphasis on last 3-5 years)? (Criterion 3)
- Are seminal older works included? (Criterion 3)
- Is breadth appropriate for the problem? (Criterion 3)
- Are references actually cited in the text?

**Common Issues:**
- Incomplete citations (missing authors, pages, or journal names)
- Abbreviated journal names
- Using "et al." in reference list
- Missing key papers in the field
- Outdated references (all pre-2015, for example)
- References cited but not in list, or vice versa
- Inconsistent citation format

**Evaluation Questions:**
- Are all required citation elements present?
- Has the student read and understood these papers?
- Are there obvious gaps in literature coverage?
- Would a domain expert see missing key references?

### Figure and Table Evaluation

**Quality Criteria:**
- **Clarity**: Axes labeled, legends included, fonts readable
- **Appropriateness**: Right type of plot for the data
- **Completeness**: Error bars, sample sizes, conditions specified
- **Professional**: High resolution, consistent style, color-blind friendly
- **Self-contained**: Can be understood without reading full text

**Common Figure Problems:**
- Too small text/labels
- Missing axis labels or units
- No error bars
- Poor color choices
- Unclear legends
- Too much information in one figure
- Too many significant figures

**Evaluation Template for Figures:**
```
Figure X: [Title]
Strengths:
- Clear presentation of [aspect]
- Appropriate use of [technique]

Issues:
- [Specific problem, e.g., "axis labels too small"]
- [Missing element, e.g., "no error bars"]

Suggestions:
- Increase font size to at least 12pt
- Add error bars showing ±1 standard deviation
- Consider splitting into panels for clarity
```

### Writing Quality Assessment

**Technical Writing Standards:**
- Clear, concise prose
- Active voice (where appropriate)
- Past tense for completed work
- Present tense for established facts
- Proper technical terminology
- Consistent notation and abbreviations

**Common Writing Issues:**
- Overly casual language
- Excessive jargon without explanation
- Run-on sentences
- Unclear pronoun references
- Inconsistent verb tense
- Missing transitions between paragraphs

**Grammar and Style:**
- Subject-verb agreement
- Parallel structure
- Proper punctuation
- Consistent formatting
- Appropriate section headings

### Comprehensive Review Checklist

#### Content Completeness
- [ ] All required sections present
- [ ] Each section of appropriate length
- [ ] Key background literature cited
- [ ] Methods sufficiently detailed
- [ ] Results clearly presented
- [ ] Discussion interprets results
- [ ] Outlook identifies next steps
- [ ] References properly formatted

#### Technical Depth
- [ ] Demonstrates understanding of techniques
- [ ] Shows ability to troubleshoot
- [ ] Interprets data correctly
- [ ] Identifies limitations
- [ ] Connects to theory
- [ ] Uses appropriate analysis methods

#### Research Skills
- [ ] Literature review is thorough
- [ ] Experimental design is sound
- [ ] Data collection is systematic
- [ ] Analysis is appropriate
- [ ] Conclusions are supported
- [ ] Future work is realistic

#### Writing Quality
- [ ] Clear and professional
- [ ] Logical flow between sections
- [ ] Figures are high quality
- [ ] Tables are well-formatted
- [ ] References are complete
- [ ] Grammar and spelling are correct

### Overall Assessment Framework

**Pass/Excellent**:
- Thorough literature review
- Clear research objectives
- Appropriate methods well-executed
- Compelling results with proper analysis
- Insightful discussion
- Realistic outlook
- Professional writing throughout
- Demonstrates clear readiness for PhD work

**Pass/Satisfactory**:
- Adequate literature review
- Clear objectives
- Methods mostly appropriate
- Results support conclusions
- Discussion connects to literature
- Reasonable outlook
- Generally professional writing
- Shows potential with some gaps to address

**Conditional Pass**:
- Missing key literature
- Some methodological concerns
- Results incomplete or poorly analyzed
- Weak discussion
- Unclear outlook
- Writing issues that obscure content
- Needs significant revision

**Not Ready**:
- Major gaps in literature
- Serious methodological flaws
- Insufficient or unreliable results
- Lack of critical analysis
- Poor understanding of techniques
- Significant writing problems
- Not demonstrating PhD readiness

## Part 2: Presentation Evaluation

### Presentation Structure Assessment

**Expected Flow (20 minutes):**
- Title/Introduction: 2-3 minutes
- Background/Literature: 3-4 minutes
- Methods: 3-4 minutes
- Results: 6-8 minutes
- Discussion/Conclusions: 3-4 minutes
- Future Work: 1-2 minutes
- Acknowledgments: < 1 minute

### Slide-by-Slide Evaluation

#### Title Slide
- Clear, informative title
- Student name, advisor, department, date
- University/department logo if appropriate

#### Introduction/Motivation Slides
**What to Look For:**
- Compelling motivation
- Clear problem statement
- Accessible to general ChE audience
- Sets up the research questions

**Common Issues:**
- Too much background
- Too technical too quickly
- Unclear why this matters
- Missing "big picture"

#### Background Slides
**What to Look For:**
- Appropriate depth
- Key concepts clearly explained
- Relevant literature highlighted
- Builds logically to research gap

**Common Issues:**
- Information overload
- Assuming too much knowledge
- Too many details
- Missing key context

#### Methods Slides
**What to Look For:**
- Clear overview of approach
- Key techniques explained
- Visual diagrams/schematics
- Appropriate level of detail

**Common Issues:**
- Too detailed (don't need every parameter)
- Not enough context
- Text-heavy slides
- Missing experimental design overview

#### Results Slides
**What to Look For:**
- One clear message per slide
- High-quality figures
- Progressive build of story
- Clear labels and legends
- Logical sequence

**Common Issues:**
- Too much data per slide
- Unclear figures
- No interpretation provided
- Jumping between topics
- Missing controls or comparisons

#### Discussion/Conclusions Slides
**What to Look For:**
- Clear summary of findings
- Connection to literature
- Identified limitations
- Implications stated
- Take-home messages

**Common Issues:**
- Just repeating results
- Missing interpretation
- Over-claiming
- No acknowledgment of limitations

#### Future Work Slide
**What to Look For:**
- Logical next steps
- Clear connection to current work
- Realistic scope
- Exciting opportunities

**Common Issues:**
- Vague plans
- Disconnected from current results
- Overly ambitious
- Just a bullet list with no explanation

### Visual Design Evaluation

**Slide Design Principles:**
- **Simplicity**: One main point per slide
- **Readability**: Large fonts (>24pt for body, >32pt for titles)
- **Consistency**: Uniform style throughout
- **Contrast**: Text easily readable on background
- **Color**: Purposeful, color-blind friendly
- **White space**: Don't overcrowd

**Figure Quality:**
- High resolution (vector graphics preferred)
- Large, clear labels
- Consistent color scheme
- Self-explanatory with minimal text
- Appropriate complexity for oral presentation

**Common Design Issues:**
- Text too small
- Too much text per slide
- Inconsistent formatting
- Busy backgrounds
- Poor color choices (red-green combinations)
- Low-resolution figures
- Unnecessary animations

### Content Evaluation

**Story Coherence:**
- Clear narrative arc
- Logical transitions
- Motivation maintained throughout
- Results build toward conclusions
- Satisfying resolution

**Technical Depth:**
- Appropriate for audience (ChE faculty)
- Key concepts well-explained
- Not oversimplified or overcomplicated
- Shows mastery of subject
- Demonstrates critical thinking

**Time Management:**
- Appropriate content for 20 minutes
- Well-paced (roughly 1 min per slide)
- Most time on results/discussion
- Not rushing or dragging

### Presentation Feedback Template

```
Overall Assessment:
[Strong/Satisfactory/Needs Improvement]

Story and Flow:
Strengths:
- Clear motivation and well-defined problem
- Logical progression from background to results
- Results build convincingly toward conclusions

Areas for Improvement:
- Transition between methods and results could be smoother
- Consider adding a "roadmap" slide after introduction

Content Quality:
Strengths:
- Thorough background coverage
- Results clearly support conclusions
- Good connection to literature

Areas for Improvement:
- Methods section could be more concise
- Discussion of limitations should be more explicit

Visual Design:
Strengths:
- Consistent, professional design
- High-quality figures
- Appropriate use of color

Areas for Improvement:
- Some slides have too much text
- Font size on Figure 3 is too small
- Consider larger axis labels

Specific Slide Comments:
Slide 5: [specific feedback]
Slide 8: [specific feedback]
Slide 12: [specific feedback]

Time Management:
- Current: ~25 slides for 20 minutes
- Recommendation: Reduce to 18-20 slides
- Specific slides to condense or remove: [list]

Priority Improvements:
1. [Most important change]
2. [Second priority]
3. [Third priority]
```

## Part 3: Question Preparation

### Question Categories

Generate questions in these categories:

#### 1. General/Broad Questions
Test understanding of how work connects to Chemical Engineering:

**Examples:**
- "How does your work relate to fundamental principles of [thermodynamics/transport/kinetics]?"
- "What are the industrial applications of your research?"
- "How might your findings impact sustainability or environmental concerns?"
- "Can you connect your work to other areas of chemical engineering?"

#### 2. Technical Depth Questions
Test deep understanding of the work:

**Examples:**
- "Why did you choose [specific method] over [alternative]?"
- "What would happen if you changed [specific parameter]?"
- "How did you validate your [measurement/calculation]?"
- "What are the sources of uncertainty in your results?"
- "Can you derive/explain [key equation] from first principles?"

#### 3. Literature Knowledge Questions
Test awareness of the field:

**Examples:**
- "Are you familiar with the work of [key researcher in field]?"
- "How do your results compare with [specific paper]?"
- "There's recent work by [author] on [related topic]. How does that relate to your work?"
- "What are the current debates in your research area?"

#### 4. Critical Thinking Questions
Test ability to think beyond current work:

**Examples:**
- "What if your hypothesis had been wrong? What would you have concluded?"
- "What are alternative explanations for [specific result]?"
- "What's the most surprising thing you found, and why?"
- "If you could only do one more experiment, what would it be and why?"
- "What are the limitations of your approach?"

#### 5. Future Directions Questions
Test ability to plan research:

**Examples:**
- "What are the next three experiments you should do?"
- "How would you scale this up for industrial application?"
- "What fundamental questions remain unanswered?"
- "How might you extend this to other systems?"

#### 6. Fundamental Knowledge Questions
Test basic ChE concepts:

**Examples:**
- "Can you explain [relevant fundamental principle]?"
- "How does [phenomenon] work at the molecular level?"
- "Derive the relationship between [key variables]"
- "What assumptions are built into [model/equation]?"

### Question Preparation Process

#### Step 1: Document Review
- Read report and presentation thoroughly
- Identify key claims and results
- Note any gaps or unclear points
- Mark sections that need deeper probing

#### Step 2: Literature Search
Use web search to:
- Find recent papers in the same area
- Check if similar work has been done
- Identify key researchers
- Find review articles
- Assess novelty

**Search Strategy:**
- Google Scholar: "[topic] [year]"
- Look for recent reviews
- Check citations of student's references
- Search for competing approaches

#### Step 3: Identify Knowledge Gaps
Look for:
- Missing references
- Unexplained results
- Weak areas in methods
- Unsupported claims
- Areas where deeper understanding is needed

#### Step 4: Generate Questions

Create 15-20 questions organized by:
- Difficulty (easy → hard)
- Topic (broad → specific)
- Type (factual → conceptual → critical)

**Question Quality Criteria:**
- Specific to the student's work
- Test understanding, not memory
- Have clear "good" answers
- Progress logically
- Mix difficulties
- Cover breadth and depth

### Question List Template

```
PhD Qualifier Questions for [Student Name]
Project: [Title]

=== WARM-UP QUESTIONS (5-7 minutes) ===
These establish baseline and help student relax:

1. [Easy question about motivation]
2. [Question about overall approach]
3. [Question about personal contribution]

=== GENERAL CHEMICAL ENGINEERING (5-7 minutes) ===
Connect work to broader field:

4. [Connection to ChE fundamentals]
5. [Industrial relevance]
6. [Sustainability/energy/environment]

=== TECHNICAL DEPTH (10-15 minutes) ===
Probe understanding of methods and results:

7. [Specific method choice]
8. [Parameter justification]
9. [Data interpretation]
10. [Validation/controls]
11. [Uncertainty/error analysis]

=== LITERATURE AND CONTEXT (5-7 minutes) ===
Test awareness of field:

12. [Comparison to key paper]
13. [Recent development awareness]
14. [Alternative approaches]

=== CRITICAL THINKING (10 minutes) ===
Test analytical abilities:

15. [Alternative explanation]
16. [Surprising result interpretation]
17. [Limitation recognition]
18. [Hypothetical scenario]

=== FUTURE DIRECTIONS (5 minutes) ===
Test research planning:

19. [Next experiments]
20. [Long-term vision]

=== FOLLOW-UP/CLARIFICATION ===
(Generated during exam based on responses)

Backup questions if time remains:
21. [Harder technical question]
22. [Broader impact question]
```

### Question Difficulty Calibration

**Easy (Warm-up):**
- Factual, from their report
- Allows demonstration of knowledge
- Builds confidence

**Medium (Core):**
- Requires understanding, not just memory
- Tests connections between concepts
- Probes depth of knowledge

**Challenging (Discriminating):**
- Requires synthesis
- Tests ability to think on feet
- May not have "right" answer
- Shows research maturity

### Areas to Probe Based on Report Weaknesses

If report shows:

**Weak literature review:**
- Ask about specific papers
- Test awareness of recent work
- Probe understanding of context

**Methodological concerns:**
- Ask about method validation
- Probe understanding of technique
- Test troubleshooting abilities

**Data interpretation issues:**
- Ask about alternative explanations
- Probe statistical significance
- Test understanding of uncertainties

**Missing connections:**
- Ask about fundamental principles
- Probe industrial relevance
- Test broader implications

## Evaluation Rubrics

### Written Report Rubric

| Criterion | Excellent (4) | Good (3) | Satisfactory (2) | Needs Work (1) |
|-----------|--------------|----------|------------------|----------------|
| Literature Review | Comprehensive, current, critically analyzed | Thorough, mostly current, good synthesis | Adequate coverage, some gaps | Superficial, missing key works |
| Methods | Clear, detailed, reproducible | Mostly clear, minor gaps | Basic description, some ambiguity | Unclear, incomplete |
| Results | Complete, well-presented, properly analyzed | Good presentation, appropriate analysis | Adequate data, basic analysis | Incomplete, poor presentation |
| Discussion | Insightful, connected to literature, limitations noted | Good interpretation, mostly connected | Basic interpretation, limited context | Weak analysis, missing connections |
| Figures | Professional, clear, self-contained | Good quality, mostly clear | Adequate, some issues | Poor quality, unclear |
| Writing | Excellent clarity, professional, no errors | Clear, professional, minor errors | Generally clear, some issues | Unclear, multiple errors |
| Overall PhD Readiness | Clearly ready, exceeds expectations | Ready with minor gaps | Ready with guidance needed | Not yet ready |

### Presentation Rubric

| Criterion | Excellent (4) | Good (3) | Satisfactory (2) | Needs Work (1) |
|-----------|--------------|----------|------------------|----------------|
| Story Coherence | Compelling narrative, logical flow | Clear story, good flow | Understandable story, some gaps | Unclear narrative, disconnected |
| Content Quality | Appropriate depth, well-explained | Good content, mostly clear | Adequate content, some confusion | Insufficient or unclear content |
| Visual Design | Professional, clear, consistent | Good design, readable | Acceptable, some issues | Poor design, hard to read |
| Time Management | Perfect pacing, right amount of content | Good pacing, appropriate content | Slightly off pace, manageable | Poor timing, wrong amount |
| Technical Depth | Demonstrates mastery, handles complexity | Shows good understanding | Shows basic understanding | Understanding unclear |

### Question Session Assessment

Evaluate responses on:

**Knowledge Depth:**
- Can explain methods thoroughly
- Understands underlying principles
- Aware of literature

**Critical Thinking:**
- Considers alternatives
- Recognizes limitations
- Makes logical connections

**Communication:**
- Answers clearly and directly
- Admits when doesn't know
- Asks for clarification when needed

**Research Maturity:**
- Shows independent thinking
- Identifies important questions
- Sees bigger picture

## Response Frameworks

**CRITICAL REMINDER: Before providing any feedback, you MUST:**
1. Read the actual document using the Read tool
2. Base ALL feedback on actual content from the file
3. Quote specific text and reference specific page numbers
4. Never make up examples or assume content
5. If you cannot read the file, explicitly state this

### When Providing Written Report Feedback

**REQUIRED FIRST STEP: Read the actual report file**
- Use Read tool to access the complete document
- Note the actual page count, section lengths, figures present
- Verify all format requirements from actual document
- Base all feedback on what is actually written

```
PhD QUALIFYING EXAM REPORT REVIEW
[Document: filename.pdf - XX pages reviewed]

=== FORMAT COMPLIANCE CHECK ===
(Based on actual document analysis)
Status: [COMPLIANT / NON-COMPLIANT]

Format Issues (if any):
□ Title/Abstract: [Issues or ✓]
□ Abstract word count: [XX words] [✓ ≤300 / ✗ >300]
□ Body page count: [XX pages] [✓ ≤10 / ✗ >10]
□ Margins: [✓ 1 inch / ✗ Issues noted]
□ Font size: [✓ 12pt / ✗ Issues noted]
□ Spacing: [✓ Double / ✗ Issues noted]
□ Literature Cited: [✓ Complete / ✗ Incomplete citations]

**If NON-COMPLIANT: Report must be revised to meet format requirements before content evaluation.**

=== EXECUTIVE SUMMARY ===
Overall Assessment: [Pass/Conditional/Not Ready]

Evaluation Against Official Criteria:
1. Problem Definition: [Strong/Adequate/Weak]
2. Fundamental Principles: [Strong/Adequate/Weak]
3. Literature Knowledge: [Strong/Adequate/Weak]
4. Approach & Results Quality: [Strong/Adequate/Weak]
5. Critical Evaluation & Future Work: [Strong/Adequate/Weak]
6. Presentation Quality: [Strong/Adequate/Weak]

Key Strengths:
- [Specific strength tied to criterion]
- [Specific strength tied to criterion]

Critical Issues:
- [Must-address problem with criterion reference]
- [Must-address problem with criterion reference]

Recommended Revisions: [Priority items]

=== DETAILED SECTION FEEDBACK ===

Title and Abstract (Criterion 1, 6)
Strengths:
- [Specific positive aspects]

Issues:
- [Specific problems with page/line references]

Recommendations:
- [Concrete suggestions]

Introduction (Criterion 1, 2, 3)
Problem Definition (Criterion 1):
- [Assessment of problem statement]

Literature Knowledge (Criterion 3):
- [Assessment of literature review]

Fundamental Principles (Criterion 2):
- [Assessment of ChE principles connection]

Recommendations:
- [Specific improvements needed]

Methods (Criterion 2, 4, 6)
Approach Quality (Criterion 4):
- [Assessment of methods appropriateness]

Fundamental Understanding (Criterion 2):
- [Assessment of principle understanding]

Recommendations:
- [Specific improvements needed]

Results (Criterion 4, 6)
Quality of Results (Criterion 4):
- [Assessment of preliminary results]

Presentation (Criterion 6):
- [Assessment of clarity and organization]

Recommendations:
- [Specific improvements needed]

Discussion (Criterion 2, 3, 5, 6)
Critical Evaluation (Criterion 5):
- [Assessment of result interpretation]

Connection to Literature (Criterion 3):
- [Assessment of context]

Recommendations:
- [Specific improvements needed]

Outlook/Future Work (Criterion 1, 5)
Future Direction (Criterion 5):
- [Assessment of proposed next steps]

Recommendations:
- [Specific improvements needed]

Literature Cited (Criterion 3, 6)
Issues:
- [List any format or content problems]

Missing Key Papers:
- [List important missing references, if any]

Recommendations:
- [Specific corrections needed]

=== FIGURES AND TABLES ===
Figure 1: [Title]
- Format: [✓ Compliant / Issues: XX]
- Quality: [Assessment]
- Recommendations: [Specific changes]

[Repeat for each figure/table]

=== RECOMMENDATIONS FOR REVISION ===

PRIORITY 1 (Required for acceptance):
1. [Critical fix with section reference]
2. [Critical fix with section reference]

PRIORITY 2 (Strongly recommended):
1. [Important improvement with section reference]
2. [Important improvement with section reference]

PRIORITY 3 (Will strengthen report):
1. [Enhancement with section reference]
2. [Enhancement with section reference]

=== OVERALL ASSESSMENT ===

This report [does / does not] demonstrate readiness for PhD-level research.

[Detailed concluding assessment addressing all six criteria]

Recommendation: [Pass / Conditional Pass with revisions / Not Ready]

[Encouraging closing remarks and guidance]
```

### When Providing Presentation Feedback

```
PRESENTATION REVIEW

Overall: [Strong/Good/Needs Work]

Strengths:
1. [Specific strength with example]
2. [Specific strength with example]
3. [Specific strength with example]

Areas for Improvement:
1. [Specific issue with concrete suggestion]
2. [Specific issue with concrete suggestion]
3. [Specific issue with concrete suggestion]

Slide-Specific Comments:
[Organized list of slide-by-slide feedback]

Priority Changes for Final Version:
1. [Highest priority]
2. [Second priority]
3. [Third priority]

Time Management:
[Assessment and recommendations]

Delivery Tips:
[Suggestions for oral delivery]
```

### When Generating Questions

```
QUALIFYING EXAM QUESTIONS

Student: [Name]
Project: [Title]
Date: [Date]

BACKGROUND NOTES
[Brief summary of project and any concerns]

QUESTIONS (organized by category and difficulty)

[Organized list as per template above]

AREAS OF PARTICULAR FOCUS
[Topics that need extra probing]

POTENTIAL ISSUES TO EXPLORE
[Gaps or concerns to address]

NOTES FOR COMMITTEE
[Any special considerations or background]
```

## Best Practices

### When Evaluating Reports

1. **Read through completely first** - Get overall impression before detailed evaluation
2. **Be specific** - Point to exact locations, not vague criticisms
3. **Be constructive** - Frame criticism as opportunities for improvement
4. **Balance criticism and praise** - Acknowledge what's done well
5. **Prioritize feedback** - Distinguish must-fix from nice-to-have
6. **Check assumptions** - What's appropriate for first-year vs later
7. **Be fair** - Consider student's starting point and available time

### When Evaluating Presentations

1. **Consider the audience** - Should be accessible to ChE faculty
2. **Check timing** - One minute per slide rule of thumb
3. **Assess story flow** - Does it build logically?
4. **Evaluate visuals** - Can you read from back of room?
5. **Think about questions** - What will committee ask?

### When Preparing Questions

1. **Do background research** - Search recent literature
2. **Start broad, go deep** - Build from general to specific
3. **Mix difficulty levels** - Include warm-up and challenging
4. **Test understanding** - Not just memory
5. **Be fair but rigorous** - Expect PhD-level thinking
6. **Leave room for follow-up** - Not everything needs advance preparation

## Common Pitfalls to Identify

### In Reports

- Over-claiming novelty without adequate literature review
- Methods section that couldn't be reproduced
- Cherry-picking data, ignoring contradictions
- Figures that are illegible or poorly designed
- Discussion that just repeats results
- No acknowledgment of limitations
- Unrealistic future plans

### In Presentations

- Information overload (too many slides)
- Text-heavy slides
- Tiny fonts
- Unclear figures
- Poor time management
- No clear take-home message
- Assuming too much background knowledge

### In Exam Preparation

- Questions that are too vague
- Only asking memory questions
- Not checking recent literature
- Missing fundamental understanding tests
- Not probing weak areas from report

## Example Scenarios

### Strong Qualifier Package

**Report:**
- Thorough 50+ reference literature review
- Clear, detailed methods
- Comprehensive results with proper analysis
- Insightful discussion connecting to theory
- Realistic, well-thought-out outlook
- Professional figures with proper error analysis
- Clear, well-written throughout

**Presentation:**
- Compelling motivation
- Clear, logical story
- Professional slides with readable figures
- Appropriate depth for 20 minutes
- Strong conclusions and future plans

**Recommended Questions:**
Focus on:
- Deeper theoretical understanding
- Broader applications
- Future research directions
- Publications plans

### Adequate But Needs Improvement

**Report:**
- Basic literature review, missing some key papers
- Methods mostly clear but some ambiguity
- Results adequate but analysis could be deeper
- Discussion somewhat shallow
- Future work vague
- Some figure quality issues

**Presentation:**
- Story understandable but not compelling
- Some slides too text-heavy
- Figures adequate but could be clearer
- Timing slightly off

**Recommended Questions:**
Focus on:
- Literature gaps (specific papers to ask about)
- Method choices and alternatives
- Data interpretation depth
- Next experimental steps

### Concerning Package

**Report:**
- Thin literature review
- Methods unclear or incomplete
- Limited results or poor analysis
- Weak discussion
- No clear path forward
- Poor figure quality

**Presentation:**
- Unclear story
- Text-heavy slides
- Poor visual design
- Timing problems

**Recommended Questions:**
Focus on:
- Fundamental understanding
- Method validation
- Basic data interpretation
- Whether student can identify problems

## Maintaining Appropriate Standards

Remember:

**This is a qualifying exam, not a defense**
- Should show potential, not perfection
- First-year work, not thesis-level
- Learning is still happening
- Some gaps are expected

**But standards matter**
- Must demonstrate PhD-level thinking
- Should show research skills are developing
- Need to see ability to work independently
- Literature knowledge should be developing

**Goal is developmental**
- Identify what student knows well
- Find gaps that need addressing
- Provide guidance for next steps
- Build confidence while maintaining rigor
- Help student succeed in PhD program

## Resources and Tools

When evaluating, consider using:

**Literature Search:**
- Google Scholar for recent papers
- Web of Science for citation analysis
- SciFinder for chemistry-specific searches
- Review articles for field overview

**Technical Verification:**
- Check equations and calculations
- Verify units and conversions
- Confirm statistical methods
- Review figure legends and captions

**Writing Tools:**
- Grammar and clarity
- Citation format
- Figure numbering
- Reference completeness

---

### planning

**Path**: `skills/productivity/planning/SKILL.md`

# Planning & Project Breakdown Skill

Guide users through structured, realistic planning for projects, goals, and strategic initiatives using proven project management frameworks.

## Quick Start Workflow

When a planning request arrives, follow this systematic approach:

1. **Clarify**: Understand goal, constraints, deadline, resources
2. **Choose Approach**: Select planning methodology based on project type
3. **Decompose**: Break down into phases, milestones, and tasks
4. **Sequence**: Identify dependencies and critical path
5. **Estimate**: Set realistic timelines with buffers (20-30% for uncertain work)
6. **Define Success**: Establish milestones and success criteria
7. **Identify Risks**: Anticipate obstacles and plan mitigation
8. **Document**: Create clear, actionable plan

## When to Use This Skill

Activate for requests involving:
- "Help me plan..." / "Create a roadmap for..."
- "Break down this project..." / "What are the steps to..."
- "How should I approach..." / "Build a timeline for..."
- Strategic planning, project kickoff, goal setting

## Clarification Phase

Before planning, gather essential information:

**Goal & Scope**:
- What are you trying to achieve? (clear end state)
- What's in/out of scope?
- What would success look like?

**Constraints**:
- Deadline? Fixed or flexible?
- Resources available? (people, budget, tools)
- Dependencies? (external factors, approvals)
- Non-negotiables?

**Context**:
- Stakeholders? Decision makers?
- Past lessons learned?
- Similar projects to reference?

**Don't skip this**: 5 minutes of clarification saves hours later.

## Planning Approach Selector

Choose methodology based on project characteristics:

### Work Breakdown Structure (WBS)
**Use when**: Large, complex projects; scope unclear; need comprehensive task inventory

**Process**: Top-down decomposition (Project → Phases → Deliverables → Tasks)

**Best for**: Construction, IT projects, events, product launches

### Backward Planning
**Use when**: Fixed deadline; event planning; goal clear but path uncertain

**Process**: Start from end goal, work backwards identifying prerequisites

**Best for**: Event planning, product launches, campaigns, deadline-driven work

### Agile/Iterative Planning
**Use when**: Uncertain requirements; need flexibility; can deliver incrementally

**Process**: Plan in short iterations (sprints), adapt based on learning

**Best for**: Software development, research, new product development

### Phased/Milestone Planning
**Use when**: Long project (3+ months); need checkpoints; staged delivery

**Process**: Divide into phases with gates, plan phase-by-phase

**Best for**: Research, construction, strategic initiatives, transformation

### Hybrid Approach
**Combine methods**: WBS for decomposition + Agile for execution, etc.

See `references/frameworks-detailed.md` for detailed guides on each methodology.

## Core Frameworks (Concise)

### Work Breakdown Structure (WBS)

Hierarchical decomposition of work:

**Levels**: Project → Phases (3-7) → Deliverables → Tasks (1-3 days each) → Sub-tasks (if needed)

**100% Rule**: Each level represents 100% of parent's work

**Tips**:
- Nouns for deliverables, verbs for tasks
- Stop when tasks are 1-3 days
- 3-4 levels usually sufficient

### Backward Planning

**Process**:
1. Define end goal (what, when, success criteria)
2. Ask: "What must happen right before this?"
3. Continue backwards to present
4. Reverse sequence for forward plan
5. Add parallel tasks and dependencies
6. Estimate durations

**Key**: Be thorough with prerequisites - missing steps are common

### Critical Path Method

Identify longest sequence of dependent tasks (determines minimum duration):

**Concepts**:
- **Critical Path**: Tasks with zero slack (can't be delayed)
- **Float/Slack**: Time a task can delay without affecting project

**Use**: Focus management attention on critical path tasks

Use `scripts/critical_path.py` for calculation.

### Timeline Estimation

**Methods**:
- **Bottom-Up**: Estimate each task, sum up (most accurate)
- **Top-Down**: Estimate overall, allocate to phases (faster)
- **Three-Point**: (Optimistic + 4×Most Likely + Pessimistic) / 6
- **Analogous**: Compare to similar past projects

**Key Principles**:
- Include 20-30% buffers for uncertain work
- Distinguish effort vs duration (40 hours work ≠ 40 hours elapsed)
- Account for capacity (people aren't 100% productive)
- Add contingency for risk mitigation

**Avoid**: Planning fallacy (underestimating), optimism bias

See `references/estimation-techniques.md` for detailed methods.

### Milestones & Success Criteria

**Good Milestones**:
- Specific, measurable, meaningful
- Time-bound, visible to stakeholders
- Represent significant progress

**Types**: Deliverable completion, decision point, event, phase completion

**Spacing**: Weekly (short projects), bi-weekly/monthly (medium), monthly/quarterly (long)

### OKR Framework (Objectives & Key Results)

**Structure**: 1 Objective + 3-5 Key Results

**Objective**: Qualitative, aspirational goal (what to achieve)

**Key Results**: Quantitative measures (how to measure success)

**Example**:
- Objective: Launch product successfully to market
- Key Results: 1000 active users first month; NPS 50+; $50K MRR by Q4 end

**Use for**: Strategic planning, not tactical tasks

### SMART Goals

**S**pecific - **M**easurable - **A**chievable - **R**elevant - **T**ime-bound

**Example**: "Increase NPS from 30 to 50 by Q4 end through improved onboarding"

**Use for**: Individual goals, small initiatives

## Dependencies & Sequencing

### Dependency Types

**Finish-to-Start** (most common): B starts when A finishes

**Start-to-Start**: B starts when A starts

**Finish-to-Finish**: B finishes when A finishes

### Identifying Dependencies

**Ask for each task**:
- What must complete before this starts?
- What can run in parallel?
- What's waiting for this?
- Any external dependencies?

**Categories**: Mandatory (technical), Discretionary (preference), External (outside control), Internal (team control)

### Parallelization

**Look for**:
- Tasks with no dependencies
- Tasks with same prerequisites
- Tasks that can be split

**Benefit**: Shorter duration, better resource use

**Caution**: Don't over-parallelize (coordination overhead)

## Risk Management (Brief)

### Identification

**Common categories**: Schedule, Technical, Resource, External, Scope, Quality

**Ask**: "What could go wrong?" Review past issues. Use pre-mortems.

### Assessment

**For each risk**: Probability (1-5) × Impact (1-5) = Risk Score

**Prioritize**: High-score risks for mitigation

### Response Strategies

- **Avoid**: Eliminate risk by changing plan
- **Mitigate**: Reduce probability or impact
- **Transfer**: Shift to another party (insurance, contracts)
- **Accept**: Monitor with contingency plan

See `references/frameworks-detailed.md` for risk register template.

## Resource Planning (Brief)

### Categories

**People**: Roles, skills, time commitment, availability

**Tools**: Software, hardware, procurement time

**Budget**: Personnel, tools, contingency (10-20%)

**Other**: Space, materials, information access

### RACI Matrix

**R**esponsible (does work) - **A**ccountable (ultimately accountable) - **C**onsulted (provides input) - **I**nformed (kept in loop)

See `references/templates.md` for RACI template.

## Planning Horizons

**Strategic** (Annual/Quarterly): Goals, themes, major initiatives. Tools: OKRs, roadmaps. Review quarterly.

**Tactical** (Monthly/Sprint): Deliverables, projects. Tools: WBS, sprint planning. Review weekly.

**Operational** (Weekly/Daily): Immediate tasks. Tools: Task lists, kanban. Review daily.

**Principle**: Plan detail should match certainty - detailed near-term, high-level long-term.

## Agile/Iterative Planning (Brief)

### Sprint Planning (2-week iterations)

1. Review prioritized backlog
2. Select items based on team capacity
3. Break items into tasks
4. Estimate and commit
5. Define sprint goal

### Iteration Reviews

- **Demo**: Show completed work
- **Retrospective**: What went well, what to improve
- **Adapt**: Adjust for next iteration

**Tips**: Keep iterations short (1-2 weeks). Don't skip retrospectives. Protect from disruptions.

See `references/templates.md` for sprint planning template.

## Contingency Planning

### Buffers

**Schedule**: 20-30% for uncertain work, more for novel/complex

**Resource**: 10-20% budget contingency, backup personnel

**Scope**: Prioritize features (must-have vs nice-to-have), have cut list

### Plan B

**For critical paths, ask**:
- What if this takes 2x longer?
- What if resources unavailable?
- What if dependency fails?

**Document**: Trigger points, alternatives, decision makers

### Monitoring & Adaptation

**Track**: Compare actual vs planned, identify variances early

**Re-plan when**: Assumptions wrong, scope changes, resource changes, risks occur

**Remember**: Plans are tools, not contracts. Adapt when reality differs.

## Documentation Formats

### Essential Plan Elements

1. Goal/Objective (what and why)
2. Scope (included/excluded)
3. Timeline (key dates, milestones)
4. Tasks/Phases (work breakdown)
5. Dependencies (critical path)
6. Resources (who, what needed)
7. Risks (identified + responses)
8. Success Criteria (measurements)

### Output Formats

**High-Level Plan**:
```
PROJECT: [Name]
GOAL: [What achieving]
TIMELINE: [Start] - [End]
OWNER: [Person]

PHASES:
1. Phase Name (dates) - Major deliverables
2. Phase Name (dates) - Major deliverables

MILESTONES:
- [Date]: Milestone
- [Date]: Milestone

TOP RISKS:
1. Risk [Mitigation]
2. Risk [Mitigation]
```

**Detailed Task List**:
```
TASK: [Description]
├─ Owner: [Person]
├─ Duration: [Estimate]
├─ Dependencies: [Prerequisites]
├─ Deliverable: [Output]
└─ Status: [Not started/In progress/Complete]
```

Use `scripts/timeline_visualizer.py` for visual timelines.

See `assets/templates/` for ready-to-use formats.

## Common Patterns

**Product Launch**: Backward plan from launch date, include dry-run, post-launch monitoring

**Research Project**: WBS + phased approach, exploratory time, iteration based on findings

**Event Planning**: Backward plan, critical path for venue/speakers, detailed day-of checklist

**Software Dev**: Agile sprints, testing in each iteration, deployment and monitoring

**Process Improvement**: Phased rollout, training/change management, measurement cycles

## Tips for Effective Facilitation

1. **Start with why** - Ensure goal clarity before methodology
2. **Right-size approach** - Don't over-plan simple projects
3. **Involve the team** - People doing work should help plan
4. **Plan iteratively** - Start high-level, refine progressively
5. **Include buffers** - Be realistic about uncertainty
6. **Make it visual** - Diagrams > text walls
7. **Assign ownership** - Every task needs owner
8. **Plan for learning** - First time takes longer
9. **Build in reviews** - Regular check-ins catch issues early
10. **Stay flexible** - Reality trumps plans

## Using Supporting Resources

Additional resources in this skill:

- **references/frameworks-detailed.md**: Step-by-step methodology guides
- **references/estimation-techniques.md**: Complete time estimation methods
- **references/templates.md**: Ready-to-use planning templates
- **scripts/critical_path.py**: Calculate project critical path
- **scripts/timeline_visualizer.py**: Generate visual timelines
- **assets/templates/**: Markdown and CSV templates for immediate use

Reference these for deeper guidance or ready-made formats.

---

**Remember**: The best plan is the one that gets executed. Make plans clear, actionable, and realistic. Perfect planning is the enemy of starting.

---

### pycalphad

**Path**: `skills/programming/pycalphad/SKILL.md`

# pycalphad - Computational Thermodynamics in Python

## Overview

**pycalphad** is a Python library for computational thermodynamics within the CALPHAD (CALculation of PHAse Diagrams) framework. It enables users to design thermodynamic models, compute phase diagrams, calculate phase equilibria, and predict thermodynamic properties for multicomponent materials systems.

**Core capabilities:**
- Read and process Thermo-Calc database (TDB) files
- Calculate binary, ternary, and multicomponent phase diagrams
- Perform Gibbs energy minimization for phase equilibrium
- Compute thermodynamic properties (activity, chemical potentials, heat capacity)
- Model ordered phases and systems with charged species
- Calculate driving forces and metastability

**Target users:** Researchers and practitioners in materials science, metallurgy, and computational thermodynamics who want to perform CALPHAD calculations programmatically.

## When to Use This Skill

Use pycalphad when:
- Calculating phase diagrams (binary, ternary, or multicomponent systems)
- Determining phase equilibria at specific temperature/composition/pressure conditions
- Computing thermodynamic properties from CALPHAD databases
- Analyzing metastability and driving forces for phase transformations
- Modeling systems with ordered phases or charged species
- Automating thermodynamic calculations for materials design
- Creating custom thermodynamic models or property frameworks
- Visualizing phase stability regions and property maps

**Don't use when:**
- You need first-principles calculations (use VASP, Quantum ESPRESSO, GPAW instead)
- You need molecular dynamics simulations (use ASE, LAMMPS instead)
- Working with kinetic models (pycalphad is for equilibrium thermodynamics)

## Core Concepts

### 1. Database (TDB Files)
Thermodynamic databases in Thermo-Calc format containing:
- Phase definitions and constituents
- Gibbs energy models for each phase
- Model parameters fitted to experimental data
- Temperature, composition, and pressure ranges

### 2. Components and Species
- **Components**: Independent chemical elements (e.g., Al, Fe, Ni)
- **Species**: Charged or molecular entities in ionic systems

### 3. Phases
Distinct thermodynamic phases with specific crystal structures and energy models:
- Liquid, FCC, BCC, HCP, intermetallic compounds, etc.
- Each phase has a sublattice model defining site occupancy

### 4. State Variables
Conditions that define the system state:
- Temperature (T)
- Pressure (P)
- Composition (mole fractions: X_i, Y_i, etc.)
- Chemical potential (MU)

### 5. Calculate vs Equilibrium
- **calculate()**: Evaluate properties at specified conditions (no equilibrium minimization)
- **equilibrium()**: Find stable phase assemblage by minimizing Gibbs energy

## Quick Reference

| Task | Function | Key Parameters |
|------|----------|----------------|
| Load database | `Database('database.tdb')` | TDB file path |
| Binary phase diagram | `binplot(db, comps, phases, conditions)` | components, phases, T/X ranges |
| Equilibrium calculation | `equilibrium(db, comps, phases, conditions)` | P, T, composition |
| Property calculation | `calculate(db, comps, phases, conditions)` | P, T, composition, output |
| Ternary diagram | Use `equilibrium()` with 3 components | T, P, composition grid |
| Activity calculation | Access from equilibrium dataset | `ACR_*` variables |
| Driving force | `equilibrium()` with metastable phases | Compare energies |

## Installation

```bash
# via pip
pip install pycalphad

# via conda
conda install -c conda-forge pycalphad

# development version
pip install git+https://github.com/pycalphad/pycalphad.git
```

**Dependencies:** Python 3.9+, numpy, scipy, xarray, sympy, matplotlib

## Common Workflows

### 1. Binary Phase Diagram

```python
from pycalphad import Database, binplot
import matplotlib.pyplot as plt

# Load thermodynamic database
db = Database('alzn_mey.tdb')

# Define components and phases
comps = ['AL', 'ZN', 'VA']  # VA = vacancy for gas phase
phases = ['LIQUID', 'FCC_A1', 'HCP_A3']

# Plot isobaric (constant pressure) binary diagram
fig = plt.figure(figsize=(8, 6))
binplot(db, comps, phases,
        conditions={'P': 101325, 'T': (300, 1000, 10), 'X(ZN)': (0, 1, 0.01)})
plt.xlabel('X(ZN)')
plt.ylabel('Temperature (K)')
plt.title('Al-Zn Binary Phase Diagram')
plt.savefig('al-zn-diagram.png')
```

### 2. Equilibrium Calculation at Fixed Conditions

```python
from pycalphad import Database, equilibrium, variables as v
import numpy as np

# Load database
db = Database('nist_ni_al.tdb')

# Define system
comps = ['NI', 'AL', 'VA']
phases = ['LIQUID', 'FCC_L12', 'BCC_B2']

# Calculate equilibrium at 1500K, 1 atm, 50 at% Al
result = equilibrium(db, comps, phases,
                    {v.T: 1500, v.P: 101325, v.X('AL'): 0.5})

# Access results
print("Stable phases:", result.Phase.values.squeeze())
print("Phase fractions:", result.NP.values.squeeze())
print("Compositions:", result.X.values.squeeze())
print("Gibbs energy:", result.GM.values.squeeze())
```

### 3. Property Map (T-X Diagram with Property Overlay)

```python
from pycalphad import Database, equilibrium, variables as v
import numpy as np
import matplotlib.pyplot as plt

db = Database('crfe.tdb')
comps = ['CR', 'FE', 'VA']
phases = ['LIQUID', 'BCC_A2', 'SIGMA']

# Create T-X grid
temps = np.linspace(1000, 2000, 50)
x_cr = np.linspace(0, 1, 50)
T_grid, X_grid = np.meshgrid(temps, x_cr)

# Calculate equilibrium at each point
result = equilibrium(db, comps, phases,
                    {v.T: T_grid.flatten(),
                     v.P: 101325,
                     v.X('CR'): X_grid.flatten()},
                    pdens=500)

# Extract heat capacity
cp = result.CPM.values.reshape(T_grid.shape)

# Plot
plt.contourf(X_grid, T_grid, cp, levels=20, cmap='viridis')
plt.colorbar(label='Heat Capacity (J/mol-atom-K)')
plt.xlabel('X(CR)')
plt.ylabel('Temperature (K)')
plt.title('Cr-Fe Heat Capacity Map')
plt.savefig('crfe_cp_map.png')
```

### 4. Activity Calculation

```python
from pycalphad import Database, equilibrium, variables as v

db = Database('feni.tdb')
comps = ['FE', 'NI', 'VA']
phases = ['FCC_A1']

# Calculate at specific conditions
result = equilibrium(db, comps, phases,
                    {v.T: 1200, v.P: 101325, v.X('NI'): 0.3})

# Extract activities (relative to pure element reference state)
activity_fe = result['ACR_FE'].values.squeeze()
activity_ni = result['ACR_NI'].values.squeeze()

print(f"Activity of Fe: {activity_fe:.4f}")
print(f"Activity of Ni: {activity_ni:.4f}")

# Chemical potentials
mu_fe = result['MU_FE'].values.squeeze()
mu_ni = result['MU_NI'].values.squeeze()

print(f"Chemical potential Fe: {mu_fe:.2f} J/mol")
print(f"Chemical potential Ni: {mu_ni:.2f} J/mol")
```

### 5. Ternary Phase Diagram

```python
from pycalphad import Database, equilibrium, variables as v
from pycalphad.plot.eqplot import eqplot
import numpy as np
import matplotlib.pyplot as plt

db = Database('ternary.tdb')
comps = ['AL', 'CU', 'ZN', 'VA']
phases = ['LIQUID', 'FCC_A1', 'HCP_A3', 'THETA']

# Calculate equilibrium at constant T
result = equilibrium(db, comps, phases,
                    {v.T: 800, v.P: 101325,
                     v.X('CU'): (0, 1, 0.02),
                     v.X('ZN'): (0, 1, 0.02)},
                    pdens=2000)

# Plot using ternary coordinates
fig = plt.figure(figsize=(8, 8))
eqplot(result, x=v.X('CU'), y=v.X('ZN'))
plt.title('Al-Cu-Zn Ternary at 800K')
plt.savefig('alcuzn_ternary.png')
```

### 6. Driving Force for Precipitation

```python
from pycalphad import Database, equilibrium, variables as v
import numpy as np

db = Database('alcu.tdb')
comps = ['AL', 'CU', 'VA']

# Supersaturated parent phase
parent_phases = ['FCC_A1']

# Calculate parent phase energy (metastable)
parent_eq = equilibrium(db, comps, parent_phases,
                       {v.T: 500, v.P: 101325, v.X('CU'): 0.02})
gm_parent = parent_eq.GM.values.squeeze()

# Equilibrium with precipitate phase allowed
all_phases = ['FCC_A1', 'THETA']
eq = equilibrium(db, comps, all_phases,
                {v.T: 500, v.P: 101325, v.X('CU'): 0.02})
gm_stable = eq.GM.values.squeeze()

# Driving force for precipitation
driving_force = gm_parent - gm_stable
print(f"Driving force: {driving_force:.2f} J/mol-atom")
```

### 7. T0 Temperature Calculation

```python
from pycalphad import Database, equilibrium, variables as v
import numpy as np
from scipy.optimize import brentq

db = Database('steel.tdb')
comps = ['FE', 'C', 'VA']

def t0_condition(temp, composition):
    """Calculate GM difference between phases at equal composition"""
    # FCC energy
    fcc_eq = equilibrium(db, comps, ['FCC_A1'],
                        {v.T: temp, v.P: 101325, v.X('C'): composition})
    gm_fcc = fcc_eq.GM.values.squeeze()

    # BCC energy
    bcc_eq = equilibrium(db, comps, ['BCC_A2'],
                        {v.T: temp, v.P: 101325, v.X('C'): composition})
    gm_bcc = bcc_eq.GM.values.squeeze()

    return gm_fcc - gm_bcc

# Find T0 temperature where FCC and BCC have equal energy
composition = 0.01  # 1 at% C
try:
    t0_temp = brentq(t0_condition, 800, 1200, args=(composition,))
    print(f"T0 temperature at X(C)={composition}: {t0_temp:.1f} K")
except ValueError:
    print("T0 not found in temperature range")
```

### 8. Partial Ordering in Phases

```python
from pycalphad import Database, equilibrium, variables as v

# Database with ordering model (e.g., Al-Ni with L12 ordering)
db = Database('alni.tdb')
comps = ['AL', 'NI', 'VA']
phases = ['FCC_L12']  # FCC with L12 ordering capability

# Calculate at composition where ordering occurs
result = equilibrium(db, comps, phases,
                    {v.T: 900, v.P: 101325, v.X('NI'): 0.25})

# Access site fractions to determine ordering
# FCC_L12 has sublattices for corner and face sites
print("Phase composition:")
for phase in result.Phase.values.squeeze():
    if phase != '':
        print(f"  {phase}")

# Site fractions show ordering (corner vs face sites)
print("Site fractions:", result.Y.values.squeeze())
```

## Key Output Variables

### From equilibrium() Dataset (xarray)

**Phase Information:**
- `Phase`: Names of stable phases
- `NP`: Phase fractions (moles)
- `Mode`: Phase mode (for output control)

**Composition:**
- `X`: Overall mole fractions
- `Y`: Site fractions (sublattice model)
- `MU`: Chemical potentials (J/mol)

**Thermodynamic Properties:**
- `GM`: Molar Gibbs energy (J/mol-atom)
- `HM`: Molar enthalpy (J/mol-atom)
- `SM`: Molar entropy (J/mol-atom-K)
- `CPM`: Molar heat capacity (J/mol-atom-K)

**Activity:**
- `ACR_*`: Activity (ratio to reference state)
- Reference states defined in database

**Structure:**
Dataset is indexed by conditions (P, T, X) and has dimensions for phases, components, sublattices.

## Advanced Features

### Custom Models

```python
from pycalphad import Model, Database

# Extend base Model class for custom properties
class ViscosityModel(Model):
    def build_viscosity(self):
        """Add viscosity as a custom property"""
        # Implementation here
        pass

# Use custom model
db = Database('liquid.tdb')
model = ViscosityModel(db, ['AL', 'CU'], 'LIQUID')
```

### xarray Dataset Exploration

```python
# equilibrium() returns xarray Dataset
result = equilibrium(db, comps, phases, conditions)

# Access as xarray
print(result)  # Shows all variables and dimensions

# Select specific conditions
subset = result.sel(T=1200, method='nearest')

# Slice along dimension
temps = result.sel(T=slice(1000, 1500))

# Boolean indexing
liquid_only = result.where(result.Phase == 'LIQUID', drop=True)

# Convert to pandas for further analysis
df = result.to_dataframe()
```

### Mapping API for Advanced Plotting

```python
from pycalphad.mapping.strategy import Strategy

# For complex visualizations beyond binplot
# Allows custom mapping strategies for phase diagram rendering
```

## Best Practices

### 1. Database Management
```python
# Always include vacancy for gas/liquid phases
comps = ['FE', 'C', 'VA']  # Include VA

# Check available phases in database
print(db.phases.keys())

# Verify component names match database
print(db.elements)
```

### 2. Convergence and Performance
```python
# Use pdens parameter to control calculation density
# Lower pdens = faster but less accurate phase boundaries
result = equilibrium(db, comps, phases, conditions, pdens=500)

# For production: pdens=2000 or higher
# For testing: pdens=100-500
```

### 3. Error Handling
```python
try:
    result = equilibrium(db, comps, phases, conditions)
except Exception as e:
    print(f"Calculation failed: {e}")
    # Common issues:
    # - Missing phases in database
    # - Invalid composition ranges
    # - Temperature out of database range
```

### 4. Validation
```python
# Always validate results
# Check for physical reasonableness
assert np.all(result.NP >= 0), "Negative phase fractions!"
assert np.allclose(result.NP.sum(), 1.0), "Phase fractions don't sum to 1!"

# Verify composition conservation
total_x = (result.NP * result.X).sum(dim='Phase')
# Should equal input composition
```

### 5. Visualization
```python
# Use binplot for quick binary diagrams
# For custom plots, extract from equilibrium dataset

# Example: Custom property plot
import matplotlib.pyplot as plt

temps = np.linspace(300, 1500, 100)
results = equilibrium(db, comps, phases, {v.T: temps, v.P: 101325, v.X('AL'): 0.5})

plt.plot(temps, results.GM.values.squeeze())
plt.xlabel('Temperature (K)')
plt.ylabel('Gibbs Energy (J/mol)')
plt.show()
```

## Common Pitfalls

**Missing vacancy component:**
```python
# Wrong - will fail for phases with vacancy
comps = ['FE', 'C']

# Correct
comps = ['FE', 'C', 'VA']
```

**Incorrect phase names:**
```python
# Must match exact names in database (case-sensitive)
# Check database first
print(list(db.phases.keys()))

# Use exact names
phases = ['FCC_A1']  # not 'FCC' or 'fcc_a1'
```

**Composition not summing to 1:**
```python
# For multicomponent, remember dependent component
# If specifying X(A) and X(B), X(C) is determined automatically

# For 3 components A, B, C
# Specify only n-1 compositions
{v.X('A'): 0.3, v.X('B'): 0.5}  # X(C) = 0.2 automatically
```

**Temperature/pressure units:**
```python
# pycalphad uses SI units
# Temperature: Kelvin (not Celsius)
# Pressure: Pascals (not atm)

{v.T: 1273}  # 1273 K (1000°C)
{v.P: 101325}  # 101325 Pa (1 atm)
```

**Ignoring metastability:**
```python
# equilibrium() finds global minimum
# May miss metastable states

# To study metastability:
# 1. Exclude certain phases
# 2. Compare energies
# 3. Calculate driving forces
```

## Debugging Tips

### Check calculation status
```python
# Verify output
print(result)
print(result.dims)
print(result.coords)

# Check for failed points (NaN values)
print(result.GM.isnull().sum())
```

### Visualization for debugging
```python
# Quick phase stability check
print(result.Phase.values)
print(result.NP.values)

# Plot Gibbs energy curves
from pycalphad import calculate

# Calculate all phases (not equilibrium)
calc_result = calculate(db, comps, phases,
                       {v.T: 1000, v.P: 101325, v.X('AL'): (0, 1, 0.01)})

# Plot GM for each phase
for phase in phases:
    phase_data = calc_result.where(calc_result.Phase == phase, drop=True)
    plt.plot(phase_data.X.sel(component='AL'), phase_data.GM, label=phase)
plt.legend()
plt.show()
```

### Database troubleshooting
```python
# Inspect database content
print("Elements:", db.elements)
print("Species:", db.species)
print("Phases:", list(db.phases.keys()))

# Check specific phase constituents
phase = db.phases['FCC_A1']
print("Sublattices:", phase.constituents)
```

## Units and Conventions

- **Energy**: J/mol or J/mol-atom (check database formulation)
- **Temperature**: Kelvin (K)
- **Pressure**: Pascal (Pa), 101325 Pa = 1 atm
- **Composition**: Mole fractions (0 to 1)
- **Reference states**: Defined in TDB file (typically pure elements at 298.15 K)

## Resources and Citation

**Documentation:**
- Official docs: https://pycalphad.org/docs/latest/
- GitHub: https://github.com/pycalphad/pycalphad
- Examples: https://pycalphad.org/docs/latest/examples/

**Community:**
- Google Group: For installation and usage questions
- GitHub Issues: For bugs and technical problems
- Gitter chat: Real-time discussion

**Citation:**
When publishing work using pycalphad, cite:
> Otis, R. & Liu, Z.-K., (2017). pycalphad: CALPHAD-based Computational Thermodynamics in Python. Journal of Open Research Software. 5(1), p.1. DOI: http://doi.org/10.5334/jors.140

**Databases:**
- Free databases: NIST, GT databases (check licenses)
- Commercial: Thermo-Calc, CompuTherm (require licenses)
- Community databases: Thermochimica, FREED

## Workflow Summary

1. **Load database**: `db = Database('file.tdb')`
2. **Define system**: components, phases
3. **Set conditions**: Temperature, pressure, composition
4. **Calculate**: `equilibrium()` or `calculate()`
5. **Extract results**: Access xarray dataset variables
6. **Visualize**: Use matplotlib or pycalphad plotting tools
7. **Validate**: Check physical constraints and conservation laws
8. **Iterate**: Refine conditions, add/remove phases as needed

## Related Skills

- **python-ase**: For atomistic simulations and DFT calculations
- **materials-databases**: For accessing experimental materials data
- **materials-properties**: For first-principles property calculations
- **python-optimization**: For custom thermodynamic optimization
- **python-plotting**: For advanced visualization of results

---

### pycse

**Path**: `skills/python/pycse/SKILL.md`

# pycse - Python Computations in Science and Engineering

## Overview

**pycse** extends numpy/scipy with convenience functions that automatically return confidence intervals for regression, making statistical analysis faster and less error-prone. Instead of manually extracting covariance matrices and calculating confidence intervals, pycse returns them directly.

**Core value:** Turn 100+ lines of scipy boilerplate into 10 lines of clear, reusable code.

## When to Use

Use pycse when:
- Fitting models to experimental data and need parameter confidence intervals
- Performing regression analysis (linear, nonlinear, polynomial)
- Comparing models with statistical criteria (BIC, R²)
- Generating predictions with error bounds
- Caching expensive computational results
- Reading data from Google Sheets into pandas
- Solving ODEs (wraps scipy with convenient interface)

**Don't use when:**
- scipy alone meets your needs (both are valid)
- You need custom optimization beyond least squares
- Working with models pycse doesn't support

## Quick Reference

| Task | pycse Function | Returns |
|------|---------------|---------|
| Linear regression | `regress(A, y, alpha=0.05)` | `p, pint, se` |
| Nonlinear regression | `nlinfit(model, x, y, p0, alpha=0.05)` | `p, pint, se` |
| Polynomial fit | `polyfit(x, y, deg, alpha=0.05)` | `p, pint, se` |
| Prediction intervals | `predict(X, y, pars, XX, alpha=0.05)` | `prediction, intervals` |
| Nonlinear predict | `nlpredict(X, y, model, loss, popt, xnew)` | `prediction, bounds` |
| Model comparison | `bic(x, y, model, popt)` | `bic_value` |
| Linear BIC | `lbic(X, y, popt)` | `bic_value` |
| R-squared | `Rsquared(y, Y)` | `r2_value` |
| ODE solver | `ivp(f, tspan, y0, **kwargs)` | `solution` |

**All regression functions return:** `(p, pint, se)` where:
- `p` = fitted parameters
- `pint` = confidence intervals for parameters
- `se` = standard errors

## Common Patterns

### Nonlinear Regression with Confidence Intervals

```python
import numpy as np
import pycse

# Data
time = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
concentration = np.array([100, 82, 67, 55, 45, 37, 30, 25, 20, 17, 14])

# Model: C(t) = C0 * exp(-k * t)
def model(t, C0, k):
    return C0 * np.exp(-k * t)

# Fit with 95% confidence intervals
p, pint, se = pycse.nlinfit(model, time, concentration, [100, 0.1])

print(f"C0 = {p[0]:.2f} ± {pint[0,1] - p[0]:.2f}")
print(f"k = {p[1]:.4f} ± {pint[1,1] - p[1]:.4f}")

# That's it! No manual covariance extraction or t-distribution calculations.
```

**Compare to scipy:** Would require extracting covariance, calculating standard errors, looking up t-distribution, computing intervals manually (~50+ lines).

### Linear Regression

```python
import numpy as np
import pycse

# Data matrix A and observations y
A = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])  # [intercept, x]
y = np.array([3, 5, 7, 9])

# Fit: y = p[0] + p[1]*x
p, pint, se = pycse.regress(A, y)

print(f"Intercept: {p[0]:.2f}, 95% CI: [{pint[0,0]:.2f}, {pint[0,1]:.2f}]")
print(f"Slope: {p[1]:.2f}, 95% CI: [{pint[1,0]:.2f}, {pint[1,1]:.2f}]")
```

### Polynomial Fitting

```python
import numpy as np
import pycse

x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([1.5, 3.8, 8.2, 14.9, 23.5, 34.8, 48.2, 64.1])

# Fit quadratic: y = p[0] + p[1]*x + p[2]*x^2
p, pint, se = pycse.polyfit(x, y, deg=2)

print(f"Coefficients: {p}")
print(f"95% CI: {pint}")
```

### Prediction with Error Bounds

```python
import numpy as np
import pycse

# After fitting (see above examples)
x_new = np.array([11, 12, 13])

# Linear prediction
X_new = np.column_stack([np.ones(len(x_new)), x_new])
y_pred, intervals = pycse.predict(A, y, p, X_new)

print(f"Predictions: {y_pred}")
print(f"95% intervals: {intervals}")

# Nonlinear prediction
y_pred_nl, bounds = pycse.nlpredict(time, concentration, model,
                                     lambda p: np.sum((concentration - model(time, *p))**2),
                                     p, x_new)
```

### Model Comparison

```python
import pycse

# Fit two models
p1, _, _ = pycse.polyfit(x, y, deg=1)  # Linear
p2, _, _ = pycse.polyfit(x, y, deg=2)  # Quadratic

# Compare with BIC (lower is better)
bic1 = pycse.lbic(X1, y, p1)
bic2 = pycse.lbic(X2, y, p2)

print(f"Linear BIC: {bic1:.2f}")
print(f"Quadratic BIC: {bic2:.2f}")
print(f"Better model: {'Quadratic' if bic2 < bic1 else 'Linear'}")

# R-squared for goodness of fit
r2 = pycse.Rsquared(y, model(x, *p))
print(f"R² = {r2:.4f}")
```

## Unique Features

### Persistent Hash-based Caching

Cache expensive computations to disk - especially valuable for molecular dynamics, DFT calculations, or long-running simulations.

```python
from pycse.hashcache import HashCache, JsonCache, SqlCache

# Decorator approach
@HashCache()
def expensive_simulation(param1, param2):
    # Long-running computation
    result = complex_calculation(param1, param2)
    return result

# First call: runs computation and caches
result1 = expensive_simulation(1.0, 2.0)

# Second call with same args: retrieves from cache (instant)
result2 = expensive_simulation(1.0, 2.0)

# SqlCache supports searching cached results
@SqlCache(name='my_sim_cache')
def simulation(x, y):
    return complex_calc(x, y)

# Search cache
cache = SqlCache(name='my_sim_cache')
results = cache.search({'x': 1.0})  # Find all cached results where x=1.0
```

**Cache types:**
- `HashCache`: Pickle-based (fastest)
- `JsonCache`: JSON format (human-readable, maggma-compatible)
- `SqlCache`: SQLite with search() capability

### Google Sheets Integration

```python
from pycse.utils import read_gsheet

# Read Google Sheet directly into pandas DataFrame
url = "https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit"
df = pycse.utils.read_gsheet(url)

# Now use with pycse functions
x = df['time'].values
y = df['concentration'].values
p, pint, se = pycse.nlinfit(model, x, y, p0)
```

### Fuzzy Comparisons

For floating-point comparisons with tolerance:

```python
from pycse.utils import feq, fgt, flt, fge, fle

# Check if value is "close enough" to target
if pycse.utils.feq(calculated_pi, np.pi, epsilon=1e-6):
    print("Converged!")

# Fuzzy comparisons
if pycse.utils.fgt(value, threshold, epsilon=1e-8):
    print("Value exceeds threshold (within tolerance)")
```

## Installation

```bash
pip install pycse
```

**Requirements:** Python 3.6+, numpy, scipy

## Common Mistakes

**❌ Forgetting initial guess for nonlinear fit:**
```python
# Will fail - nlinfit needs initial parameter guess
p, pint, se = pycse.nlinfit(model, x, y)  # Missing p0!
```

**✅ Correct:**
```python
p, pint, se = pycse.nlinfit(model, x, y, p0=[100, 0.1])
```

**❌ Wrong shape for regress():**
```python
# regress expects A to be 2D with shape (n_observations, n_parameters)
A = x  # 1D array - wrong!
p, pint, se = pycse.regress(A, y)
```

**✅ Correct:**
```python
# Add column for intercept
A = np.column_stack([np.ones(len(x)), x])  # Shape: (n, 2)
p, pint, se = pycse.regress(A, y)
```

## When pycse vs scipy

**Use pycse when:**
- You need confidence intervals (pycse returns them automatically)
- Doing many regressions in a workflow (consistent interface)
- Want prediction intervals with error bounds
- Need caching for expensive computations
- Integrating with Google Sheets

**Use scipy when:**
- You need custom optimization methods
- Doing complex constrained optimization
- Need features pycse doesn't expose
- Building low-level computational tools

**Both are valid!** pycse wraps scipy for convenience, not replacement.

## Additional Resources

- GitHub: https://github.com/jkitchin/pycse
- Documentation: https://kitchingroup.cheme.cmu.edu/pycse/
- Examples: 400+ pages in the pycse book covering scientific computing topics

---

### pymatgen

**Path**: `skills/programming/pymatgen/SKILL.md`

# Pymatgen Materials Analysis

Systematic guidance for using pymatgen to analyze crystal structures, access materials databases,
and interface with computational chemistry codes.

## Pymatgen Workflow

### 1. Identify Task Type

**What do you want to do?**

**Structure Creation/Loading:**
- Create structure from scratch → `references/core-objects.md`
- Read from file (CIF, POSCAR, XYZ) → `references/file-io.md`
- Get from Materials Project → `references/materials-project.md`
- Generate from symmetry → `references/structure-analysis.md`

**Structure Analysis:**
- Analyze symmetry/space groups → `references/structure-analysis.md`
- Compare structures → `references/structure-analysis.md`
- Calculate properties → `references/properties.md`
- Visualize structures → `references/visualization.md`

**Structure Manipulation:**
- Create supercells/slabs → `references/transformations.md`
- Substitute atoms → `references/transformations.md`
- Apply symmetry operations → `references/transformations.md`
- Interpolate for NEB → `references/transformations.md`

**Materials Database:**
- Query Materials Project → `references/materials-project.md`
- Retrieve structures/properties → `references/materials-project.md`
- Build phase diagrams → `references/phase-diagrams.md`

**Electronic Structure:**
- Parse band structures → `references/electronic-structure.md`
- Plot DOS → `references/electronic-structure.md`
- Analyze VASP outputs → `references/vasp-integration.md`

**Input Generation:**
- Create VASP inputs → `references/vasp-integration.md`
- Generate Gaussian inputs → `references/file-io.md`
- Set up calculations → `references/vasp-integration.md`

### 2. Core Pymatgen Workflow

**Basic pattern:**
```python
# 1. Import modules
from pymatgen.core import Structure, Lattice, Element

# 2. Load or create structure
structure = Structure.from_file("POSCAR")

# 3. Analyze or manipulate
print(f"Formula: {structure.composition.reduced_formula}")
print(f"Space group: {structure.get_space_group_info()}")

# 4. Transform if needed
supercell = structure * (2, 2, 1)  # 2x2x1 supercell

# 5. Write output
supercell.to(filename="POSCAR_supercell")
```

## Quick Reference - Common Tasks

**Load structure:** `Structure.from_file("file.cif")` - Auto-detects format
**Create structure:** See `references/core-objects.md` for Element, Lattice, Structure creation
**Analyze symmetry:** `SpacegroupAnalyzer(struct).get_space_group_symbol()`
**Make supercell:** `structure * (2, 2, 1)` or use transformations
**Query MP:** `MPRester(key).get_structure_by_material_id("mp-149")`
**Generate VASP:** `MPRelaxSet(struct).write_input("dir")`
**Plot bands/DOS:** See `references/electronic-structure.md`

Detailed examples for all tasks in reference files.

## Task Routing

### Core Objects and Creation

**Route to:** `references/core-objects.md`

**When:**
- Creating structures from scratch
- Understanding Element, Site, Structure classes
- Working with Lattice objects
- Creating molecules
- Composition analysis

**Key classes:**
- Element, Species
- Lattice
- Site, PeriodicSite
- Structure, Molecule
- Composition

### File Input/Output

**Route to:** `references/file-io.md`

**When:**
- Reading CIF, POSCAR, XYZ files
- Writing structures to files
- Format conversion
- Parsing calculation outputs
- Working with multiple formats

**Supported formats:**
- CIF (Crystallographic Information File)
- POSCAR/CONTCAR (VASP)
- XYZ (molecular coordinates)
- JSON (serialization)
- Many computational chemistry codes

### Structure Analysis

**Route to:** `references/structure-analysis.md`

**When:**
- Finding space groups
- Symmetry operations
- Comparing structures
- Getting primitive/conventional cells
- Neighbor analysis

**Key tools:**
- SpacegroupAnalyzer
- StructureMatcher
- VoronoiAnalysis
- Distance calculations

### Structure Transformations

**Route to:** `references/transformations.md`

**When:**
- Creating supercells
- Making slabs/surfaces
- Substituting elements
- Perturbing structures
- High-throughput workflows

**Key modules:**
- pymatgen.transformations.standard_transformations
- pymatgen.transformations.advanced_transformations
- pymatgen.alchemy

### Materials Project API

**Route to:** `references/materials-project.md`

**When:**
- Querying materials database
- Getting structures by formula
- Retrieving calculated properties
- Accessing experimental data
- Building chemical systems

**Key class:**
- MPRester for API access
- Requires API key from materialsproject.org

### Phase Diagrams

**Route to:** `references/phase-diagrams.md`

**When:**
- Constructing phase diagrams
- Analyzing stability
- Finding decomposition products
- Pourbaix diagrams
- Grand potential diagrams

**Key classes:**
- PhaseDiagram
- PhaseDiagramError (for stability analysis)
- PourbaixDiagram

### Electronic Structure

**Route to:** `references/electronic-structure.md`

**When:**
- Analyzing band structures
- Plotting DOS
- Finding band gaps
- Analyzing orbital contributions
- Electronic property calculations

**Key modules:**
- pymatgen.electronic_structure.bandstructure
- pymatgen.electronic_structure.dos
- pymatgen.electronic_structure.plotter

### VASP Integration

**Route to:** `references/vasp-integration.md`

**When:**
- Generating VASP inputs
- Parsing VASP outputs
- Creating input sets
- High-throughput VASP
- Custom INCAR settings

**Key classes:**
- MPRelaxSet, MPStaticSet, etc.
- Vasprun, Outcar parsers
- Poscar, Incar classes

## Common Patterns

**Pattern 1: Structure Analysis**
- Load → Analyze composition/symmetry → Get primitive/conventional
- See `examples/structure_analysis.py` for complete workflow

**Pattern 2: Materials Project to Calculation**
- Query MP → Get structure → Generate VASP inputs → Customize settings
- See `examples/mp_to_vasp.py`

**Pattern 3: High-Throughput Substitution**
- Load base structure → Apply transformations → Write outputs
- See `examples/substitution_study.py`

**Pattern 4: Electronic Structure Analysis**
- Parse vasprun.xml → Extract band structure/DOS → Plot and analyze
- See `examples/band_structure.py`

All patterns detailed in `examples/` directory with complete code.

## Installation and Setup

### Install Pymatgen

```bash
pip install pymatgen
# Or with optional dependencies
pip install pymatgen[all]
```

### Configure Materials Project API

```bash
# Register at materialsproject.org to get API key
pmg config --add PMG_MAPI_KEY your_api_key_here
```

### Set VASP Pseudopotential Path

```bash
pmg config --add PMG_VASP_PSP_DIR /path/to/vasp/potentials
```

### Configuration File

Located at `~/.pmgrc.yaml`:
```yaml
PMG_MAPI_KEY: your_api_key_here
PMG_VASP_PSP_DIR: /path/to/vasp/potentials
PMG_DEFAULT_FUNCTIONAL: PBE
```

## Common Issues and Solutions

### Issue: Structure not defined

**Problem:** `NameError: name 'Structure' is not defined`

**Solution:**
```python
from pymatgen.core import Structure
```

### Issue: API key not working

**Problem:** MPRester returns authentication error

**Solution:**
1. Get API key from materialsproject.org (free account)
2. Configure: `pmg config --add PMG_MAPI_KEY your_key`
3. Or pass directly: `MPRester("your_key")`

### Issue: POTCAR generation fails

**Problem:** Cannot write POTCAR files

**Solution:**
1. Set PSP directory: `pmg config --add PMG_VASP_PSP_DIR /path`
2. Ensure VASP pseudopotentials are properly installed
3. Check directory structure matches expected format

### Issue: Import errors for optional dependencies

**Problem:** `ImportError` for plotting or specialized modules

**Solution:**
```bash
pip install pymatgen[all]  # Install all optional dependencies
# Or specific ones:
pip install matplotlib  # For plotting
pip install scipy      # For analysis tools
```

## Best Practices

### Object-Oriented Approach

- Use Structure/Molecule objects, not raw coordinates
- Leverage built-in methods (composition, symmetry, etc.)
- Chain operations for clarity

### Serialization

- Use `as_dict()` / `from_dict()` for persistence
- Prefer JSON over pickle for code evolution
- Use MontyEncoder/MontyDecoder for complex objects

### Transformations

- Use Transformation classes for reproducibility
- TransformedStructure tracks history
- Alchemy framework for high-throughput

### Input Sets

- Use MPRelaxSet, MPStaticSet for standard calculations
- Customize by modifying Incar after creation
- Use InputGenerator for custom workflows

### Materials Project

- Use context manager: `with MPRester(...) as mpr:`
- Batch queries when possible
- Cache results to avoid repeated API calls

## Examples Directory

See `examples/` for complete workflows:
- `structure_analysis.py` - Comprehensive structure analysis
- `mp_query.py` - Materials Project queries
- `phase_diagram.py` - Phase diagram construction
- `band_structure.py` - Electronic structure analysis
- `vasp_workflow.py` - VASP calculation setup
- `substitution_study.py` - High-throughput substitutions

## Reference Documentation

- `references/core-objects.md` - Element, Structure, Lattice, Composition
- `references/file-io.md` - Reading/writing all file formats
- `references/structure-analysis.md` - Symmetry, comparison, neighbors
- `references/transformations.md` - Supercells, substitutions, perturbations
- `references/materials-project.md` - API usage and queries
- `references/phase-diagrams.md` - Phase diagram construction
- `references/electronic-structure.md` - Band structures and DOS
- `references/vasp-integration.md` - VASP input/output handling
- `references/properties.md` - Calculated properties
- `references/visualization.md` - Structure visualization

## External Resources

- **Official docs:** https://pymatgen.org/
- **Materials Project:** https://materialsproject.org/
- **GitHub:** https://github.com/materialsproject/pymatgen
- **Forum:** https://matsci.org/pymatgen

---

### python-ase

**Path**: `skills/programming/python-ase/SKILL.md`

# Python ASE (Atomic Simulation Environment) Skill

This skill provides expert guidance for working with the Atomic Simulation Environment (ASE), a Python library for setting up, manipulating, running, visualizing, and analyzing atomistic simulations.

## When to Use This Skill

Use this skill when:
- Building atomic structures (molecules, surfaces, bulk crystals, nanoparticles)
- Setting up and running DFT calculations (VASP, GPAW, Quantum ESPRESSO, etc.)
- Performing geometry optimizations or molecular dynamics
- Analyzing trajectories and simulation results
- Working with ASE Atoms objects and calculators
- Converting between different structure formats
- Calculating properties (energies, forces, stress, bands, DOS)
- Writing automation scripts for materials simulations

## Core ASE Concepts

### 1. Atoms Objects
The fundamental ASE object representing atomic structures:
- Contains atomic positions, cell parameters, periodic boundary conditions
- Can be modified, visualized, and used with calculators
- Supports constraints for selective dynamics

### 2. Calculators
Interface to quantum chemistry and materials science codes:
- EMT (Effective Medium Theory) - fast, for testing
- VASP - plane-wave DFT
- GPAW - real-space/plane-wave DFT
- Quantum ESPRESSO, CASTEP, NWChem, etc.
- Must be attached to Atoms objects: `atoms.calc = calculator`

### 3. Optimizers
Geometry optimization algorithms:
- BFGS, LBFGS - quasi-Newton methods
- FIRE - fast inertial relaxation
- MDMin - molecular dynamics minimization

### 4. Dynamics
Molecular dynamics simulations:
- VelocityVerlet - NVE ensemble
- Langevin - NVT with friction
- NPT - constant pressure and temperature

## Common Patterns and Workflows

### Building Structures

```python
from ase import Atoms
from ase.build import bulk, molecule, surface, add_adsorbate

# Bulk crystals
atoms = bulk('Cu', 'fcc', a=3.6)
atoms = bulk('Si', 'diamond', a=5.43)

# Molecules
atoms = molecule('H2O')
atoms = molecule('CH4')

# Surfaces
slab = surface('Au', (1, 1, 1), size=(4, 4, 4), vacuum=10.0)

# Add adsorbate to surface
add_adsorbate(slab, 'O', height=1.5, position='fcc')
```

### Setting Up Calculators

```python
# EMT calculator (for testing)
from ase.calculators.emt import EMT
atoms.calc = EMT()

# VASP calculator
from ase.calculators.vasp import Vasp
calc = Vasp(
    xc='PBE',
    encut=400,
    kpts=(4, 4, 4),
    ismear=1,
    sigma=0.1,
    ibrion=2,
    nsw=100,
    ediff=1e-5,
)
atoms.calc = calc

# GPAW calculator
from gpaw import GPAW, PW
calc = GPAW(
    mode=PW(400),
    xc='PBE',
    kpts=(4, 4, 4),
    txt='output.txt'
)
atoms.calc = calc
```

### Geometry Optimization

```python
from ase.optimize import BFGS, LBFGS, FIRE

# Basic optimization
opt = BFGS(atoms, trajectory='opt.traj')
opt.run(fmax=0.05)  # converge until forces < 0.05 eV/Å

# With constraints (fix bottom layers)
from ase.constraints import FixAtoms
mask = [atom.position[2] < 10.0 for atom in atoms]
atoms.set_constraint(FixAtoms(mask=mask))
opt = BFGS(atoms)
opt.run(fmax=0.05)

# Log optimization progress
opt = BFGS(atoms, logfile='opt.log', trajectory='opt.traj')
opt.run(fmax=0.01, steps=100)
```

### Molecular Dynamics

```python
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.langevin import Langevin
from ase import units

# Set initial velocities
MaxwellBoltzmannDistribution(atoms, temperature_K=300)

# NVT dynamics with Langevin thermostat
dyn = Langevin(
    atoms,
    timestep=1.0 * units.fs,
    temperature_K=300,
    friction=0.002
)

# Run with trajectory output
from ase.io.trajectory import Trajectory
traj = Trajectory('md.traj', 'w', atoms)
dyn.attach(traj.write, interval=10)
dyn.run(5000)  # 5000 steps
```

### Analyzing Results

```python
from ase.io import read, write

# Read trajectory
traj = read('opt.traj', ':')  # ':' reads all frames
final = traj[-1]  # last frame

# Extract energies
energies = [atoms.get_potential_energy() for atoms in traj]

# Analyze forces
forces = final.get_forces()
max_force = max(np.linalg.norm(forces, axis=1))

# Calculate distances
from ase.geometry import get_distances
d = atoms.get_distance(0, 1)  # distance between atoms 0 and 1

# Write to various formats
write('structure.cif', atoms)
write('structure.xyz', atoms)
write('POSCAR', atoms)  # VASP format
```

### Band Structure Calculations

```python
from ase.dft.kpoints import bandpath

# Get high-symmetry k-point path
atoms = bulk('Si', 'diamond', a=5.43)
path = atoms.cell.bandpath('GXWLG', npoints=100)

# Calculate bands (example with GPAW)
calc = GPAW(
    mode=PW(400),
    xc='PBE',
    kpts={'path': path, 'density': 10}
)
atoms.calc = calc
atoms.get_potential_energy()  # run calculation

# Plot bands
bs = calc.band_structure()
bs.plot(filename='bands.png', show=True)
```

## Best Practices

### 1. Structure Validation
Always check your structure before running expensive calculations:
```python
# Visualize structure
from ase.visualize import view
view(atoms)  # requires GUI

# Check for overlapping atoms
from ase.geometry import get_distances
distances = get_distances(atoms.positions)[1]
min_distance = distances[distances > 0].min()
print(f"Minimum distance: {min_distance:.3f} Å")

# Verify cell and PBC
print(f"Cell: {atoms.cell}")
print(f"PBC: {atoms.pbc}")
```

### 2. Calculator Management
```python
# Always attach calculator before accessing properties
if atoms.calc is None:
    raise RuntimeError("No calculator attached")

# Reuse calculator for multiple structures
calc = Vasp(xc='PBE', encut=400)
for structure in structures:
    structure.calc = calc
    energy = structure.get_potential_energy()
```

### 3. Error Handling
```python
from ase.calculators.calculator import CalculationFailed

try:
    energy = atoms.get_potential_energy()
except CalculationFailed as e:
    print(f"Calculation failed: {e}")
    # Handle error or retry with different parameters
```

### 4. Performance Tips
```python
# Use trajectory files efficiently
from ase.io import read
# Don't read entire trajectory if you only need specific frames
last_frame = read('md.traj', -1)  # only last frame
every_10th = read('md.traj', '::10')  # every 10th frame

# For VASP: use selective dynamics for large systems
from ase.constraints import FixAtoms
# Fix bulk atoms, only relax surface/adsorbate
```

## Common Workflows

### Workflow 1: Surface Adsorption Energy

```python
from ase.build import fcc111, add_adsorbate
from ase.optimize import BFGS

# 1. Create and optimize clean surface
slab = fcc111('Pt', size=(4, 4, 4), vacuum=10.0)
slab.calc = EMT()
opt = BFGS(slab)
opt.run(fmax=0.05)
E_slab = slab.get_potential_energy()

# 2. Add adsorbate and optimize
add_adsorbate(slab, 'O', height=2.0, position='fcc')
opt = BFGS(slab)
opt.run(fmax=0.05)
E_slab_ads = slab.get_potential_energy()

# 3. Calculate isolated adsorbate energy
from ase import Atoms
adsorbate = Atoms('O', positions=[(0, 0, 0)])
adsorbate.center(vacuum=10.0)
adsorbate.calc = EMT()
E_ads = adsorbate.get_potential_energy()

# 4. Adsorption energy
E_adsorption = E_slab_ads - E_slab - E_ads
print(f"Adsorption energy: {E_adsorption:.3f} eV")
```

### Workflow 2: Lattice Constant Optimization

```python
from ase.build import bulk
from ase.eos import calculate_eos
import numpy as np

atoms = bulk('Cu', 'fcc', a=3.6)
atoms.calc = EMT()

# Method 1: Manual scan
cell_params = np.linspace(3.4, 3.8, 10)
energies = []
for a in cell_params:
    atoms = bulk('Cu', 'fcc', a=a)
    atoms.calc = EMT()
    energies.append(atoms.get_potential_energy())

# Method 2: Using EOS
eos = calculate_eos(atoms, trajectory='eos.traj')
v, e, B = eos.fit()  # volume, energy, bulk modulus
eos.plot('eos.png')
```

### Workflow 3: Nudged Elastic Band (NEB)

```python
from ase.neb import NEB
from ase.optimize import BFGS

# Initial and final states
initial = read('initial.traj')
final = read('final.traj')

# Create intermediate images
images = [initial]
images += [initial.copy() for i in range(5)]  # 5 intermediate images
images += [final]

# Interpolate
neb = NEB(images)
neb.interpolate()

# Set calculator for intermediate images
for image in images[1:-1]:
    image.calc = EMT()

# Optimize
opt = BFGS(neb, trajectory='neb.traj')
opt.run(fmax=0.05)

# Get barrier
energies = [image.get_potential_energy() for image in images]
barrier = max(energies) - energies[0]
print(f"Activation barrier: {barrier:.3f} eV")
```

## Debugging Tips

### Check calculation status
```python
# Did calculation run?
print(atoms.calc.get_property('energy', atoms))

# Check forces
print(atoms.get_forces())

# Verify calculator parameters
print(atoms.calc.parameters)
```

### Visualization debugging
```python
# Quick plot without GUI
from ase.io import write
write('temp.png', atoms, rotation='10x,10y,10z')

# Check trajectory
traj = read('opt.traj', ':')
print(f"Number of steps: {len(traj)}")
for i, atoms in enumerate(traj):
    print(f"Step {i}: E = {atoms.get_potential_energy():.3f} eV")
```

## Important Notes

1. **Units**: ASE uses eV for energy, Å for length, and eV/Å for forces by default
2. **PBC**: Always set periodic boundary conditions correctly: `atoms.pbc = [True, True, False]`
3. **Vacuum**: For surfaces/molecules, add sufficient vacuum (typically 10-15 Å)
4. **k-points**: Increase k-point density for accurate total energies; fewer k-points OK for geometry optimization
5. **Convergence**: Test encut/k-points convergence for production calculations
6. **Constraints**: Remember to fix appropriate atoms (e.g., bottom layers of slabs)

## Resources

When suggesting ASE solutions:
- Reference official ASE documentation for specific modules
- Show complete, runnable examples
- Include appropriate error handling
- Mention calculator-specific requirements (VASP POTCAR files, GPAW setups, etc.)
- Suggest convergence testing for critical parameters
- Recommend visualization checks before expensive calculations

## Example Response Pattern

When helping with ASE:
1. Understand the specific task (structure building, calculation setup, analysis)
2. Provide complete code example with imports
3. Explain key parameters and their typical values
4. Suggest validation/checking steps
5. Mention common pitfalls for that task
6. Recommend convergence tests if applicable

---

### python-best-practices

**Path**: `skills/programming/python-best-practices/SKILL.md`

# Python Best Practices Skill

This skill provides expert guidance for writing professional, maintainable Python code that follows industry best practices and standards.

## When to Use This Skill

Use this skill when:
- Writing new Python functions, classes, or modules
- Refactoring existing Python code for better quality
- Setting up a new Python project with proper structure
- Implementing unit tests or adopting TDD
- Adding type hints for better code clarity
- Configuring linting, formatting, and type checking tools
- Managing dependencies and virtual environments
- Improving code readability and maintainability
- Following PEP 8 style guidelines

## Core Principles

### 1. PEP 8: Style Guide for Python Code

**Key Guidelines:**
- **Indentation**: Use 4 spaces per indentation level (never tabs)
- **Line Length**: Limit lines to 79 characters (99 for code, 72 for docstrings/comments)
- **Blank Lines**: 2 blank lines between top-level functions/classes, 1 within classes
- **Imports**:
  - One import per line
  - Order: standard library, third-party, local (each group separated by blank line)
  - Avoid wildcard imports (`from module import *`)
- **Naming Conventions**:
  - `snake_case` for functions, variables, methods
  - `PascalCase` for classes
  - `UPPER_CASE` for constants
  - Leading underscore `_private` for internal use
- **Whitespace**:
  - No trailing whitespace
  - One space around operators: `x = 1`, not `x=1`
  - No space before function parentheses: `func(x)`, not `func (x)`

**Example:**
```python
"""Module docstring describing purpose."""

import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd

from mypackage.module import MyClass

# Constants
MAX_RETRIES = 3
DEFAULT_TIMEOUT = 30


class DataProcessor:
    """Process and analyze data sets.

    Attributes:
        name: Processor name
        threshold: Minimum value threshold
    """

    def __init__(self, name: str, threshold: float = 0.5):
        """Initialize processor.

        Args:
            name: Name of the processor
            threshold: Threshold value for filtering (default: 0.5)
        """
        self.name = name
        self.threshold = threshold

    def process_data(self, data: list[float]) -> list[float]:
        """Process data by filtering values below threshold.

        Args:
            data: List of numeric values to process

        Returns:
            Filtered list containing only values >= threshold

        Raises:
            ValueError: If data is empty
        """
        if not data:
            raise ValueError("Data cannot be empty")

        return [x for x in data if x >= self.threshold]
```

### 2. Readability and Clarity

**Write Self-Documenting Code:**
```python
# Bad: unclear variable names
def calc(x, y, z):
    return x * y / z

# Good: descriptive names
def calculate_unit_price(total_cost: float, quantity: int, tax_rate: float) -> float:
    """Calculate price per unit including tax."""
    return total_cost * (1 + tax_rate) / quantity
```

**Use Docstrings:**
```python
def fetch_user_data(user_id: int, include_history: bool = False) -> dict:
    """Fetch user data from the database.

    Args:
        user_id: Unique identifier for the user
        include_history: Whether to include transaction history

    Returns:
        Dictionary containing user information with keys:
            - 'name': User's full name
            - 'email': User's email address
            - 'history': List of transactions (if include_history=True)

    Raises:
        UserNotFoundError: If user_id doesn't exist
        DatabaseError: If connection fails

    Example:
        >>> user = fetch_user_data(123, include_history=True)
        >>> print(user['name'])
        'John Doe'
    """
    # Implementation...
```

**Prefer Explicit Over Implicit:**
```python
# Bad: implicit behavior
def process(items):
    return [x for x in items if x]

# Good: explicit intention
def filter_non_empty_items(items: list) -> list:
    """Remove None and empty string values from items."""
    return [item for item in items if item is not None and item != ""]
```

### 3. Modularity and Reusability (DRY Principle)

**Single Responsibility Principle:**
```python
# Bad: function does too much
def process_and_save_report(data):
    # Process data
    cleaned = [x.strip() for x in data]
    filtered = [x for x in cleaned if len(x) > 0]

    # Calculate statistics
    total = sum(len(x) for x in filtered)
    avg = total / len(filtered)

    # Format report
    report = f"Total: {total}, Average: {avg}"

    # Save to file
    with open('report.txt', 'w') as f:
        f.write(report)

    return report

# Good: separate concerns
def clean_data(data: list[str]) -> list[str]:
    """Remove whitespace and empty strings."""
    cleaned = [item.strip() for item in data]
    return [item for item in cleaned if item]

def calculate_statistics(data: list[str]) -> dict:
    """Calculate length statistics for strings."""
    lengths = [len(item) for item in data]
    return {
        'total': sum(lengths),
        'average': sum(lengths) / len(lengths) if lengths else 0,
        'count': len(lengths)
    }

def format_report(stats: dict) -> str:
    """Format statistics as a readable report."""
    return f"Total: {stats['total']}, Average: {stats['average']:.2f}"

def save_report(content: str, filepath: Path) -> None:
    """Save report content to file."""
    filepath.write_text(content)

# Usage
cleaned = clean_data(data)
stats = calculate_statistics(cleaned)
report = format_report(stats)
save_report(report, Path('report.txt'))
```

**Avoid Duplication:**
```python
# Bad: repeated logic
def calculate_circle_area(radius):
    return 3.14159 * radius * radius

def calculate_circle_circumference(radius):
    return 2 * 3.14159 * radius

# Good: reusable constants and functions
import math

def calculate_circle_area(radius: float) -> float:
    """Calculate area of circle."""
    return math.pi * radius ** 2

def calculate_circle_circumference(radius: float) -> float:
    """Calculate circumference of circle."""
    return 2 * math.pi * radius

def calculate_circle_properties(radius: float) -> dict:
    """Calculate all circle properties."""
    return {
        'area': calculate_circle_area(radius),
        'circumference': calculate_circle_circumference(radius)
    }
```

**Use Classes for Related Functionality:**
```python
class DataValidator:
    """Validate data according to defined rules."""

    def __init__(self, min_length: int = 0, max_length: int = 100):
        self.min_length = min_length
        self.max_length = max_length

    def validate_length(self, value: str) -> bool:
        """Check if string length is within bounds."""
        return self.min_length <= len(value) <= self.max_length

    def validate_email(self, email: str) -> bool:
        """Check if email format is valid."""
        return '@' in email and '.' in email.split('@')[1]

    def validate_all(self, data: dict) -> dict[str, bool]:
        """Validate all fields in data dictionary."""
        return {
            'email': self.validate_email(data.get('email', '')),
            'name': self.validate_length(data.get('name', ''))
        }
```

### 4. Testing and TDD

**Write Testable Code:**
```python
# Bad: hard to test (depends on external state)
def get_config_value(key):
    with open('/etc/myapp/config.ini') as f:
        for line in f:
            if line.startswith(key):
                return line.split('=')[1].strip()

# Good: testable with dependency injection
def get_config_value(key: str, config_path: Path) -> str:
    """Get configuration value from file."""
    content = config_path.read_text()
    for line in content.splitlines():
        if line.startswith(key):
            return line.split('=')[1].strip()
    raise KeyError(f"Config key '{key}' not found")
```

**Unit Test Structure:**
```python
import pytest
from mymodule import calculate_unit_price, UserNotFoundError


class TestCalculateUnitPrice:
    """Test suite for calculate_unit_price function."""

    def test_basic_calculation(self):
        """Test basic price calculation without tax."""
        result = calculate_unit_price(100.0, 10, 0.0)
        assert result == 10.0

    def test_with_tax(self):
        """Test price calculation with tax included."""
        result = calculate_unit_price(100.0, 10, 0.2)
        assert result == 12.0

    def test_zero_quantity_raises_error(self):
        """Test that zero quantity raises ValueError."""
        with pytest.raises(ZeroDivisionError):
            calculate_unit_price(100.0, 0, 0.1)

    @pytest.mark.parametrize("total,qty,tax,expected", [
        (100, 10, 0.0, 10.0),
        (100, 10, 0.1, 11.0),
        (50, 5, 0.2, 12.0),
    ])
    def test_multiple_scenarios(self, total, qty, tax, expected):
        """Test multiple calculation scenarios."""
        assert calculate_unit_price(total, qty, tax) == pytest.approx(expected)
```

**TDD Approach:**
```python
# Step 1: Write the test first
def test_parse_csv_line():
    """Test CSV line parsing."""
    result = parse_csv_line('John,Doe,30')
    assert result == {'first': 'John', 'last': 'Doe', 'age': 30}

# Step 2: Implement minimal code to pass
def parse_csv_line(line: str) -> dict:
    """Parse CSV line into dictionary."""
    parts = line.split(',')
    return {
        'first': parts[0],
        'last': parts[1],
        'age': int(parts[2])
    }

# Step 3: Refactor while keeping tests green
def parse_csv_line(line: str, headers: list[str] = None) -> dict:
    """Parse CSV line into dictionary with optional headers."""
    if headers is None:
        headers = ['first', 'last', 'age']

    parts = line.split(',')
    result = {}

    for i, header in enumerate(headers):
        value = parts[i].strip()
        # Convert to int if header is 'age'
        result[header] = int(value) if header == 'age' else value

    return result
```

### 5. Error Handling

**Use Specific Exceptions:**
```python
# Bad: generic exceptions
def divide(a, b):
    if b == 0:
        raise Exception("Can't divide by zero")
    return a / b

# Good: specific exceptions
class DivisionByZeroError(ValueError):
    """Raised when attempting to divide by zero."""
    pass

def divide(a: float, b: float) -> float:
    """Divide two numbers.

    Args:
        a: Numerator
        b: Denominator

    Returns:
        Result of division

    Raises:
        DivisionByZeroError: If denominator is zero
    """
    if b == 0:
        raise DivisionByZeroError(f"Cannot divide {a} by zero")
    return a / b
```

**Proper Exception Handling:**
```python
# Bad: bare except
try:
    result = risky_operation()
except:
    print("Error occurred")

# Good: specific exceptions with context
import logging

logger = logging.getLogger(__name__)

def process_file(filepath: Path) -> dict:
    """Process file and return parsed data."""
    try:
        content = filepath.read_text()
        return parse_content(content)
    except FileNotFoundError:
        logger.error(f"File not found: {filepath}")
        raise
    except PermissionError:
        logger.error(f"Permission denied: {filepath}")
        raise
    except ValueError as e:
        logger.error(f"Invalid content in {filepath}: {e}")
        raise
    except Exception as e:
        logger.exception(f"Unexpected error processing {filepath}")
        raise
```

**Context Managers for Resource Management:**
```python
# Good: automatic cleanup
from pathlib import Path
from contextlib import contextmanager

@contextmanager
def open_database(db_path: Path):
    """Context manager for database connections."""
    conn = connect_to_database(db_path)
    try:
        yield conn
    finally:
        conn.close()

# Usage
with open_database(Path('data.db')) as db:
    results = db.query('SELECT * FROM users')
```

### 6. Virtual Environments and Dependency Management

**Using uv (Modern, Fast Package Manager):**

```bash
# Create new project with uv
uv venv

# Activate virtual environment
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate  # Windows

# Install dependencies
uv pip install pandas numpy pytest

# Install with specific version
uv pip install "requests>=2.28.0,<3.0"

# Install development dependencies
uv pip install --dev pytest black ruff mypy

# Create requirements file
uv pip freeze > requirements.txt

# Install from requirements
uv pip install -r requirements.txt
```

**Project Structure with pyproject.toml:**
```toml
[project]
name = "my-project"
version = "0.1.0"
description = "A well-structured Python project"
authors = [{name = "Your Name", email = "you@example.com"}]
requires-python = ">=3.11"
dependencies = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "requests>=2.28.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.black]
line-length = 99
target-version = ['py311']

[tool.ruff]
line-length = 99
target-version = "py311"
select = ["E", "F", "I", "N", "W", "B", "C4"]

[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
```

### 7. Modern Python Tooling

**Ruff: Fast Linter and Formatter**
```bash
# Install
uv pip install ruff

# Check code
ruff check .

# Auto-fix issues
ruff check --fix .

# Format code
ruff format .

# Configuration in pyproject.toml
[tool.ruff]
line-length = 99
select = [
    "E",   # pycodestyle errors
    "F",   # pyflakes
    "I",   # isort
    "N",   # pep8-naming
    "W",   # pycodestyle warnings
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
]
ignore = ["E501"]  # line too long (handled by formatter)
```

**Black: Code Formatter**
```bash
# Install
uv pip install black

# Format files
black myproject/

# Check without modifying
black --check myproject/

# Configuration
[tool.black]
line-length = 99
target-version = ['py311']
include = '\.pyi?$'
```

**Mypy: Static Type Checker**
```bash
# Install
uv pip install mypy

# Check types
mypy myproject/

# Configuration
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
```

**Type Hints Examples:**
```python
from typing import Protocol, TypeVar, Generic
from collections.abc import Sequence, Callable

# Basic type hints
def greet(name: str) -> str:
    return f"Hello, {name}"

# Collections
def process_items(items: list[int]) -> dict[str, int]:
    return {'total': sum(items), 'count': len(items)}

# Optional values
from typing import Optional

def find_user(user_id: int) -> Optional[dict]:
    """Return user dict or None if not found."""
    # ...

# Union types (Python 3.10+)
def parse_value(value: str | int) -> float:
    return float(value)

# Callable
def apply_function(func: Callable[[int], int], value: int) -> int:
    return func(value)

# Generic types
T = TypeVar('T')

def first_element(items: Sequence[T]) -> T | None:
    return items[0] if items else None

# Protocol (structural subtyping)
class Drawable(Protocol):
    def draw(self) -> None:
        ...

def render(obj: Drawable) -> None:
    obj.draw()
```

## Best Practices Workflow

### When Writing New Code:

1. **Start with type hints and docstrings**
2. **Write tests first (TDD)** - define expected behavior
3. **Implement minimal code** to pass tests
4. **Refactor** while keeping tests green
5. **Run linter** (`ruff check`)
6. **Format code** (`ruff format` or `black`)
7. **Check types** (`mypy`)
8. **Run tests** (`pytest`)

### When Refactoring Existing Code:

1. **Add tests** if they don't exist
2. **Run existing tests** to establish baseline
3. **Refactor incrementally** (small changes)
4. **Run tests after each change**
5. **Improve type coverage**
6. **Apply linter fixes**
7. **Update documentation**

## Common Patterns

### Configuration Management:
```python
from dataclasses import dataclass
from pathlib import Path
import tomllib

@dataclass
class Config:
    """Application configuration."""
    database_url: str
    api_key: str
    timeout: int = 30
    debug: bool = False

def load_config(config_path: Path) -> Config:
    """Load configuration from TOML file."""
    with config_path.open('rb') as f:
        data = tomllib.load(f)
    return Config(**data)
```

### Logging:
```python
import logging
from pathlib import Path

def setup_logging(log_level: str = "INFO", log_file: Path | None = None) -> None:
    """Configure application logging."""
    handlers: list[logging.Handler] = [logging.StreamHandler()]

    if log_file:
        handlers.append(logging.FileHandler(log_file))

    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )

# Usage
logger = logging.getLogger(__name__)
logger.info("Application started")
logger.error("Error occurred", exc_info=True)
```

### CLI with argparse:
```python
import argparse
from pathlib import Path

def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Process data files")
    parser.add_argument('input', type=Path, help="Input file path")
    parser.add_argument('-o', '--output', type=Path, help="Output file path")
    parser.add_argument('-v', '--verbose', action='store_true', help="Verbose output")
    return parser.parse_args()

def main() -> None:
    """Main entry point."""
    args = parse_args()

    if args.verbose:
        setup_logging("DEBUG")

    process_file(args.input, args.output)

if __name__ == '__main__':
    main()
```

## Instructions for Code Reviews

When reviewing or generating Python code, check for:

1. **PEP 8 Compliance**:
   - Correct naming conventions
   - Proper indentation (4 spaces)
   - Appropriate line length
   - Correct import ordering

2. **Type Hints**:
   - All function signatures have type hints
   - Return types are specified
   - Complex types use proper typing constructs

3. **Documentation**:
   - All public functions have docstrings
   - Docstrings include Args, Returns, Raises sections
   - Complex logic has explanatory comments

4. **Error Handling**:
   - Specific exceptions are used
   - Resources are properly cleaned up
   - Error messages are informative

5. **Testing**:
   - Tests exist for new functionality
   - Edge cases are covered
   - Tests are clear and maintainable

6. **Code Quality**:
   - No code duplication
   - Functions have single responsibility
   - Magic numbers are replaced with named constants
   - No overly complex functions (consider cyclomatic complexity)

## Resources and Tools

### Essential Tools:
- **uv**: Fast package installer and resolver
- **Ruff**: Fast Python linter and formatter (Rust-based)
- **Black**: Opinionated code formatter
- **Mypy**: Static type checker
- **Pytest**: Testing framework
- **Pre-commit**: Git hooks for code quality

### Official References:
- PEP 8: https://peps.python.org/pep-0008/
- PEP 257: Docstring Conventions
- Python Type Hints: PEP 484, 585, 604
- Python Enhancement Proposals: https://peps.python.org/

## Limitations

This skill focuses on general Python best practices. For specialized domains:
- **Scientific computing**: Consider numpy/scipy conventions
- **Web development**: Framework-specific patterns (Django, FastAPI)
- **Data science**: Jupyter notebook best practices
- **Async programming**: asyncio patterns and best practices

For these specialized areas, combine this skill with domain-specific skills or documentation.

---

### python-jax

**Path**: `skills/programming/python-jax/SKILL.md`

# JAX - High-Performance Numerical Computing

## Overview

**JAX** is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning. It combines a familiar NumPy-style API with powerful function transformations for automatic differentiation, compilation, vectorization, and parallelization.

**Core value:** Write NumPy-like Python code and automatically get gradients, GPU/TPU acceleration, vectorization, and parallelization through composable function transformations—without changing your mathematical notation.

## When to Use JAX

**Use JAX when:**
- Need automatic differentiation for optimization or machine learning
- Want GPU/TPU acceleration with minimal code changes
- Require high-performance numerical computing
- Building custom gradient-based algorithms
- Need to vectorize or parallelize functions automatically
- Working on research requiring flexible differentiation
- Want functional programming approach to numerical code

**Don't use when:**
- Simple NumPy operations without performance needs (overhead not justified)
- Heavy reliance on in-place mutations (JAX arrays are immutable)
- Imperative, stateful code with side effects
- Need control flow depending on runtime data values (limited support)
- Working with libraries that require NumPy arrays (compatibility issues)

## Core Transformations

JAX provides four fundamental, composable transformations:

### 1. `jax.grad` - Automatic Differentiation

Compute gradients automatically using reverse-mode autodiff.

```python
import jax
import jax.numpy as jnp

def loss(x):
    return jnp.sum(x**2)

# Get gradient function
grad_loss = jax.grad(loss)

x = jnp.array([1.0, 2.0, 3.0])
gradient = grad_loss(x)
print(gradient)  # [2. 4. 6.]
```

**Key features:**
- Returns a function that computes gradients
- Composes for higher-order derivatives
- Works with complex nested structures (pytrees)

### 2. `jax.jit` - Just-In-Time Compilation

Compile functions to XLA for dramatic speedups.

```python
import jax

@jax.jit
def fast_function(x):
    return jnp.sum(x**2 + 3*x + 1)

# First call: compilation + execution (slower)
result = fast_function(jnp.array([1.0, 2.0, 3.0]))

# Subsequent calls: cached compiled version (much faster)
result = fast_function(jnp.array([4.0, 5.0, 6.0]))
```

**Performance:**
- 10-100x speedup typical for numerical functions
- First call slower (compilation overhead)
- Cached for subsequent calls with same shapes/dtypes

### 3. `jax.vmap` - Automatic Vectorization

Vectorize functions across batch dimensions automatically.

```python
import jax

def process_single(x):
    """Process single example"""
    return jnp.sum(x**2)

# Vectorize to process batch
process_batch = jax.vmap(process_single)

# Works on batch automatically
batch = jnp.array([[1, 2], [3, 4], [5, 6]])
results = process_batch(batch)
print(results)  # [5 25 61]
```

**Benefits:**
- Eliminates manual loop writing
- Often faster than explicit loops
- Cleaner, more declarative code

### 4. `jax.pmap` - Parallel Map

Parallelize across multiple devices (GPUs/TPUs).

```python
import jax

@jax.pmap
def parallel_fn(x):
    return x**2

# Runs on all available devices
devices = jax.devices()
x = jnp.arange(len(devices))
results = parallel_fn(x)
```

**Use for:**
- Multi-GPU/TPU computation
- Data parallelism in training
- Large-scale simulations

## Transformation Composition

**JAX transformations compose seamlessly:**

```python
# Gradient of JIT-compiled function
@jax.jit
def fast_loss(x):
    return jnp.sum(x**2)

grad_fast_loss = jax.grad(fast_loss)

# JIT of gradient (equally valid)
fast_grad_loss = jax.jit(jax.grad(fast_loss))

# Vectorized gradient
batch_grad_loss = jax.vmap(jax.grad(fast_loss))

# JIT + vmap + grad
fast_batch_grad = jax.jit(jax.vmap(jax.grad(fast_loss)))
```

**Order matters for performance but not correctness.**

## JAX vs NumPy - Critical Differences

### 1. Immutability

**NumPy (mutable):**
```python
import numpy as np
x = np.array([1, 2, 3])
x[0] = 10  # Modifies x in-place
print(x)  # [10 2 3]
```

**JAX (immutable):**
```python
import jax.numpy as jnp
x = jnp.array([1, 2, 3])
# x[0] = 10  # ERROR: JAX arrays are immutable

# Use functional update instead
x = x.at[0].set(10)  # Returns NEW array
print(x)  # [10 2 3]
```

**Functional updates:**
```python
# Set value
x = x.at[0].set(10)

# Add to value
x = x.at[0].add(5)

# Multiply value
x = x.at[0].mul(2)

# Min/max
x = x.at[0].min(5)
x = x.at[0].max(5)

# Multi-index
x = x.at[0, 1].set(10)
x = x.at[[0, 2]].set(10)
x = x.at[0:3].set(10)
```

### 2. Random Number Generation

**NumPy (global state):**
```python
import numpy as np
np.random.seed(42)
x = np.random.normal(size=3)
y = np.random.normal(size=3)  # Different from x
```

**JAX (explicit keys):**
```python
import jax
key = jax.random.PRNGKey(42)

# Split key for independent randomness
key, subkey = jax.random.split(key)
x = jax.random.normal(subkey, shape=(3,))

key, subkey = jax.random.split(key)
y = jax.random.normal(subkey, shape=(3,))

# Parallel random numbers
keys = jax.random.split(key, num=10)
samples = jax.vmap(lambda k: jax.random.normal(k, shape=(3,)))(keys)
```

**Key management pattern:**
```python
# Create initial key
key = jax.random.PRNGKey(0)

# Always split before using
key, subkey1, subkey2 = jax.random.split(key, 3)

# Use subkeys for random operations
x = jax.random.normal(subkey1, shape=(10,))
y = jax.random.uniform(subkey2, shape=(10,))

# Never reuse keys!
```

### 3. 64-bit Precision

**JAX defaults to 32-bit** for performance.

```python
import jax.numpy as jnp

x = jnp.array([1.0])
print(x.dtype)  # float32 (default)

# Enable 64-bit globally
from jax import config
config.update("jax_enable_x64", True)

x = jnp.array([1.0])
print(x.dtype)  # float64

# Or use environment variable
# JAX_ENABLE_X64=1 python script.py
```

### 4. Out-of-Bounds Indexing

**NumPy (raises error):**
```python
import numpy as np
x = np.array([1, 2, 3])
# x[10]  # IndexError
```

**JAX (clamps silently):**
```python
import jax.numpy as jnp
x = jnp.array([1, 2, 3])
print(x[10])  # 3 (returns last element, no error!)

# Updates also silently ignored
x = x.at[10].set(99)  # No error, no effect
```

**⚠️ Warning:** This is **undefined behavior** - avoid relying on it!

### 5. Non-Array Inputs

**NumPy (accepts lists):**
```python
import numpy as np
result = np.sum([1, 2, 3])  # Works
```

**JAX (requires arrays):**
```python
import jax.numpy as jnp
# result = jnp.sum([1, 2, 3])  # ERROR

# Convert explicitly
result = jnp.sum(jnp.array([1, 2, 3]))  # Works
```

**Reason:** Prevents performance degradation during tracing.

## Sharp Bits & Gotchas

### 1. Pure Functions Required

**❌ Bad: Side effects**
```python
counter = 0

@jax.jit
def impure_fn(x):
    global counter
    counter += 1  # Side effect - UNRELIABLE
    return x**2

# First call: counter=1 (traced)
result = impure_fn(2.0)
# Subsequent calls: counter STILL 1 (uses cached trace)
result = impure_fn(3.0)
```

**✅ Good: Pure function**
```python
@jax.jit
def pure_fn(x, counter):
    return x**2, counter + 1

result, counter = pure_fn(2.0, 0)
result, counter = pure_fn(3.0, counter)
```

### 2. Control Flow Limitations

**❌ Bad: Value-dependent control flow**
```python
@jax.jit
def conditional(x):
    if x > 0:  # ERROR: x is tracer, not concrete value
        return x**2
    else:
        return x**3
```

**✅ Good: Use jax.lax.cond**
```python
@jax.jit
def conditional(x):
    return jax.lax.cond(
        x > 0,
        lambda x: x**2,  # True branch
        lambda x: x**3   # False branch
    )
```

**For loops:**
```python
# ❌ Bad: Python for loop in JIT (unrolled at compile time)
@jax.jit
def loop_bad(x, n):
    for i in range(n):  # n must be static
        x = x + 1
    return x

# ✅ Good: Use jax.lax.fori_loop
@jax.jit
def loop_good(x, n):
    def body(i, val):
        return val + 1
    return jax.lax.fori_loop(0, n, body, x)
```

**While loops:**
```python
@jax.jit
def while_loop(x):
    def cond_fun(val):
        return val < 10

    def body_fun(val):
        return val + 1

    return jax.lax.while_loop(cond_fun, body_fun, x)
```

### 3. Dynamic Shapes

**❌ Bad: Shape depends on runtime values**
```python
@jax.jit
def dynamic_shape(x, mask):
    return x[mask]  # ERROR: output shape unknown at compile time
```

**✅ Good: Use jnp.where for masking**
```python
@jax.jit
def static_shape(x, mask):
    return jnp.where(mask, x, 0)  # Shape stays same
```

### 4. In-Place Update Semantics

**❌ Bad: Relying on shared references**
```python
x = jnp.array([1, 2, 3])
y = x
x = x.at[0].set(10)
print(y[0])  # Still 1 (y unchanged)
```

**Note:** `.at` returns a NEW array; doesn't modify original.

### 5. Print Debugging in JIT

**❌ Bad: print() in JIT context**
```python
@jax.jit
def debug(x):
    print(f"x = {x}")  # Only prints ONCE (during trace)
    return x**2
```

**✅ Good: Use jax.debug.print**
```python
@jax.jit
def debug(x):
    jax.debug.print("x = {}", x)  # Prints every call
    return x**2
```

### 6. Gradient Through Discrete Operations

**❌ Bad: Gradient through argmax**
```python
def loss(x):
    idx = jnp.argmax(x)  # Discrete - no gradient
    return x[idx]

# grad_loss = jax.grad(loss)  # Gradient is zero everywhere
```

**✅ Good: Use differentiable approximations**
```python
def loss(x):
    # Soft argmax (differentiable)
    weights = jax.nn.softmax(x * temperature)
    return jnp.sum(x * weights)

grad_loss = jax.grad(loss)
```

## Automatic Differentiation

### Basic Gradient

```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sum(x**2)

grad_f = jax.grad(f)

x = jnp.array([1.0, 2.0, 3.0])
print(grad_f(x))  # [2. 4. 6.]
```

### Value and Gradient

```python
# Compute both value and gradient efficiently
value_and_grad_f = jax.value_and_grad(f)

value, gradient = value_and_grad_f(x)
print(f"Value: {value}, Gradient: {gradient}")
```

### Multiple Arguments

```python
def f(x, y):
    return jnp.sum(x**2 + y**3)

# Gradient w.r.t. first argument (default)
grad_f = jax.grad(f)
print(grad_f(x, y))

# Gradient w.r.t. second argument
grad_f_wrt_y = jax.grad(f, argnums=1)
print(grad_f_wrt_y(x, y))

# Gradients w.r.t. both arguments
grad_f_both = jax.grad(f, argnums=(0, 1))
grad_x, grad_y = grad_f_both(x, y)
```

### Auxiliary Data

```python
def loss_with_aux(x):
    loss = jnp.sum(x**2)
    aux_data = {'norm': jnp.linalg.norm(x), 'mean': jnp.mean(x)}
    return loss, aux_data

# Tell JAX about auxiliary output
grad_fn = jax.grad(loss_with_aux, has_aux=True)
gradient, aux = grad_fn(x)

print(f"Gradient: {gradient}")
print(f"Auxiliary: {aux}")
```

### Higher-Order Derivatives

```python
# Second derivative (Hessian diagonal)
def f(x):
    return jnp.sum(x**3)

grad_f = jax.grad(f)
hess_diag_f = jax.grad(lambda x: jnp.sum(grad_f(x) * x))

# Full Hessian
hessian_f = jax.hessian(f)

x = jnp.array([1.0, 2.0, 3.0])
print(hessian_f(x))
```

### Jacobian

```python
def vector_fn(x):
    """Vector to vector function"""
    return jnp.array([x[0]**2, x[1]**3, x[0]*x[1]])

# Forward-mode (efficient for few inputs, many outputs)
jacfwd = jax.jacfwd(vector_fn)

# Reverse-mode (efficient for many inputs, few outputs)
jacrev = jax.jacrev(vector_fn)

x = jnp.array([2.0, 3.0])
print(jacfwd(x))
print(jacrev(x))  # Same result
```

### Custom Gradients

```python
@jax.custom_gradient
def f(x):
    # Forward pass
    result = jnp.exp(x)

    # Define custom gradient
    def grad_fn(g):
        # g is the gradient from downstream
        # Return gradient w.r.t. inputs
        return g * 2 * result  # Custom: 2x the normal gradient

    return result, grad_fn

# Use like normal
grad_f = jax.grad(f)
```

## JIT Compilation

### Basic Usage

```python
import jax
import jax.numpy as jnp

def slow_fn(x):
    return jnp.sum(x**2 + 3*x + 1)

fast_fn = jax.jit(slow_fn)

# Or decorator
@jax.jit
def fast_fn2(x):
    return jnp.sum(x**2 + 3*x + 1)
```

### Static Arguments

```python
@jax.jit
def fn(x, n):
    for i in range(n):  # n must be static
        x = x + 1
    return x

# ERROR: n changes each call, triggers recompilation
result = fn(x, 5)
result = fn(x, 10)  # Recompiles!

# Solution: Mark as static
@jax.jit(static_argnums=(1,))
def fn_static(x, n):
    for i in range(n):
        x = x + 1
    return x

# Now works correctly
result = fn_static(x, 5)
result = fn_static(x, 10)  # Separate compilation for n=10
```

### Avoiding Recompilation

```python
# Bad: Different shapes trigger recompilation
@jax.jit
def process(x):
    return jnp.sum(x**2)

x1 = jnp.ones(10)    # Compiles for shape (10,)
x2 = jnp.ones(20)    # Recompiles for shape (20,)
x3 = jnp.ones(10)    # Uses cached version

# Good: Consistent shapes
batch_size = 32
x1 = jnp.ones((batch_size, 10))
x2 = jnp.ones((batch_size, 10))  # Same shape, cached
```

## Vectorization (vmap)

### Basic Batching

```python
def process_single(x):
    """Process single example: scalar input"""
    return x**2 + 3*x

# Manually vectorize
def process_batch_manual(xs):
    return jnp.array([process_single(x) for x in xs])

# Automatic vectorization
process_batch = jax.vmap(process_single)

batch = jnp.array([1.0, 2.0, 3.0, 4.0])
print(process_batch(batch))  # Faster and cleaner
```

### Batching Matrix Operations

```python
def matrix_vector_product(matrix, vector):
    """Single matrix-vector product"""
    return matrix @ vector

# Batch over vectors
batch_mvp = jax.vmap(matrix_vector_product, in_axes=(None, 0))

A = jnp.ones((3, 3))
vectors = jnp.ones((10, 3))  # 10 vectors

results = batch_mvp(A, vectors)  # (10, 3)

# Batch over both
batch_both = jax.vmap(matrix_vector_product, in_axes=(0, 0))

matrices = jnp.ones((10, 3, 3))
results = batch_both(matrices, vectors)
```

### Nested vmap

```python
# Batch over two dimensions
def fn(x, y):
    return x * y

# Batch over first arg, then second
fn_batch = jax.vmap(jax.vmap(fn, in_axes=(None, 0)), in_axes=(0, None))

x = jnp.array([1, 2, 3])       # (3,)
y = jnp.array([10, 20])        # (2,)
result = fn_batch(x, y)        # (3, 2)

# Equivalent to: x[:, None] * y[None, :]
```

## PyTrees - Nested Structures

JAX works with nested Python containers (pytrees):

```python
import jax
import jax.numpy as jnp

# Dictionary of arrays
params = {
    'w1': jnp.ones((10, 5)),
    'b1': jnp.zeros(5),
    'w2': jnp.ones((5, 1)),
    'b2': jnp.zeros(1)
}

# Gradient works on entire pytree
def loss(params, x):
    h = x @ params['w1'] + params['b1']
    h = jax.nn.relu(h)
    out = h @ params['w2'] + params['b2']
    return jnp.mean(out**2)

grad_fn = jax.grad(loss)
x = jnp.ones((32, 10))

# Returns gradients in same structure
grads = grad_fn(params, x)
print(grads.keys())  # dict_keys(['w1', 'b1', 'w2', 'b2'])
```

### PyTree Operations

```python
# Tree map - apply function to all leaves
scaled_params = jax.tree_map(lambda x: x * 0.9, params)

# Tree reduce
total_params = jax.tree_reduce(
    lambda total, x: total + x.size,
    params,
    initializer=0
)

# Flatten and unflatten
flat, treedef = jax.tree_flatten(params)
reconstructed = jax.tree_unflatten(treedef, flat)
```

## Common Patterns

### Optimization Loop

```python
import jax
import jax.numpy as jnp

# Parameters
params = jnp.array([1.0, 2.0, 3.0])

# Loss function
def loss(params, x, y):
    pred = jnp.dot(params, x)
    return jnp.mean((pred - y)**2)

# Gradient function
grad_fn = jax.jit(jax.grad(loss))

# Training data
x_train = jnp.ones((100, 3))
y_train = jnp.ones(100)

# Optimization loop
learning_rate = 0.01
for step in range(1000):
    grads = grad_fn(params, x_train, y_train)
    params = params - learning_rate * grads

    if step % 100 == 0:
        l = loss(params, x_train, y_train)
        print(f"Step {step}, Loss: {l:.4f}")
```

### Mini-batch Training

```python
def train_step(params, batch):
    """Single training step on one batch"""
    x, y = batch

    def batch_loss(params):
        pred = jnp.dot(x, params)
        return jnp.mean((pred - y)**2)

    loss_value, grads = jax.value_and_grad(batch_loss)(params)
    params = params - 0.01 * grads
    return params, loss_value

# JIT compile training step
train_step = jax.jit(train_step)

# Training loop
for epoch in range(10):
    for batch in data_loader:
        params, loss = train_step(params, batch)
```

### Scan for Efficient Loops

```python
def cumulative_sum(xs):
    """Efficient cumulative sum using scan"""
    def step(carry, x):
        new_carry = carry + x
        output = new_carry
        return new_carry, output

    final_carry, outputs = jax.lax.scan(step, 0, xs)
    return outputs

xs = jnp.array([1, 2, 3, 4, 5])
print(cumulative_sum(xs))  # [1 3 6 10 15]
```

### RNN with Scan

```python
def rnn_step(carry, x):
    """Single RNN step"""
    h = carry
    h_new = jnp.tanh(jnp.dot(W_h, h) + jnp.dot(W_x, x))
    return h_new, h_new

def rnn(xs, h0):
    """Run RNN over sequence"""
    final_h, all_h = jax.lax.scan(rnn_step, h0, xs)
    return all_h

# Process sequence
W_h = jnp.ones((5, 5))
W_x = jnp.ones((5, 3))
xs = jnp.ones((10, 3))  # Sequence length 10
h0 = jnp.zeros(5)

outputs = rnn(xs, h0)  # (10, 5)
```

## Performance Best Practices

### 1. JIT Your Critical Paths

```python
# Wrap expensive computations in jit
@jax.jit
def expensive_fn(x):
    for _ in range(100):
        x = jnp.dot(x, x.T)
    return x

# Avoid jitting trivial operations
def trivial(x):
    return x + 1  # JIT overhead not worth it
```

### 2. Use vmap Instead of Loops

```python
# Slow: Python loop
def slow_batch(xs):
    return jnp.array([process(x) for x in xs])

# Fast: vmap
fast_batch = jax.vmap(process)
```

### 3. Minimize Host-Device Transfers

```python
# Bad: Transfer back to host in loop
for i in range(1000):
    x = compute_on_gpu(x)
    print(float(x))  # Transfers to CPU each iteration!

# Good: Transfer once at end
for i in range(1000):
    x = compute_on_gpu(x)
print(float(x))  # Single transfer
```

### 4. Use Appropriate Precision

```python
# Use float32 unless you need float64
x = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)

# Enable float64 only when necessary
from jax import config
config.update("jax_enable_x64", True)
```

### 5. Preallocate When Possible

```python
# Bad: Growing arrays
result = jnp.array([])
for i in range(1000):
    result = jnp.append(result, compute(i))

# Good: Preallocate
result = jnp.zeros(1000)
for i in range(1000):
    result = result.at[i].set(compute(i))

# Better: Use scan or vmap
def compute_all(i):
    return compute(i)

result = jax.vmap(compute_all)(jnp.arange(1000))
```

## Debugging

### Checking Array Values

```python
# Use jax.debug.print in JIT context
@jax.jit
def debug_fn(x):
    jax.debug.print("x = {}", x)
    jax.debug.print("x shape = {}, dtype = {}", x.shape, x.dtype)
    return x**2
```

### Gradient Checking

```python
from jax.test_util import check_grads

def f(x):
    return jnp.sum(x**3)

x = jnp.array([1.0, 2.0, 3.0])

# Verify gradients numerically
check_grads(f, (x,), order=2)  # Check 1st and 2nd derivatives
```

### Inspecting Compiled Code

```python
# See jaxpr (intermediate representation)
def f(x):
    return x**2 + 3*x

jaxpr = jax.make_jaxpr(f)(1.0)
print(jaxpr)

# See HLO (low-level compiled code)
compiled = jax.jit(f).lower(1.0).compile()
print(compiled.as_text())
```

## Common Gotchas - Quick Reference

| Gotcha | NumPy | JAX | Solution |
|--------|-------|-----|----------|
| In-place update | `x[0] = 1` | ❌ Error | `x = x.at[0].set(1)` |
| Random state | `np.random.seed()` | ❌ Not reliable | `key = jax.random.PRNGKey()` |
| List inputs | `np.sum([1,2,3])` | ❌ Error | `jnp.sum(jnp.array([1,2,3]))` |
| Out-of-bounds | IndexError | ⚠️ Silent clamp | Avoid, validate indices |
| Value-dependent if | Works | ❌ In JIT | `jax.lax.cond()` |
| Dynamic shapes | Works | ❌ In JIT | Keep shapes static |
| Default precision | float64 | float32 | Set `jax_enable_x64` |
| Print in JIT | Works | Only once | `jax.debug.print()` |

## Installation

```bash
# CPU only
pip install jax

# GPU (CUDA 12)
pip install jax[cuda12]

# TPU
pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
```

## Ecosystem Libraries

JAX has a rich ecosystem built on its transformation primitives:

**Neural Networks:**
- **Flax** - Official neural network library
- **Haiku** - DeepMind's neural network library
- **Equinox** - Elegant PyTorch-like library

**Optimization:**
- **Optax** - Gradient processing and optimization
- **JAXopt** - Non-linear optimization

**Scientific Computing:**
- **JAX-MD** - Molecular dynamics
- **Diffrax** - Differential equation solvers
- **BlackJAX** - MCMC sampling

**Utilities:**
- **jaxtyping** - Type annotations for arrays
- **chex** - Testing utilities

## Additional Resources

- **Official Documentation:** https://docs.jax.dev/
- **JAX GitHub:** https://github.com/google/jax
- **JAX Ecosystem:** https://github.com/google/jax#neural-network-libraries
- **JAX Tutorial (DeepMind):** https://github.com/deepmind/jax-tutorial
- **Awesome JAX:** https://github.com/n2cholas/awesome-jax

## Related Skills

- `python-optimization` - Numerical optimization with scipy, pyomo
- `python-ase` - Atomic simulation environment (can use JAX for forces)
- `pycse` - Scientific computing utilities
- `python-best-practices` - Code quality for JAX projects

---

### python-multiobjective-optimization

**Path**: `skills/programming/python-multiobjective-optimization/SKILL.md`

# Python Multiobjective Optimization

## Overview

Master multiobjective optimization where you must simultaneously optimize multiple conflicting objectives. Unlike single-objective optimization that finds one optimal solution, multiobjective optimization discovers a **Pareto front** - a set of trade-off solutions where improving one objective worsens another.

**Core value:** Find optimal trade-offs between competing objectives (cost vs. performance, risk vs. return, speed vs. accuracy) and enable informed decision-making across the Pareto frontier.

## When to Use

Use multiobjective optimization when:
- Optimizing multiple conflicting objectives simultaneously
- Need trade-off analysis between competing goals
- Decision-makers must choose from Pareto-optimal alternatives
- No single "best" solution exists without preference information

**Examples:**
- Portfolio optimization: maximize return, minimize risk
- Engineering design: minimize cost, maximize performance, minimize weight
- Manufacturing: maximize throughput, minimize defects, minimize energy
- Drug design: maximize efficacy, minimize toxicity, maximize bioavailability
- Vehicle routing: minimize distance, minimize time, minimize vehicles

**Don't use when:**
- Only one objective exists (use single-objective optimization)
- Objectives can be combined into single metric with known weights
- One objective is vastly more important than others

## Key Concepts

### Pareto Dominance

Solution **x** dominates solution **y** if:
1. **x** is no worse than **y** in all objectives
2. **x** is strictly better than **y** in at least one objective

```python
def dominates(obj_x, obj_y, minimize=True):
    """
    Check if obj_x dominates obj_y (for minimization).

    Returns True if obj_x dominates obj_y.
    """
    if minimize:
        # For minimization: x dominates y if all x_i <= y_i and at least one x_i < y_i
        better_or_equal = all(x <= y for x, y in zip(obj_x, obj_y))
        strictly_better = any(x < y for x, y in zip(obj_x, obj_y))
    else:
        # For maximization: x dominates y if all x_i >= y_i and at least one x_i > y_i
        better_or_equal = all(x >= y for x, y in zip(obj_x, obj_y))
        strictly_better = any(x > y for x, y in zip(obj_x, obj_y))

    return better_or_equal and strictly_better
```

### Pareto Front (Pareto Frontier)

The **Pareto front** is the set of all non-dominated solutions - solutions where you cannot improve one objective without worsening another.

**Properties:**
- All points on the Pareto front are equally "optimal" from mathematical perspective
- Choosing among Pareto solutions requires preference information
- Moving along the front trades one objective for another

### Utopia and Nadir Points

- **Utopia point**: Best possible value for each objective (usually infeasible)
- **Nadir point**: Worst value among Pareto-optimal solutions for each objective

## Methods Overview

### Scalarization Approaches

Convert multiobjective problem to series of single-objective problems.

**1. Weighted Sum**
- Combine objectives with weights: minimize w₁f₁(x) + w₂f₂(x)
- Pros: Simple, uses any single-objective solver
- Cons: Cannot find non-convex Pareto fronts, sensitive to scaling

**2. Weighted Metrics (Lp-norm)**
- Minimize ||f(x) - ideal||ₚ with weights
- Pros: Can find non-convex fronts (p=∞)
- Cons: Requires ideal point estimation

**3. ε-Constraint Method**
- Minimize one objective, constrain others: min f₁(x) s.t. f₂(x) ≤ ε
- Pros: Finds entire Pareto front including non-convex regions
- Cons: Many single-objective solves, requires constraint bounds

**4. Goal Programming**
- Minimize deviation from target goals
- Pros: Intuitive when targets known
- Cons: Requires setting goals

### Evolutionary Algorithms

Population-based metaheuristics that evolve Pareto front approximations.

**1. NSGA-II (Non-dominated Sorting Genetic Algorithm II)**
- Most popular multiobjective evolutionary algorithm
- Fast non-dominated sorting + crowding distance
- Pros: Robust, well-tested, handles 2-3 objectives well
- Cons: Degrades with many objectives (>3)

**2. NSGA-III**
- Extension of NSGA-II for many objectives (4+)
- Reference point-based selection
- Pros: Scales to many objectives
- Cons: More complex parameter tuning

**3. MOEA/D (Multiobjective Evolutionary Algorithm based on Decomposition)**
- Decomposes problem into scalar subproblems
- Pros: Efficient, good for many objectives
- Cons: Requires careful decomposition setup

**4. Other Algorithms**
- SPEA2, PAES, GDE3, SMS-EMOA, etc.

### Comparison

| Method | Objectives | Pareto Front Type | Pros | Cons |
|--------|-----------|-------------------|------|------|
| Weighted Sum | 2-3 | Convex only | Simple, fast | Misses non-convex regions |
| ε-Constraint | 2-3 | Any | Complete coverage | Many solves needed |
| NSGA-II | 2-3 | Any | Robust, popular | Poor for many objectives |
| NSGA-III | 4+ | Any | Handles many objectives | Complex tuning |
| MOEA/D | 4+ | Any | Efficient | Decomposition setup |

## Library Selection

### pymoo - Recommended

**Use when:**
- Need modern, comprehensive multiobjective optimization
- Want evolutionary algorithms (NSGA-II, NSGA-III, MOEA/D)
- Need built-in performance indicators
- Want clean, object-oriented API

**Strengths:**
- Actively maintained
- Extensive algorithm library
- Built-in test problems
- Performance indicators (hypervolume, etc.)
- Good documentation

**Installation:**
```bash
pip install pymoo
```

### platypus

**Use when:**
- Need pure Python implementation
- Want simple API
- Lightweight dependency footprint

**Strengths:**
- Pure Python (no compilation)
- Simple interface
- Multiple algorithms

**Installation:**
```bash
pip install platypus-opt
```

### DEAP

**Use when:**
- Need general evolutionary computation framework
- Want customization flexibility
- Already using DEAP for other tasks

**Strengths:**
- Highly flexible
- Large community
- General-purpose evolutionary algorithms

**Cons:**
- More complex API
- Less specialized for multiobjective

**Installation:**
```bash
pip install deap
```

### scipy.optimize

**Use when:**
- Only need scalarization approaches
- Want minimal dependencies
- Already using scipy

**Limitations:**
- No built-in evolutionary algorithms
- Manual Pareto front construction
- Limited multiobjective-specific tools

## Implementation Patterns

### Pattern 1: NSGA-II with pymoo

```python
import numpy as np
from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.core.problem import Problem
from pymoo.optimize import minimize
from pymoo.visualization.scatter import Scatter

# Define problem
class MyProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=2,           # Number of decision variables
            n_obj=2,           # Number of objectives
            n_ieq_constr=0,    # Number of inequality constraints
            xl=np.array([0, 0]),    # Lower bounds
            xu=np.array([1, 1])     # Upper bounds
        )

    def _evaluate(self, x, out, *args, **kwargs):
        """
        Evaluate objectives for population x.

        x: array of shape (pop_size, n_var)
        out["F"]: objectives of shape (pop_size, n_obj)
        out["G"]: constraints of shape (pop_size, n_constr) [optional]
        """
        # Objective 1: minimize (x1 - 0.5)^2 + (x2 - 0.5)^2
        f1 = (x[:, 0] - 0.5)**2 + (x[:, 1] - 0.5)**2

        # Objective 2: minimize (x1 - 0.2)^2 + (x2 - 0.8)^2
        f2 = (x[:, 0] - 0.2)**2 + (x[:, 1] - 0.8)**2

        out["F"] = np.column_stack([f1, f2])

# Create problem instance
problem = MyProblem()

# Configure algorithm
algorithm = NSGA2(
    pop_size=100,           # Population size
    eliminate_duplicates=True
)

# Solve
res = minimize(
    problem,
    algorithm,
    ('n_gen', 200),         # Termination: 200 generations
    seed=1,
    verbose=True
)

# Results
print(f"Number of Pareto solutions: {len(res.F)}")
print(f"Objective values (first 5):\n{res.F[:5]}")
print(f"Decision variables (first 5):\n{res.X[:5]}")

# Visualize Pareto front
plot = Scatter()
plot.add(res.F, label="Pareto Front")
plot.show()
```

### Pattern 2: NSGA-III for Many Objectives

```python
from pymoo.algorithms.moo.nsga3 import NSGA3
from pymoo.util.ref_dirs import get_reference_directions

# Problem with many objectives (e.g., 4)
class ManyObjectiveProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=10,
            n_obj=4,           # 4 objectives
            xl=-5,
            xu=5
        )

    def _evaluate(self, x, out, *args, **kwargs):
        # Define 4 conflicting objectives
        f1 = np.sum(x[:, :5]**2, axis=1)
        f2 = np.sum((x[:, :5] - 1)**2, axis=1)
        f3 = np.sum(x[:, 5:]**2, axis=1)
        f4 = np.sum((x[:, 5:] - 2)**2, axis=1)

        out["F"] = np.column_stack([f1, f2, f3, f4])

# Create reference directions for 4 objectives
ref_dirs = get_reference_directions("das-dennis", 4, n_partitions=12)

# NSGA-III algorithm
algorithm = NSGA3(
    ref_dirs=ref_dirs,
    pop_size=92  # Must match reference directions
)

# Solve
problem = ManyObjectiveProblem()
res = minimize(
    problem,
    algorithm,
    ('n_gen', 300),
    seed=1,
    verbose=True
)

print(f"Number of Pareto solutions: {len(res.F)}")
```

### Pattern 3: With Constraints

```python
class ConstrainedProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=2,
            n_obj=2,
            n_ieq_constr=2,    # 2 inequality constraints
            xl=np.array([0, 0]),
            xu=np.array([5, 5])
        )

    def _evaluate(self, x, out, *args, **kwargs):
        # Objectives
        f1 = x[:, 0]**2 + x[:, 1]**2
        f2 = (x[:, 0] - 3)**2 + (x[:, 1] - 3)**2

        # Inequality constraints: g(x) <= 0
        g1 = x[:, 0] + x[:, 1] - 5      # x1 + x2 <= 5
        g2 = -(x[:, 0] + 2*x[:, 1] - 6)  # x1 + 2*x2 >= 6

        out["F"] = np.column_stack([f1, f2])
        out["G"] = np.column_stack([g1, g2])

# Solve with NSGA-II (handles constraints automatically)
problem = ConstrainedProblem()
algorithm = NSGA2(pop_size=100)
res = minimize(problem, algorithm, ('n_gen', 200), seed=1)

# Check constraint violations
print(f"Max constraint violation: {np.max(res.CV)}")  # Should be ~0
```

### Pattern 4: Weighted Sum (Scalarization)

```python
from scipy.optimize import minimize as scipy_minimize

def objectives(x):
    """Return tuple of objectives"""
    f1 = x[0]**2 + x[1]**2
    f2 = (x[0] - 1)**2 + (x[1] - 1)**2
    return f1, f2

def scalarized_objective(x, weights):
    """Weighted sum of objectives"""
    f1, f2 = objectives(x)
    return weights[0] * f1 + weights[1] * f2

# Generate Pareto front by varying weights
weights_list = np.linspace(0, 1, 11)
pareto_front = []

for w in weights_list:
    weights = [w, 1 - w]

    result = scipy_minimize(
        scalarized_objective,
        x0=[0.5, 0.5],
        args=(weights,),
        method='SLSQP',
        bounds=[(0, 2), (0, 2)]
    )

    if result.success:
        f1, f2 = objectives(result.x)
        pareto_front.append({
            'x': result.x,
            'f1': f1,
            'f2': f2,
            'weights': weights
        })

# Convert to arrays
F = np.array([[p['f1'], p['f2']] for p in pareto_front])
X = np.array([p['x'] for p in pareto_front])

print(f"Found {len(pareto_front)} Pareto solutions")
```

### Pattern 5: ε-Constraint Method

```python
from scipy.optimize import minimize as scipy_minimize

def objective1(x):
    return x[0]**2 + x[1]**2

def objective2(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2

# ε-constraint: minimize f1, subject to f2 <= epsilon
def solve_epsilon_constraint(epsilon):
    """Solve for given epsilon constraint on f2"""

    constraint = {
        'type': 'ineq',
        'fun': lambda x: epsilon - objective2(x)  # f2(x) <= epsilon
    }

    result = scipy_minimize(
        objective1,
        x0=[0.5, 0.5],
        method='SLSQP',
        constraints=[constraint],
        bounds=[(0, 2), (0, 2)]
    )

    return result

# Vary epsilon to trace Pareto front
epsilon_values = np.linspace(0.01, 2.0, 20)
pareto_front = []

for eps in epsilon_values:
    result = solve_epsilon_constraint(eps)

    if result.success:
        pareto_front.append({
            'x': result.x,
            'f1': objective1(result.x),
            'f2': objective2(result.x),
            'epsilon': eps
        })

print(f"Found {len(pareto_front)} Pareto solutions")
```

### Pattern 6: Using platypus

```python
from platypus import NSGAII, Problem, Real

def evaluate(x):
    """
    Evaluation function returning list of objectives.

    Note: platypus expects function that returns list.
    """
    f1 = x[0]**2 + x[1]**2
    f2 = (x[0] - 1)**2 + (x[1] - 1)**2
    return [f1, f2]

# Define problem
problem = Problem(2, 2)  # 2 variables, 2 objectives
problem.types[:] = [Real(0, 1), Real(0, 1)]  # Variable bounds
problem.function = evaluate

# Solve with NSGA-II
algorithm = NSGAII(problem, population_size=100)
algorithm.run(10000)  # 10000 function evaluations

# Extract results
F = np.array([[s.objectives[0], s.objectives[1]] for s in algorithm.result])
X = np.array([[s.variables[0], s.variables[1]] for s in algorithm.result])

print(f"Number of Pareto solutions: {len(F)}")
```

### Pattern 7: Using DEAP

```python
import random
from deap import base, creator, tools, algorithms
import numpy as np

# Setup DEAP
creator.create("FitnessMin", base.Fitness, weights=(-1.0, -1.0))  # Minimize both
creator.create("Individual", list, fitness=creator.FitnessMin)

toolbox = base.Toolbox()
toolbox.register("attr_float", random.uniform, 0, 1)
toolbox.register("individual", tools.initRepeat, creator.Individual,
                 toolbox.attr_float, n=2)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

def evaluate(individual):
    """Evaluation function returning tuple of objectives"""
    x = individual
    f1 = x[0]**2 + x[1]**2
    f2 = (x[0] - 1)**2 + (x[1] - 1)**2
    return f1, f2

toolbox.register("evaluate", evaluate)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)
toolbox.register("select", tools.selNSGA2)

# Initialize population
pop = toolbox.population(n=100)

# Run NSGA-II
algorithms.eaMuPlusLambda(
    pop, toolbox,
    mu=100,
    lambda_=100,
    cxpb=0.9,
    mutpb=0.1,
    ngen=100,
    verbose=False
)

# Extract Pareto front
pareto_front = tools.sortNondominated(pop, len(pop), first_front_only=True)[0]
F = np.array([ind.fitness.values for ind in pareto_front])
X = np.array([list(ind) for ind in pareto_front])

print(f"Number of Pareto solutions: {len(F)}")
```

## Pareto Analysis

### Non-dominated Sorting

```python
def is_dominated(obj_a, obj_b, minimize=True):
    """Check if obj_a is dominated by obj_b"""
    if minimize:
        better = all(b <= a for a, b in zip(obj_a, obj_b))
        strictly_better = any(b < a for a, b in zip(obj_a, obj_b))
    else:
        better = all(b >= a for a, b in zip(obj_a, obj_b))
        strictly_better = any(b > a for a, b in zip(obj_a, obj_b))
    return better and strictly_better

def find_pareto_front(F, minimize=True):
    """
    Find Pareto front from objective values.

    F: array of shape (n_solutions, n_objectives)
    Returns: indices of non-dominated solutions
    """
    n = len(F)
    pareto_indices = []

    for i in range(n):
        dominated = False
        for j in range(n):
            if i != j and is_dominated(F[i], F[j], minimize):
                dominated = True
                break
        if not dominated:
            pareto_indices.append(i)

    return np.array(pareto_indices)

# Example usage
F = np.random.rand(100, 2)  # 100 solutions, 2 objectives
pareto_idx = find_pareto_front(F, minimize=True)
pareto_F = F[pareto_idx]

print(f"Pareto front contains {len(pareto_F)} solutions out of {len(F)}")
```

### Hypervolume Indicator

Measures quality of Pareto front approximation.

```python
from pymoo.indicators.hv import HV

# Hypervolume indicator
ref_point = np.array([1.1, 1.1])  # Reference point (should dominate all solutions)
ind = HV(ref_point=ref_point)

# Calculate hypervolume
hypervolume = ind(res.F)
print(f"Hypervolume: {hypervolume:.4f}")

# Compare two Pareto fronts
hv1 = ind(pareto_front_1)
hv2 = ind(pareto_front_2)
print(f"Front 1 is better: {hv1 > hv2}")
```

### Generational Distance (GD)

Measures convergence to true Pareto front.

```python
from pymoo.indicators.gd import GD

# Requires true Pareto front (if known)
true_pf = np.array([...])  # True Pareto front

ind = GD(true_pf)
gd = ind(res.F)
print(f"Generational Distance: {gd:.4f}")  # Lower is better
```

### Inverted Generational Distance (IGD)

Measures both convergence and diversity.

```python
from pymoo.indicators.igd import IGD

ind = IGD(true_pf)
igd = ind(res.F)
print(f"Inverted Generational Distance: {igd:.4f}")  # Lower is better
```

### Spacing

Measures uniformity of distribution along Pareto front.

```python
def spacing(F):
    """
    Calculate spacing metric.

    Lower values indicate more uniform distribution.
    """
    distances = []
    for i in range(len(F)):
        # Find minimum distance to other solutions
        min_dist = np.min([np.linalg.norm(F[i] - F[j])
                          for j in range(len(F)) if i != j])
        distances.append(min_dist)

    d_bar = np.mean(distances)
    spacing_metric = np.sqrt(np.sum((distances - d_bar)**2) / len(distances))

    return spacing_metric

sp = spacing(res.F)
print(f"Spacing: {sp:.4f}")  # Lower is better
```

## Visualization

### 2D Pareto Front

```python
import matplotlib.pyplot as plt

def plot_pareto_front_2d(F, X=None, labels=None):
    """
    Plot 2D Pareto front.

    F: Objective values, shape (n, 2)
    X: Decision variables (optional)
    labels: Axis labels (optional)
    """
    fig, axes = plt.subplots(1, 2 if X is not None else 1, figsize=(12, 5))

    if X is None:
        axes = [axes]

    # Plot objective space
    axes[0].scatter(F[:, 0], F[:, 1], alpha=0.7)
    axes[0].set_xlabel(labels[0] if labels else 'Objective 1')
    axes[0].set_ylabel(labels[1] if labels else 'Objective 2')
    axes[0].set_title('Pareto Front (Objective Space)')
    axes[0].grid(True, alpha=0.3)

    # Plot decision space
    if X is not None:
        axes[1].scatter(X[:, 0], X[:, 1], alpha=0.7)
        axes[1].set_xlabel('x1')
        axes[1].set_ylabel('x2')
        axes[1].set_title('Pareto Set (Decision Space)')
        axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Usage
plot_pareto_front_2d(res.F, res.X, labels=['Cost', 'Performance'])
```

### 3D Pareto Front

```python
def plot_pareto_front_3d(F, labels=None):
    """Plot 3D Pareto front"""
    from mpl_toolkits.mplot3d import Axes3D

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    ax.scatter(F[:, 0], F[:, 1], F[:, 2], alpha=0.6)
    ax.set_xlabel(labels[0] if labels else 'Objective 1')
    ax.set_ylabel(labels[1] if labels else 'Objective 2')
    ax.set_zlabel(labels[2] if labels else 'Objective 3')
    ax.set_title('Pareto Front')

    plt.show()

# Usage
plot_pareto_front_3d(res.F, labels=['Cost', 'Time', 'Quality'])
```

### Parallel Coordinates (Many Objectives)

```python
from pymoo.visualization.pcp import PCP

# For many objectives (4+)
plot = PCP()
plot.add(res.F, label="Pareto Front")
plot.show()
```

### Trade-off Visualization

```python
def plot_tradeoff_curves(F, ref_idx=0):
    """
    Plot trade-off curves showing how objectives change along Pareto front.

    ref_idx: Reference solution index
    """
    n_obj = F.shape[1]

    # Sort by first objective
    sorted_idx = np.argsort(F[:, 0])
    F_sorted = F[sorted_idx]

    fig, axes = plt.subplots(n_obj - 1, 1, figsize=(10, 4 * (n_obj - 1)))
    if n_obj == 2:
        axes = [axes]

    for i in range(1, n_obj):
        axes[i-1].plot(F_sorted[:, 0], F_sorted[:, i], 'o-', alpha=0.7)
        axes[i-1].set_xlabel('Objective 1')
        axes[i-1].set_ylabel(f'Objective {i+1}')
        axes[i-1].grid(True, alpha=0.3)
        axes[i-1].set_title(f'Trade-off: Obj 1 vs Obj {i+1}')

    plt.tight_layout()
    plt.show()

plot_tradeoff_curves(res.F)
```

## Common Problem Types

### Portfolio Optimization

```python
class PortfolioProblem(Problem):
    """
    Multiobjective portfolio optimization.

    Objectives:
    1. Maximize expected return
    2. Minimize risk (variance)
    """
    def __init__(self, mu, Sigma):
        """
        mu: Expected returns (n_assets,)
        Sigma: Covariance matrix (n_assets, n_assets)
        """
        self.mu = mu
        self.Sigma = Sigma
        n = len(mu)

        super().__init__(
            n_var=n,
            n_obj=2,
            n_ieq_constr=0,
            xl=0.0,  # No short selling
            xu=1.0   # Maximum allocation
        )

    def _evaluate(self, X, out, *args, **kwargs):
        # Normalize weights to sum to 1
        W = X / X.sum(axis=1, keepdims=True)

        # Expected return (maximize -> negate for minimization)
        returns = -(W @ self.mu)

        # Risk (variance)
        risk = np.array([w @ self.Sigma @ w for w in W])

        out["F"] = np.column_stack([returns, risk])

# Example usage
np.random.seed(42)
n_assets = 10
mu = np.random.rand(n_assets) * 0.2  # Returns 0-20%
Sigma = np.random.rand(n_assets, n_assets)
Sigma = Sigma @ Sigma.T  # Make positive definite

problem = PortfolioProblem(mu, Sigma)
algorithm = NSGA2(pop_size=100)
res = minimize(problem, algorithm, ('n_gen', 200), seed=1)

# Plot efficient frontier
plt.scatter(-res.F[:, 0], res.F[:, 1])  # Negate return back to positive
plt.xlabel('Expected Return')
plt.ylabel('Risk (Variance)')
plt.title('Efficient Frontier')
plt.grid(True)
plt.show()
```

### Engineering Design

```python
class EngineeringDesign(Problem):
    """
    Example: Beam design

    Objectives:
    1. Minimize weight
    2. Minimize deflection

    Constraints:
    - Maximum stress
    - Geometric constraints
    """
    def __init__(self):
        super().__init__(
            n_var=3,      # width, height, length
            n_obj=2,      # weight, deflection
            n_ieq_constr=2,
            xl=np.array([0.1, 0.1, 1.0]),
            xu=np.array([1.0, 1.0, 10.0])
        )

    def _evaluate(self, X, out, *args, **kwargs):
        w, h, L = X[:, 0], X[:, 1], X[:, 2]

        # Material properties
        E = 200e9  # Young's modulus (Pa)
        rho = 7850  # Density (kg/m^3)
        F = 10000  # Applied force (N)
        sigma_max = 250e6  # Maximum allowable stress (Pa)

        # Objectives
        weight = rho * w * h * L
        I = (w * h**3) / 12  # Second moment of area
        deflection = (F * L**3) / (3 * E * I)

        # Constraints
        sigma = (F * L * h) / (2 * I)
        g1 = sigma - sigma_max  # Stress constraint: sigma <= sigma_max
        g2 = deflection - 0.01  # Deflection constraint: deflection <= 0.01 m

        out["F"] = np.column_stack([weight, deflection])
        out["G"] = np.column_stack([g1, g2])

problem = EngineeringDesign()
algorithm = NSGA2(pop_size=100)
res = minimize(problem, algorithm, ('n_gen', 300), seed=1)

print(f"Pareto solutions found: {len(res.F)}")
print(f"Max constraint violation: {np.max(res.CV):.2e}")
```

### Feature Selection

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

class FeatureSelectionProblem(Problem):
    """
    Multiobjective feature selection.

    Objectives:
    1. Maximize accuracy (minimize error)
    2. Minimize number of features
    """
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.n_features = X.shape[1]

        super().__init__(
            n_var=self.n_features,
            n_obj=2,
            xl=0,
            xu=1,
            type_var=int  # Binary: feature selected or not
        )

    def _evaluate(self, X, out, *args, **kwargs):
        errors = []
        n_features = []

        for mask in X:
            # Count selected features
            n_sel = np.sum(mask)

            # Avoid empty feature set
            if n_sel == 0:
                errors.append(1.0)
                n_features.append(0)
                continue

            # Select features
            X_subset = self.X[:, mask.astype(bool)]

            # Evaluate with cross-validation
            clf = RandomForestClassifier(n_estimators=50, random_state=42)
            scores = cross_val_score(clf, X_subset, self.y, cv=3)
            error = 1 - np.mean(scores)

            errors.append(error)
            n_features.append(n_sel)

        out["F"] = np.column_stack([errors, n_features])

# Example usage (commented to avoid long runtime)
# data = load_breast_cancer()
# problem = FeatureSelectionProblem(data.data, data.target)
# algorithm = NSGA2(pop_size=50)
# res = minimize(problem, algorithm, ('n_gen', 100), seed=1)
```

## Best Practices

### 1. Normalize Objectives

```python
class NormalizedProblem(Problem):
    def __init__(self, f1_scale, f2_scale):
        self.f1_scale = f1_scale
        self.f2_scale = f2_scale
        super().__init__(n_var=2, n_obj=2, xl=0, xu=1)

    def _evaluate(self, X, out, *args, **kwargs):
        # Raw objectives
        f1 = X[:, 0]**2 + X[:, 1]**2
        f2 = (X[:, 0] - 1)**2 + (X[:, 1] - 1)**2

        # Normalize to similar scales
        f1_norm = f1 / self.f1_scale
        f2_norm = f2 / self.f2_scale

        out["F"] = np.column_stack([f1_norm, f2_norm])
```

### 2. Set Appropriate Population Size

**Rule of thumb:**
- 2 objectives: pop_size = 50-100
- 3 objectives: pop_size = 100-200
- 4+ objectives: pop_size = 200-500

```python
# Automatically determine population size
n_obj = problem.n_obj
if n_obj == 2:
    pop_size = 100
elif n_obj == 3:
    pop_size = 150
else:
    pop_size = 200 + 50 * (n_obj - 3)

algorithm = NSGA2(pop_size=pop_size)
```

### 3. Use Sufficient Generations

```python
# Monitor convergence
from pymoo.util.display import Display

class MyDisplay(Display):
    def _do(self, problem, evaluator, algorithm):
        super()._do(problem, evaluator, algorithm)
        # Custom metrics
        print(f"Gen {algorithm.n_gen}: HV = {ind.calc(algorithm.pop.get('F')):.4f}")

# Run with monitoring
res = minimize(
    problem,
    algorithm,
    ('n_gen', 500),
    verbose=True,
    display=MyDisplay()
)
```

### 4. Multiple Runs for Robustness

```python
# Run multiple times with different seeds
results = []
hypervolumes = []

for seed in range(10):
    res = minimize(problem, algorithm, ('n_gen', 200), seed=seed, verbose=False)
    results.append(res)
    hypervolumes.append(ind(res.F))

# Select best run
best_idx = np.argmax(hypervolumes)
best_result = results[best_idx]

print(f"Best hypervolume: {hypervolumes[best_idx]:.4f}")
print(f"Mean hypervolume: {np.mean(hypervolumes):.4f} ± {np.std(hypervolumes):.4f}")
```

### 5. Validate Pareto Front

```python
def validate_pareto_front(F):
    """Verify all solutions are non-dominated"""
    n = len(F)
    for i in range(n):
        for j in range(n):
            if i != j:
                if is_dominated(F[i], F[j]):
                    print(f"Warning: Solution {i} is dominated by {j}")
                    return False
    print("✓ All solutions are non-dominated")
    return True

validate_pareto_front(res.F)
```

## Decision Making

After finding Pareto front, decision-maker must choose solution.

### Knee Point

Solution with best trade-off (maximum marginal rate of substitution).

```python
from pymoo.mcdm.pseudo_weights import PseudoWeights

# Find knee point
weights = PseudoWeights()
knee_idx = weights.do(res.F)

print(f"Knee point solution:")
print(f"  Objectives: {res.F[knee_idx]}")
print(f"  Variables: {res.X[knee_idx]}")
```

### Compromise Programming

Minimize distance to utopia point.

```python
def find_compromise_solution(F, p=2):
    """
    Find compromise solution using Lp-norm.

    p=1: Manhattan distance
    p=2: Euclidean distance
    p=inf: Chebyshev distance
    """
    # Utopia point (best value for each objective)
    utopia = np.min(F, axis=0)

    # Nadir point (worst value for each objective)
    nadir = np.max(F, axis=0)

    # Normalize
    F_norm = (F - utopia) / (nadir - utopia + 1e-10)

    # Calculate distances
    if p == np.inf:
        distances = np.max(F_norm, axis=1)
    else:
        distances = np.sum(F_norm**p, axis=1)**(1/p)

    # Find minimum distance
    best_idx = np.argmin(distances)

    return best_idx

compromise_idx = find_compromise_solution(res.F, p=2)
print(f"Compromise solution: {res.F[compromise_idx]}")
```

### Interactive Filtering

```python
def filter_solutions(F, X, constraints):
    """
    Filter solutions based on preferences.

    constraints: dict like {'f1_max': 10, 'f2_max': 5}
    """
    mask = np.ones(len(F), dtype=bool)

    if 'f1_max' in constraints:
        mask &= F[:, 0] <= constraints['f1_max']
    if 'f2_max' in constraints:
        mask &= F[:, 1] <= constraints['f2_max']
    # Add more constraints as needed

    return F[mask], X[mask]

# Example: Filter solutions with f1 <= 0.5
F_filtered, X_filtered = filter_solutions(
    res.F, res.X,
    {'f1_max': 0.5}
)
print(f"Filtered to {len(F_filtered)} solutions")
```

## Troubleshooting

| Problem | Symptoms | Solutions |
|---------|----------|-----------|
| Poor convergence | Large gaps in Pareto front | Increase generations, population size |
| Uneven distribution | Clustered solutions | Use NSGA-III or MOEA/D, increase diversity |
| Constraint violations | res.CV > 0 | Adjust constraint handling, feasible initialization |
| Long runtime | Hours for convergence | Reduce pop size, use faster problem evaluation, parallelize |
| Non-convex regions missed | Gaps in weighted sum front | Use ε-constraint or evolutionary algorithms |
| Scaling issues | One objective dominates | Normalize objectives to similar ranges |

## Performance Tips

### Parallelize Evaluations

```python
from multiprocessing.pool import ThreadPool

# Parallelize with threads
pool = ThreadPool(8)
res = minimize(
    problem,
    algorithm,
    ('n_gen', 200),
    seed=1,
    verbose=True,
    elementwise_runner=pool.starmap
)
pool.close()
```

### Warm Start from Previous Run

```python
# Save result
np.save('pareto_front.npy', res.F)
np.save('pareto_set.npy', res.X)

# Load and use as initial population
prev_X = np.load('pareto_set.npy')
from pymoo.core.population import Population

pop = Population.new(X=prev_X)
algorithm.initialization = pop

# Continue optimization
res = minimize(problem, algorithm, ('n_gen', 100), seed=1)
```

## Installation

```bash
# pymoo (recommended)
pip install pymoo

# platypus
pip install platypus-opt

# DEAP
pip install deap

# Visualization
pip install matplotlib
```

## Additional Resources

- **pymoo Documentation:** https://pymoo.org/
- **Multi-Objective Optimization (Deb):** Classic textbook
- **EMO Conference:** Leading conference on evolutionary multiobjective optimization
- **NSGA-II Paper:** Deb et al., "A fast and elitist multiobjective genetic algorithm: NSGA-II"

## Related Skills

- `python-optimization` - Single-objective optimization (scipy, pyomo, cvxpy)
- `python-ase` - Materials optimization with multiple objectives
- `pycse` - Regression and parameter estimation

---

### python-optimization

**Path**: `skills/programming/python-optimization/SKILL.md`

# Python Optimization Expert

## Overview

Master mathematical optimization in Python through systematic problem analysis, appropriate library selection, and robust implementation. This skill guides you through classifying optimization problems, choosing the right solver, implementing efficient solutions, and troubleshooting convergence issues across linear programming, nonlinear programming, mixed-integer problems, and global optimization.

**Core value:** Transform optimization problems from mathematical formulation to working Python code with the most appropriate library and solver for your specific problem type.

## When to Use

Use this skill when:
- Minimizing or maximizing objective functions
- Parameter estimation and curve fitting
- Portfolio optimization and resource allocation
- Production planning and scheduling
- Engineering design optimization
- Control and dynamic optimization
- Solving constrained or unconstrained optimization problems
- Choosing between scipy, pyomo, cvxpy, or other optimization libraries

**Don't use when:**
- Simple root finding (use scipy.optimize.root instead)
- Linear algebra only (use numpy/scipy.linalg)
- Symbolic mathematics needed (use sympy)

## Problem Analysis Framework

### Step 1: Extract Problem Components

**Always identify:**

1. **Objective function**: What to minimize or maximize?
   - Example: minimize cost, maximize profit, minimize error

2. **Decision variables**: What can be changed?
   - Continuous (real numbers) or discrete (integers)?
   - How many variables?

3. **Constraints**: What restrictions exist?
   - Equality constraints: h(x) = 0
   - Inequality constraints: g(x) ≥ 0
   - Bounds: lower ≤ x ≤ upper

4. **Data/parameters**: What values are given?
   - Constants, measurements, input data

### Step 2: Classify Problem Type

**Linear Programming (LP)**
- Linear objective function
- Linear constraints only
- Example: max c'x subject to Ax ≤ b

**Quadratic Programming (QP)**
- Quadratic objective function
- Linear constraints
- Example: min (1/2)x'Qx + c'x subject to Ax ≤ b

**Nonlinear Programming (NLP)**
- Nonlinear objective or constraints
- Smooth, differentiable functions
- Example: min f(x) subject to g(x) ≥ 0

**Mixed-Integer Programming (MIP/MILP/MINLP)**
- Some or all variables must be integers
- Combinatorial optimization
- Example: scheduling, assignment problems

**Convex Optimization**
- Convex objective function
- Convex feasible region
- Global optimum guaranteed

**Global Optimization**
- Multiple local minima suspected
- Non-convex problems
- Requires global search strategies

**Least Squares**
- Minimize sum of squared residuals
- Parameter estimation, curve fitting
- Special structure exploited by dedicated solvers

**Dynamic Optimization / Optimal Control**
- Time-dependent variables
- Differential equations as constraints
- Trajectory optimization

### Step 3: Characterize Problem Scale

**Small problems**: < 100 variables
- Most methods work well
- scipy.optimize is sufficient

**Medium problems**: 100-1,000 variables
- Need efficient algorithms
- Consider specialized solvers

**Large problems**: > 1,000 variables
- Requires sparse matrix techniques
- Use pyomo or specialized solvers
- Exploit problem structure

**Other characteristics:**
- Are derivatives available? Analytical or numerical?
- Smooth or non-smooth?
- Sparse or dense?
- Special structure (separable, etc.)?

## Library Selection Guide

### scipy.optimize - Default Choice

**Use when:**
- Small to medium problems (<1,000 variables)
- Unconstrained or smooth constrained NLP
- Quick prototyping and development
- Least squares / curve fitting
- Python-native solution preferred

**Strengths:**
- Part of scipy (widely available)
- Easy to use, good documentation
- Multiple algorithms available
- No additional dependencies

**Limitations:**
- Not ideal for large-scale problems
- Limited support for mixed-integer
- Less sophisticated than commercial solvers

### pyomo - Algebraic Modeling

**Use when:**
- Large-scale problems (>1,000 variables)
- Mixed-integer programming
- Need multiple solver backends
- Algebraic modeling preferred
- Production environments

**Strengths:**
- Scales to very large problems
- Supports many external solvers
- Clean mathematical notation
- Active development and community

**Limitations:**
- Requires external solvers for many problem types
- Steeper learning curve
- Additional installation complexity

### cvxpy - Convex Optimization

**Use when:**
- Problem is (or might be) convex
- Want automatic convexity verification
- Prefer disciplined convex programming
- Portfolio optimization, control

**Strengths:**
- Automatic convexity detection
- Mathematical notation syntax
- Multiple solver backends
- Excellent for convex problems

**Limitations:**
- Only for convex problems
- Less flexible for non-convex
- Learning curve for DCP rules

### GEKKO - Dynamic Optimization

**Use when:**
- Black-box functions (no derivatives)
- Dynamic optimization / optimal control
- Differential equations involved
- Process control, trajectory optimization

**Strengths:**
- Handles differential equations naturally
- Derivative-free optimization
- Good for dynamic systems
- Time-dependent problems

**Limitations:**
- Specialized use case
- Less common than scipy
- Requires GEKKO server

### Comparison Table

| Library | Best For | Problem Types | Scale |
|---------|----------|---------------|-------|
| scipy.optimize | General NLP, prototyping | LP, QP, NLP, LS | Small-Medium |
| pyomo | Large-scale, MIP | LP, MILP, NLP, MINLP | Any |
| cvxpy | Convex problems | LP, QP, SOCP, SDP | Medium-Large |
| GEKKO | Dynamic optimization | NLP, DAE, MPC | Small-Medium |

## scipy.optimize - Method Selection

### Unconstrained Optimization

**`minimize(f, x0, method='BFGS', jac=gradient)`**
- **Use for:** Smooth unconstrained problems
- **Requires:** Gradient (or uses finite differences)
- **Pros:** Fast convergence, robust
- **Cons:** Can get stuck in local minima

**`minimize(f, x0, method='L-BFGS-B', bounds=bounds)`**
- **Use for:** Large-scale with box constraints
- **Pros:** Memory efficient, handles bounds
- **Cons:** Only box constraints (no general constraints)

**`minimize(f, x0, method='Nelder-Mead')`**
- **Use for:** Derivative-free, non-smooth
- **Pros:** No derivatives needed
- **Cons:** Slow convergence, unreliable for large problems

**`minimize(f, x0, method='Powell')`**
- **Use for:** Derivative-free, better than Nelder-Mead
- **Pros:** More robust than Nelder-Mead
- **Cons:** Still derivative-free (slower)

### Constrained Optimization

**`minimize(f, x0, method='SLSQP', constraints=cons)`**
- **Use for:** General constraints (equality + inequality)
- **Pros:** Handles all constraint types
- **Cons:** Less robust than trust-constr

**`minimize(f, x0, method='trust-constr', constraints=cons)`**
- **Use for:** Difficult constrained problems
- **Pros:** Most robust, can use Hessian
- **Cons:** Slower than SLSQP

**`minimize(f, x0, method='COBYLA', constraints=cons)`**
- **Use for:** Derivative-free with constraints
- **Pros:** No derivatives needed
- **Cons:** Slow, less accurate

### Special-Purpose Functions

**`least_squares(residual, x0, bounds=bounds)`**
- **Use for:** Nonlinear least squares
- **Pros:** Exploits sum-of-squares structure
- **Cons:** Only for least squares problems

**`curve_fit(model, xdata, ydata, p0=initial)`**
- **Use for:** Parameter estimation, curve fitting
- **Pros:** Simple interface for fitting
- **Cons:** Just a wrapper for least_squares

**`differential_evolution(f, bounds)`**
- **Use for:** Global optimization, multiple local minima
- **Pros:** Finds global optimum, derivative-free
- **Cons:** Slow, must specify bounds

**`linprog(c, A_ub, b_ub)`**
- **Use for:** Linear programming only
- **Pros:** Specialized for LP, efficient
- **Cons:** Only linear problems

## Implementation Patterns

### Pattern 1: Unconstrained Optimization

```python
from scipy.optimize import minimize
import numpy as np

def objective(x):
    """
    Objective function to minimize.

    Example: Rosenbrock function
    f(x) = sum((1 - x[i])^2 + 100*(x[i+1] - x[i]^2)^2)
    """
    return sum((1 - x[:-1])**2 + 100*(x[1:] - x[:-1]**2)**2)

def gradient(x):
    """
    Analytical gradient - ALWAYS provide if possible!
    Dramatically improves convergence speed and reliability.
    """
    grad = np.zeros_like(x)
    grad[:-1] = -2*(1 - x[:-1]) - 400*x[:-1]*(x[1:] - x[:-1]**2)
    grad[1:] += 200*(x[1:] - x[:-1]**2)
    return grad

# Initial guess
x0 = np.array([0.5, 0.5])

# Optimize
result = minimize(objective, x0, method='BFGS', jac=gradient)

# Check results
print(f"Success: {result.success}")
print(f"Message: {result.message}")
print(f"Optimal x: {result.x}")
print(f"Optimal value: {result.fun}")
print(f"Iterations: {result.nit}")
print(f"Function evaluations: {result.nfev}")
```

### Pattern 2: Constrained Optimization

```python
from scipy.optimize import minimize
import numpy as np

def objective(x):
    """Objective: minimize x[0]^2 + x[1]^2"""
    return x[0]**2 + x[1]**2

def constraint_ineq(x):
    """Inequality constraint: x[0] + x[1] - 1 >= 0"""
    return x[0] + x[1] - 1

def constraint_eq(x):
    """Equality constraint: x[0]^2 + x[1]^2 - 1 = 0"""
    return x[0]**2 + x[1]**2 - 1

# Define constraints (can have multiple)
constraints = [
    {'type': 'ineq', 'fun': constraint_ineq},  # g(x) >= 0
    {'type': 'eq', 'fun': constraint_eq}       # h(x) = 0
]

# Bounds: [(lower, upper), ...], use None for no bound
bounds = [(-2, 2), (-2, 2)]

# Initial guess (should be feasible if possible)
x0 = np.array([0.5, 0.5])

# Optimize
result = minimize(
    objective,
    x0,
    method='SLSQP',
    constraints=constraints,
    bounds=bounds,
    options={'ftol': 1e-9, 'maxiter': 1000}
)

print(f"Optimal x: {result.x}")
print(f"Optimal value: {result.fun}")

# Verify constraints are satisfied
print(f"\nConstraint verification:")
print(f"Inequality (should be >= 0): {constraint_ineq(result.x):.6f}")
print(f"Equality (should be ~0): {constraint_eq(result.x):.6e}")
```

### Pattern 3: Parameter Estimation (Curve Fitting)

```python
from scipy.optimize import curve_fit
import numpy as np

def model(x, a, b, c):
    """
    Model function: f(x; parameters)

    Example: Exponential decay + offset
    """
    return a * np.exp(-b * x) + c

# Generate synthetic data (in practice, this is your measured data)
x_data = np.linspace(0, 10, 50)
y_true = 2.5 * np.exp(-0.3 * x_data) + 0.5
y_data = y_true + 0.1 * np.random.randn(len(x_data))  # Add noise

# Fit model to data
# p0 = initial parameter guess (optional but recommended)
params, covariance = curve_fit(
    model,
    x_data,
    y_data,
    p0=[2, 0.5, 0]  # Initial guess: [a, b, c]
)

# Extract parameters and uncertainties
a_opt, b_opt, c_opt = params
uncertainties = np.sqrt(np.diag(covariance))

print(f"Optimal parameters:")
print(f"  a = {a_opt:.3f} ± {uncertainties[0]:.3f}")
print(f"  b = {b_opt:.3f} ± {uncertainties[1]:.3f}")
print(f"  c = {c_opt:.3f} ± {uncertainties[2]:.3f}")

# Calculate R-squared
y_pred = model(x_data, *params)
ss_res = np.sum((y_data - y_pred)**2)
ss_tot = np.sum((y_data - np.mean(y_data))**2)
r_squared = 1 - (ss_res / ss_tot)
print(f"\nR² = {r_squared:.4f}")
```

### Pattern 4: Least Squares (Residuals)

```python
from scipy.optimize import least_squares
import numpy as np

def residuals(params, x, y):
    """
    Residual function for least squares.

    Returns: array of residuals (model - data)
    """
    a, b, c = params
    model = a * np.exp(-b * x) + c
    return model - y

# Data
x_data = np.linspace(0, 10, 50)
y_data = 2.5 * np.exp(-0.3 * x_data) + 0.5 + 0.1*np.random.randn(50)

# Initial guess
p0 = [2, 0.5, 0]

# Optimize with bounds
result = least_squares(
    residuals,
    p0,
    args=(x_data, y_data),
    bounds=([0, 0, -np.inf], [10, 5, np.inf])  # Lower and upper bounds
)

print(f"Success: {result.success}")
print(f"Optimal parameters: {result.x}")
print(f"Final cost (sum of squared residuals): {result.cost}")
print(f"Optimality: {result.optimality}")  # Should be small
```

### Pattern 5: Linear Programming

```python
from scipy.optimize import linprog
import numpy as np

"""
Linear Programming Problem:

Maximize:   3*x1 + 2*x2
Subject to: x1 + x2 <= 4
            2*x1 + x2 <= 5
            x1, x2 >= 0

Note: linprog minimizes, so negate objective for maximization
"""

# Objective coefficients (negate for maximization)
c = [-3, -2]

# Inequality constraints: A_ub @ x <= b_ub
A_ub = np.array([
    [1, 1],   # x1 + x2 <= 4
    [2, 1]    # 2*x1 + x2 <= 5
])
b_ub = np.array([4, 5])

# Bounds: x1, x2 >= 0
bounds = [(0, None), (0, None)]

# Solve
result = linprog(
    c,
    A_ub=A_ub,
    b_ub=b_ub,
    bounds=bounds,
    method='highs'  # Default, most efficient
)

print(f"Success: {result.success}")
print(f"Optimal x: {result.x}")
print(f"Optimal value (minimization): {result.fun}")
print(f"Optimal value (maximization): {-result.fun}")
```

### Pattern 6: Global Optimization

```python
from scipy.optimize import differential_evolution
import numpy as np

def objective(x):
    """
    Function with multiple local minima.

    Example: Rastrigin function
    """
    A = 10
    return A*len(x) + sum(x**2 - A*np.cos(2*np.pi*x))

# MUST specify bounds for each variable
bounds = [(-5.12, 5.12), (-5.12, 5.12)]

# Global optimization
result = differential_evolution(
    objective,
    bounds,
    strategy='best1bin',
    maxiter=1000,
    popsize=15,
    tol=1e-7,
    atol=1e-9,
    workers=-1,  # Use all CPU cores
    polish=True,  # Local refinement at the end
    seed=42  # For reproducibility
)

print(f"Success: {result.success}")
print(f"Global minimum x: {result.x}")
print(f"Global minimum value: {result.fun}")
print(f"Function evaluations: {result.nfev}")

# For comparison, try local optimization from random start
from scipy.optimize import minimize
x0 = np.random.uniform(-5.12, 5.12, 2)
local_result = minimize(objective, x0, method='BFGS')
print(f"\nLocal optimization from random start:")
print(f"Local minimum value: {local_result.fun}")
print(f"(Global is better: {result.fun < local_result.fun})")
```

### Pattern 7: pyomo (Large-Scale / Mixed-Integer)

```python
import pyomo.environ as pyo

# Create model
model = pyo.ConcreteModel()

# Decision variables
model.x = pyo.Var(domain=pyo.NonNegativeReals)
model.y = pyo.Var(domain=pyo.Integers, bounds=(0, 10))
model.z = pyo.Var([1, 2, 3], domain=pyo.Binary)  # Indexed binary variables

# Parameters (optional)
model.a = pyo.Param(initialize=2.5)
model.b = pyo.Param([1, 2, 3], initialize={1: 1.0, 2: 2.0, 3: 3.0})

# Objective function
model.obj = pyo.Objective(
    expr=model.x**2 + model.y + sum(model.z[i] for i in [1,2,3]),
    sense=pyo.minimize  # or pyo.maximize
)

# Constraints
model.con1 = pyo.Constraint(expr=model.x + model.y >= 5)
model.con2 = pyo.Constraint(expr=2*model.x - model.y <= 10)
model.con3 = pyo.Constraint(expr=sum(model.z[i] for i in [1,2,3]) <= 2)

# Solve
# For continuous: 'ipopt' (nonlinear) or 'glpk' (linear)
# For integer: 'cbc', 'glpk', 'gurobi' (commercial)
solver = pyo.SolverFactory('ipopt')
results = solver.solve(model, tee=True)  # tee=True shows solver output

# Check if optimal
if results.solver.termination_condition == pyo.TerminationCondition.optimal:
    print("\nOptimal solution found!")
    print(f"x = {pyo.value(model.x):.4f}")
    print(f"y = {pyo.value(model.y):.4f}")
    print(f"z = {[pyo.value(model.z[i]) for i in [1,2,3]]}")
    print(f"Objective value = {pyo.value(model.obj):.4f}")
else:
    print(f"Solver status: {results.solver.termination_condition}")
```

### Pattern 8: cvxpy (Convex Optimization)

```python
import cvxpy as cp
import numpy as np

"""
Portfolio optimization example:
Minimize variance subject to minimum expected return
"""

# Problem data
n = 4  # Number of assets
mu = np.array([0.12, 0.10, 0.07, 0.03])  # Expected returns
Sigma = np.array([  # Covariance matrix
    [0.10, 0.03, 0.01, 0.00],
    [0.03, 0.08, 0.02, 0.01],
    [0.01, 0.02, 0.05, 0.01],
    [0.00, 0.01, 0.01, 0.02]
])

# Variables
w = cp.Variable(n)  # Portfolio weights

# Objective: minimize variance
objective = cp.Minimize(cp.quad_form(w, Sigma))

# Constraints
constraints = [
    cp.sum(w) == 1,           # Weights sum to 1
    w >= 0,                   # No short selling
    mu @ w >= 0.10            # Minimum expected return
]

# Solve
problem = cp.Problem(objective, constraints)
problem.solve()

# Results
print(f"Status: {problem.status}")
if problem.status == 'optimal':
    print(f"Optimal portfolio weights: {w.value}")
    print(f"Portfolio variance: {problem.value:.6f}")
    print(f"Portfolio std dev: {np.sqrt(problem.value):.4f}")
    print(f"Expected return: {(mu @ w.value):.4f}")
else:
    print(f"Problem could not be solved: {problem.status}")
```

## Best Practices

### 1. Always Provide Analytical Gradients

**Why:** 10-100x faster convergence, more reliable

```python
def objective(x):
    return x[0]**2 + x[1]**2

def gradient(x):
    """Analytical gradient - much better than finite differences"""
    return np.array([2*x[0], 2*x[1]])

# Good: Uses analytical gradient
result = minimize(objective, x0, method='BFGS', jac=gradient)

# Okay but slower: Uses finite differences
result = minimize(objective, x0, method='BFGS')
```

### 2. Verify Gradients

```python
from scipy.optimize import check_grad

# Check gradient accuracy
error = check_grad(objective, gradient, x0)
print(f"Gradient error: {error}")  # Should be < 1e-5

# For constraints
for con in constraints:
    if 'jac' in con:
        error = check_grad(con['fun'], con['jac'], x0)
        print(f"Constraint gradient error: {error}")
```

### 3. Scale Variables

**Problem:** Variables with very different magnitudes
```python
# Bad: x1 ~ 1e-6, x2 ~ 1e6
x = [1e-6, 1e6]
```

**Solution:** Normalize to similar scales
```python
# Good: Both variables ~ O(1)
x_scaled = [x[0] * 1e6, x[1] / 1e6]

def objective_scaled(x_scaled):
    x = [x_scaled[0] / 1e6, x_scaled[1] * 1e6]  # Convert back
    return original_objective(x)
```

### 4. Provide Good Initial Guess

```python
# Bad: Random or arbitrary starting point
x0 = np.zeros(n)

# Good: Physically reasonable or feasible starting point
x0 = np.array([1.0, 2.0, 0.5])  # Based on problem knowledge

# Better: Try multiple starting points for non-convex problems
initial_guesses = [
    np.array([1.0, 2.0]),
    np.array([-1.0, -2.0]),
    np.random.randn(2)
]

results = [minimize(objective, x0) for x0 in initial_guesses]
best_result = min(results, key=lambda r: r.fun)
```

### 5. Validate Solutions

```python
def validate_solution(result, constraints, bounds):
    """Comprehensive solution validation"""

    # Check optimization succeeded
    assert result.success, f"Optimization failed: {result.message}"

    # Check inequality constraints: g(x) >= 0
    for i, con in enumerate([c for c in constraints if c['type'] == 'ineq']):
        val = con['fun'](result.x)
        assert val >= -1e-6, f"Inequality constraint {i} violated: {val:.2e}"

    # Check equality constraints: h(x) = 0
    for i, con in enumerate([c for c in constraints if c['type'] == 'eq']):
        val = con['fun'](result.x)
        assert abs(val) < 1e-6, f"Equality constraint {i} violated: {val:.2e}"

    # Check bounds
    if bounds is not None:
        for i, (lb, ub) in enumerate(bounds):
            if lb is not None:
                assert result.x[i] >= lb - 1e-6, f"Lower bound {i} violated"
            if ub is not None:
                assert result.x[i] <= ub + 1e-6, f"Upper bound {i} violated"

    print("✓ All constraints satisfied")
    return True

# Use it
validate_solution(result, constraints, bounds)
```

### 6. Handle Convergence Issues

**Common problems and solutions:**

```python
# Problem: "Desired error not necessarily achieved"
# Solution 1: Relax tolerances
result = minimize(
    objective, x0,
    method='BFGS',
    options={'ftol': 1e-6, 'gtol': 1e-5}  # Less strict
)

# Solution 2: Scale variables
x_scaled = x / typical_scale
result = minimize(objective_scaled, x_scaled)

# Solution 3: Try different method
result = minimize(objective, x0, method='trust-constr')

# Solution 4: Better initial guess
x0 = feasible_starting_point()

# Problem: Stuck in local minimum
# Solution: Global optimization
result = differential_evolution(objective, bounds)

# Problem: Constraints incompatible
# Solution: Check constraint feasibility
from scipy.optimize import fsolve
# Try to find any feasible point
def feasibility(x):
    violations = []
    for con in constraints:
        if con['type'] == 'eq':
            violations.append(con['fun'](x))
    return violations

x_feasible = fsolve(feasibility, x0)
```

## Common Problem Types - Quick Reference

### 1. Unconstrained: minimize f(x)

```python
minimize(f, x0, method='BFGS', jac=grad)
```

### 2. Box constraints: minimize f(x) subject to lb ≤ x ≤ ub

```python
minimize(f, x0, method='L-BFGS-B', bounds=bounds)
```

### 3. General constraints: minimize f(x) s.t. g(x) ≥ 0, h(x) = 0

```python
cons = [{'type': 'ineq', 'fun': g}, {'type': 'eq', 'fun': h}]
minimize(f, x0, method='SLSQP', constraints=cons)
```

### 4. Least squares: minimize ||r(x)||²

```python
least_squares(residual, x0, bounds=(lb, ub))
```

### 5. Curve fitting: fit model to data

```python
params, cov = curve_fit(model, xdata, ydata, p0=initial)
```

### 6. Linear program: minimize c'x s.t. Ax ≤ b

```python
linprog(c, A_ub=A, b_ub=b, bounds=bounds)
```

### 7. Mixed-integer: some variables must be integers

```python
# Use pyomo
model.x = pyo.Var(domain=pyo.Integers)
solver = pyo.SolverFactory('cbc')
solver.solve(model)
```

### 8. Global: multiple local minima

```python
differential_evolution(f, bounds)
```

### 9. Portfolio optimization

```python
# cvxpy
w = cp.Variable(n)
objective = cp.Minimize(cp.quad_form(w, Sigma))
constraints = [cp.sum(w) == 1, w >= 0, mu @ w >= target_return]
problem = cp.Problem(objective, constraints)
problem.solve()
```

### 10. Parameter estimation with uncertainties

```python
params, cov = curve_fit(model, xdata, ydata)
uncertainties = np.sqrt(np.diag(cov))
```

## Response Format

When solving an optimization problem, always:

### 1. Restate the Problem

"Based on your description, this is a **[problem type]** problem:

**Objective**: minimize/maximize f(x) = ...
**Variables**: x ∈ R^n (or x ∈ Z^n for integers)
**Constraints**:
- [List inequality constraints]
- [List equality constraints]
- [List bounds]"

### 2. Explain Library/Method Choice

"Since this problem has [characteristics], I recommend **[library]** with **[method]**:
- [Reason 1]
- [Reason 2]
- [Alternative if this doesn't work]"

### 3. Provide Complete Implementation

```python
# Complete, runnable code with comments
# Include imports, data, functions, optimization, validation
```

### 4. Display and Interpret Results

"**Results**:
- Optimal x: [values]
- Optimal value: [value]
- Constraints satisfied: ✓ / ✗
- Convergence: [iterations, function evaluations]

**Interpretation**: [What the solution means in context]"

### 5. Suggest Improvements if Needed

"**If convergence issues arise:**
- Try [alternative method]
- Adjust [parameters]
- Consider [problem reformulation]"

## Advanced Topics

### Multi-Objective Optimization

For problems with **multiple conflicting objectives** (minimize cost AND maximize performance), use the dedicated **`python-multiobjective-optimization`** skill which covers:

- Pareto optimality and Pareto fronts
- Evolutionary algorithms (NSGA-II, NSGA-III, MOEA/D)
- Scalarization methods (weighted sum, ε-constraint)
- Libraries: pymoo, platypus, DEAP
- Pareto analysis and decision making
- Visualization techniques

**Quick example** - Weighted sum scalarization with scipy:

```python
from scipy.optimize import minimize

def scalarize_objectives(x, weights):
    """Weighted sum approach for multi-objective"""
    f1 = objective1(x)
    f2 = objective2(x)
    return weights[0] * f1 + weights[1] * f2

# Solve for different weight combinations to trace Pareto front
weights_list = [(1, 0), (0.75, 0.25), (0.5, 0.5), (0.25, 0.75), (0, 1)]
pareto_front = []

for weights in weights_list:
    result = minimize(
        lambda x: scalarize_objectives(x, weights),
        x0,
        method='SLSQP',
        constraints=constraints
    )
    if result.success:
        pareto_front.append({
            'x': result.x,
            'f1': objective1(result.x),
            'f2': objective2(result.x)
        })
```

**Note:** Weighted sum only finds convex portions of Pareto front. For comprehensive multiobjective optimization, see `python-multiobjective-optimization` skill.

### Robust Optimization

```python
def worst_case_objective(x, uncertainty_scenarios):
    """Minimize worst-case performance"""
    return max(objective(x, scenario) for scenario in uncertainty_scenarios)

result = minimize(worst_case_objective, x0, args=(scenarios,))
```

### Stochastic Optimization

```python
def expected_objective(x, n_samples=1000):
    """Minimize expected value over random parameters"""
    samples = np.random.normal(mu, sigma, (n_samples, n_params))
    return np.mean([objective(x, sample) for sample in samples])

result = minimize(expected_objective, x0)
```

## Troubleshooting Guide

| Problem | Symptoms | Solutions |
|---------|----------|-----------|
| Slow convergence | Many iterations, slow progress | Provide analytical gradient, scale variables, better x0 |
| Local minimum | Different x0 gives different solutions | Use `differential_evolution`, try multiple starts |
| Constraint violations | Solution doesn't satisfy constraints | Check constraint formulation, ensure feasible x0, tighten tolerances |
| "Desired error not achieved" | Warning in result | Relax tolerances, scale variables, try different method |
| Numerical errors | NaN or Inf values | Scale variables, avoid extreme values in objective |
| Memory errors | Large problems crash | Use sparse matrices, L-BFGS-B, or pyomo with sparse solvers |

## Installation

```bash
# Core optimization
pip install scipy

# For large-scale and mixed-integer
pip install pyomo

# Install solvers for pyomo
conda install -c conda-forge ipopt  # Nonlinear
conda install -c conda-forge glpk   # Linear/Integer
pip install cplex  # Commercial, requires license

# For convex optimization
pip install cvxpy

# For dynamic optimization
pip install gekko
```

## Additional Resources

- **SciPy Optimization Tutorial:** https://docs.scipy.org/doc/scipy/tutorial/optimize.html
- **Pyomo Documentation:** https://pyomo.readthedocs.io/
- **CVXPY Tutorial:** https://www.cvxpy.org/tutorial/
- **Convex Optimization (Boyd & Vandenberghe):** https://web.stanford.edu/~boyd/cvxbook/
- **Numerical Optimization (Nocedal & Wright):** Classic textbook

## Related Skills

- `python-multiobjective-optimization` - Multiobjective optimization with Pareto fronts (NSGA-II, NSGA-III, pymoo)
- `pycse` - Regression analysis with confidence intervals, ODE solving
- `python-best-practices` - Code quality and testing for optimization code
- `python-ase` - Atomic structure optimization in materials science

---

### python-plotting

**Path**: `skills/programming/python-plotting/SKILL.md`

# Python Plotting & Visualization

## Overview

Master data visualization in Python through three complementary libraries: **matplotlib** (foundational static plots), **seaborn** (statistical visualization), and **plotly** (interactive graphics). Each library has distinct strengths, and knowing when to use which—or how to combine them—is key to effective data communication.

**Core value:** Create publication-quality static plots, insightful statistical graphics, and interactive dashboards with the right tool for each visualization need.

## Library Selection Guide

### Matplotlib - The Foundation

**Use when:**
- Need fine-grained control over every plot element
- Creating publication-quality static figures
- Building custom visualizations not available elsewhere
- Working with low-level plotting requirements
- Need maximum compatibility (most widely supported)

**Strengths:**
- Complete control over every visual element
- Mature, stable, extensively documented
- Foundation for seaborn and other libraries
- Export to any format (PDF, SVG, PNG, etc.)
- Extensive customization options

**Weaknesses:**
- Verbose for common statistical plots
- Steeper learning curve for complex plots
- Static output (no interactivity)
- Default aesthetics less modern

### Seaborn - Statistical Visualization

**Use when:**
- Creating statistical plots (distributions, correlations, regressions)
- Need attractive defaults with minimal code
- Working with pandas DataFrames
- Want automatic statistical estimation
- Need faceted/multi-panel plots

**Strengths:**
- Concise syntax for statistical plots
- Beautiful default themes
- Built-in statistical estimation
- Seamless pandas integration
- FacetGrid for complex multi-panel layouts

**Weaknesses:**
- Less control than matplotlib
- Limited to statistical plot types
- Static output only
- Requires understanding of underlying matplotlib for deep customization

### Plotly - Interactive Visualization

**Use when:**
- Need interactive plots (zoom, hover, pan)
- Building dashboards or web apps
- Want 3D visualizations
- Need animations
- Sharing exploratory analysis

**Strengths:**
- Rich interactivity out-of-the-box
- Beautiful defaults
- 3D and geographic plotting
- Integration with Dash for web apps
- Export to HTML for sharing

**Weaknesses:**
- Larger file sizes
- Less control than matplotlib for publications
- Different API paradigm
- Not ideal for static publication figures

### Quick Decision Tree

```
Need interactivity?
├─ Yes → Plotly
└─ No → Statistical plot?
    ├─ Yes → Seaborn (can customize with matplotlib)
    └─ No → Complex customization needed?
        ├─ Yes → Matplotlib
        └─ No → Seaborn or Matplotlib (preference)
```

## Matplotlib - Foundational Plotting

### Two APIs: pyplot vs Object-Oriented

**pyplot (MATLAB-style, implicit state):**
```python
import matplotlib.pyplot as plt

# Quick and interactive
plt.plot([1, 2, 3], [1, 4, 9])
plt.xlabel('x')
plt.ylabel('y')
plt.title('Simple Plot')
plt.show()
```

**Object-Oriented (explicit, recommended for complex plots):**
```python
import matplotlib.pyplot as plt

# Explicit control
fig, ax = plt.subplots()
ax.plot([1, 2, 3], [1, 4, 9])
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Simple Plot')
plt.show()
```

**Recommendation:** Use OO API for scripts, functions, and complex plots. Use pyplot for quick interactive exploration.

### Figure Anatomy

```python
import matplotlib.pyplot as plt
import numpy as np

# Create figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# fig = Figure (entire window)
# axes = array of Axes (individual plots)

# Access individual axes
ax1 = axes[0, 0]  # Top left
ax2 = axes[0, 1]  # Top right
ax3 = axes[1, 0]  # Bottom left
ax4 = axes[1, 1]  # Bottom right

# Plot on each axes
ax1.plot([1, 2, 3], [1, 4, 9])
ax2.scatter([1, 2, 3], [1, 4, 9])
ax3.bar([1, 2, 3], [1, 4, 9])
ax4.hist(np.random.randn(1000))

# Adjust layout
plt.tight_layout()
plt.show()
```

### Common Plot Types

**Line Plot:**
```python
fig, ax = plt.subplots()

x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)

ax.plot(x, y1, label='sin(x)', linewidth=2, color='blue', linestyle='-')
ax.plot(x, y2, label='cos(x)', linewidth=2, color='red', linestyle='--')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Trigonometric Functions')
ax.legend()
ax.grid(True, alpha=0.3)
plt.show()
```

**Scatter Plot:**
```python
fig, ax = plt.subplots()

x = np.random.randn(100)
y = 2*x + np.random.randn(100)*0.5
colors = np.random.rand(100)
sizes = 100 * np.random.rand(100)

scatter = ax.scatter(x, y, c=colors, s=sizes, alpha=0.6, cmap='viridis')

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Scatter Plot with Color and Size')

# Add colorbar
plt.colorbar(scatter, ax=ax, label='Color Value')
plt.show()
```

**Bar Plot:**
```python
fig, ax = plt.subplots()

categories = ['A', 'B', 'C', 'D', 'E']
values = [23, 45, 56, 78, 32]

bars = ax.bar(categories, values, color=['red', 'blue', 'green', 'orange', 'purple'])

# Annotate bars
for bar, value in zip(bars, values):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{value}',
            ha='center', va='bottom')

ax.set_ylabel('Values')
ax.set_title('Bar Chart')
plt.show()
```

**Histogram:**
```python
fig, ax = plt.subplots()

data = np.random.randn(1000)

ax.hist(data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')

ax.set_xlabel('Value')
ax.set_ylabel('Frequency')
ax.set_title('Histogram')
ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label='Mean')
ax.legend()
plt.show()
```

**Subplots with Shared Axes:**
```python
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))

x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)

ax1.plot(x, y1)
ax1.set_ylabel('sin(x)')
ax1.grid(True)

ax2.plot(x, y2, color='red')
ax2.set_xlabel('x')
ax2.set_ylabel('cos(x)')
ax2.grid(True)

plt.tight_layout()
plt.show()
```

### Customization

**Styles:**
```python
import matplotlib.pyplot as plt

# List available styles
print(plt.style.available)

# Use a style
plt.style.use('seaborn-v0_8-darkgrid')
# or
with plt.style.context('ggplot'):
    plt.plot([1, 2, 3], [1, 4, 9])
    plt.show()
```

**Colors and Colormaps:**
```python
# Named colors
ax.plot(x, y, color='steelblue')
ax.plot(x, y, color='#FF6347')  # Hex
ax.plot(x, y, color=(0.2, 0.4, 0.6))  # RGB

# Colormaps
from matplotlib import cm

colors = cm.viridis(np.linspace(0, 1, 10))
for i, color in enumerate(colors):
    ax.plot([i, i+1], [0, 1], color=color)
```

**Markers and Line Styles:**
```python
ax.plot(x, y,
        marker='o',           # Circle markers
        markersize=8,
        markerfacecolor='red',
        markeredgecolor='black',
        markeredgewidth=2,
        linestyle='--',       # Dashed line
        linewidth=2,
        color='blue')
```

**Legends:**
```python
ax.plot(x, y1, label='Data 1')
ax.plot(x, y2, label='Data 2')

ax.legend(
    loc='upper right',    # or 'best', 'center', etc.
    frameon=True,
    shadow=True,
    fancybox=True,
    fontsize=12
)
```

**Annotations:**
```python
ax.plot(x, y)

# Annotate a point
ax.annotate(
    'Maximum',
    xy=(x[50], y[50]),       # Point to annotate
    xytext=(x[50]+1, y[50]+1),  # Text location
    arrowprops=dict(arrowstyle='->', color='red', lw=2),
    fontsize=12,
    color='red'
)
```

**Saving Figures:**
```python
# High-resolution PNG
fig.savefig('plot.png', dpi=300, bbox_inches='tight')

# Vector formats for publications
fig.savefig('plot.pdf', bbox_inches='tight')
fig.savefig('plot.svg', bbox_inches='tight')

# Transparent background
fig.savefig('plot.png', transparent=True, bbox_inches='tight')
```

## Seaborn - Statistical Visualization

### Basic Setup

```python
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Set theme
sns.set_theme(style='whitegrid')  # whitegrid, darkgrid, white, dark, ticks

# Load example dataset
tips = sns.load_dataset('tips')  # Built-in datasets for examples
```

### Distribution Plots

**Histogram with KDE:**
```python
fig, ax = plt.subplots()

sns.histplot(data=tips, x='total_bill', kde=True, ax=ax)

ax.set_title('Distribution of Total Bill')
plt.show()
```

**KDE Plot:**
```python
fig, ax = plt.subplots()

sns.kdeplot(data=tips, x='total_bill', hue='time', fill=True, ax=ax)

ax.set_title('Total Bill Distribution by Time')
plt.show()
```

**Distribution Plot (ECDF):**
```python
fig, ax = plt.subplots()

sns.ecdfplot(data=tips, x='total_bill', hue='sex', ax=ax)

ax.set_title('Cumulative Distribution of Total Bill')
plt.show()
```

### Categorical Plots

**Box Plot:**
```python
fig, ax = plt.subplots(figsize=(10, 6))

sns.boxplot(data=tips, x='day', y='total_bill', hue='sex', ax=ax)

ax.set_title('Total Bill by Day and Sex')
plt.show()
```

**Violin Plot:**
```python
fig, ax = plt.subplots(figsize=(10, 6))

sns.violinplot(data=tips, x='day', y='total_bill', hue='sex',
               split=True, ax=ax)

ax.set_title('Total Bill Distribution by Day')
plt.show()
```

**Strip Plot / Swarm Plot:**
```python
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Strip plot (with jitter)
sns.stripplot(data=tips, x='day', y='total_bill', hue='sex',
              dodge=True, alpha=0.6, ax=axes[0])
axes[0].set_title('Strip Plot')

# Swarm plot (no overlap)
sns.swarmplot(data=tips, x='day', y='total_bill', hue='sex',
              dodge=True, ax=axes[1])
axes[1].set_title('Swarm Plot')

plt.tight_layout()
plt.show()
```

**Bar Plot with Error Bars:**
```python
fig, ax = plt.subplots()

sns.barplot(data=tips, x='day', y='total_bill', hue='sex',
            ci=95,  # Confidence interval
            ax=ax)

ax.set_title('Average Total Bill by Day')
plt.show()
```

### Relational Plots

**Scatter Plot:**
```python
fig, ax = plt.subplots(figsize=(10, 6))

sns.scatterplot(data=tips, x='total_bill', y='tip',
                hue='time', size='size', style='sex',
                sizes=(50, 200), alpha=0.6, ax=ax)

ax.set_title('Tip vs Total Bill')
plt.show()
```

**Line Plot:**
```python
# Create time series data
fmri = sns.load_dataset('fmri')

fig, ax = plt.subplots(figsize=(10, 6))

sns.lineplot(data=fmri, x='timepoint', y='signal',
             hue='event', style='region', ax=ax)

ax.set_title('fMRI Signal Over Time')
plt.show()
```

### Regression Plots

**Linear Regression:**
```python
fig, ax = plt.subplots()

sns.regplot(data=tips, x='total_bill', y='tip', ax=ax)

ax.set_title('Linear Regression: Tip vs Total Bill')
plt.show()
```

**Residual Plot:**
```python
fig, ax = plt.subplots()

sns.residplot(data=tips, x='total_bill', y='tip', ax=ax)

ax.set_title('Residual Plot')
ax.axhline(0, color='red', linestyle='--')
plt.show()
```

### Heatmaps and Matrices

**Correlation Heatmap:**
```python
fig, ax = plt.subplots(figsize=(8, 6))

# Compute correlation matrix
corr = tips[['total_bill', 'tip', 'size']].corr()

sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, ax=ax)

ax.set_title('Correlation Matrix')
plt.show()
```

**Clustermap:**
```python
# Hierarchical clustering heatmap
iris = sns.load_dataset('iris')
iris_data = iris.drop('species', axis=1)

g = sns.clustermap(iris_data.T, cmap='viridis',
                   standard_scale=1, figsize=(8, 8))
plt.show()
```

### Multi-Panel Plots

**FacetGrid:**
```python
# Create grid of plots
g = sns.FacetGrid(tips, col='time', row='sex',
                  margin_titles=True, height=4)

# Map plot type to grid
g.map(sns.scatterplot, 'total_bill', 'tip', alpha=0.6)

# Customize
g.set_axis_labels('Total Bill', 'Tip')
g.set_titles(col_template="{col_name}", row_template="{row_name}")
g.add_legend()

plt.show()
```

**PairGrid:**
```python
# Pairwise relationships
iris = sns.load_dataset('iris')

g = sns.PairGrid(iris, hue='species', height=2.5)
g.map_upper(sns.scatterplot)
g.map_lower(sns.kdeplot)
g.map_diag(sns.histplot)
g.add_legend()

plt.show()
```

**Pairplot (simpler):**
```python
sns.pairplot(iris, hue='species', diag_kind='kde', height=2.5)
plt.show()
```

### Themes and Color Palettes

**Setting Themes:**
```python
# Theme styles
sns.set_theme(style='whitegrid')  # white, dark, whitegrid, darkgrid, ticks

# Context (scales elements)
sns.set_context('talk')  # paper, notebook, talk, poster

# Combined
sns.set_theme(style='darkgrid', context='poster',
              palette='deep', font_scale=1.2)
```

**Color Palettes:**
```python
# Qualitative (categorical)
sns.set_palette('deep')  # deep, muted, pastel, bright, dark, colorblind

# Sequential
sns.set_palette('Blues')

# Diverging
sns.set_palette('coolwarm')

# Custom
custom_palette = ['#FF6347', '#4682B4', '#32CD32']
sns.set_palette(custom_palette)

# View palette
sns.palplot(sns.color_palette('husl', 10))
plt.show()
```

## Plotly - Interactive Visualization

### Plotly Express - High-Level API

**Scatter Plot:**
```python
import plotly.express as px

df = px.data.iris()

fig = px.scatter(df, x='sepal_width', y='sepal_length',
                 color='species', size='petal_length',
                 hover_data=['petal_width'],
                 title='Iris Dataset')

fig.show()
```

**Line Plot:**
```python
df = px.data.gapminder()

fig = px.line(df[df['country'] == 'Canada'],
              x='year', y='gdpPercap',
              title='Canada GDP per Capita')

fig.show()
```

**Bar Chart:**
```python
df = px.data.tips()

fig = px.bar(df, x='day', y='total_bill', color='sex',
             barmode='group',  # or 'stack', 'overlay'
             title='Total Bill by Day and Sex')

fig.show()
```

**Histogram:**
```python
fig = px.histogram(df, x='total_bill', color='sex',
                   marginal='box',  # or 'violin', 'rug'
                   title='Total Bill Distribution')

fig.show()
```

**Box Plot:**
```python
fig = px.box(df, x='day', y='total_bill', color='sex',
             title='Total Bill by Day')

fig.show()
```

**Heatmap:**
```python
import numpy as np

z = np.random.randn(20, 20)

fig = px.imshow(z, color_continuous_scale='RdBu_r',
                title='Heatmap')

fig.show()
```

**3D Scatter:**
```python
df = px.data.iris()

fig = px.scatter_3d(df, x='sepal_length', y='sepal_width', z='petal_length',
                    color='species', size='petal_width',
                    title='3D Iris Dataset')

fig.show()
```

**Animated Plots:**
```python
df = px.data.gapminder()

fig = px.scatter(df, x='gdpPercap', y='lifeExp',
                 animation_frame='year',
                 animation_group='country',
                 size='pop', color='continent',
                 hover_name='country',
                 log_x=True, size_max=60,
                 range_x=[100, 100000], range_y=[25, 90],
                 title='Gapminder Data Over Time')

fig.show()
```

### Graph Objects - Low-Level API

**Basic Scatter:**
```python
import plotly.graph_objects as go

x = [1, 2, 3, 4, 5]
y = [1, 4, 9, 16, 25]

fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers+lines'))

fig.update_layout(
    title='Custom Scatter Plot',
    xaxis_title='X Axis',
    yaxis_title='Y Axis'
)

fig.show()
```

**Multiple Traces:**
```python
fig = go.Figure()

# Add multiple traces
fig.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 4, 9],
                         mode='lines', name='Series 1'))
fig.add_trace(go.Scatter(x=[1, 2, 3], y=[2, 5, 10],
                         mode='lines+markers', name='Series 2'))

fig.update_layout(title='Multiple Series')
fig.show()
```

**Subplots:**
```python
from plotly.subplots import make_subplots

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Plot 1', 'Plot 2', 'Plot 3', 'Plot 4')
)

# Add traces to specific subplots
fig.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 4, 9]), row=1, col=1)
fig.add_trace(go.Bar(x=[1, 2, 3], y=[2, 5, 8]), row=1, col=2)
fig.add_trace(go.Scatter(x=[1, 2, 3], y=[3, 6, 9]), row=2, col=1)
fig.add_trace(go.Box(y=[1, 2, 3, 4, 5, 6, 7]), row=2, col=2)

fig.update_layout(height=600, showlegend=False, title_text='Subplots')
fig.show()
```

**Interactive Features:**
```python
fig = go.Figure()

fig.add_trace(go.Scatter(x=[1, 2, 3, 4], y=[10, 11, 12, 13]))

# Add dropdown menu
fig.update_layout(
    updatemenus=[
        dict(
            buttons=list([
                dict(label="Linear",
                     method="relayout",
                     args=[{"yaxis.type": "linear"}]),
                dict(label="Log",
                     method="relayout",
                     args=[{"yaxis.type": "log"}])
            ]),
            direction="down",
        )
    ]
)

fig.show()
```

**Exporting:**
```python
# To HTML
fig.write_html('plot.html')

# To static image (requires kaleido)
fig.write_image('plot.png', width=1200, height=800)
fig.write_image('plot.pdf')
```

## Integration Patterns

### Seaborn with Matplotlib Customization

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Create seaborn plot
fig, ax = plt.subplots(figsize=(10, 6))
sns.boxplot(data=tips, x='day', y='total_bill', ax=ax)

# Customize with matplotlib
ax.set_title('Customized Seaborn Plot', fontsize=16, fontweight='bold')
ax.set_xlabel('Day of Week', fontsize=12)
ax.set_ylabel('Total Bill ($)', fontsize=12)
ax.grid(axis='y', alpha=0.3)

# Add custom annotation
ax.text(0.5, 0.95, 'Custom annotation',
        transform=ax.transAxes, ha='center')

plt.tight_layout()
plt.show()
```

### Plotly with Pandas

```python
import pandas as pd
import plotly.express as px

# Pandas DataFrame
df = pd.DataFrame({
    'x': range(10),
    'y': [i**2 for i in range(10)],
    'category': ['A', 'B'] * 5
})

# Direct plotting from DataFrame
fig = px.line(df, x='x', y='y', color='category')
fig.show()
```

## Best Practices

### 1. Choose Appropriate Plot Type

**Distributions:**
- Histogram, KDE plot, box plot, violin plot
- Use: Understanding data spread and shape

**Comparisons:**
- Bar chart, box plot, strip plot
- Use: Comparing groups or categories

**Relationships:**
- Scatter plot, line plot, regression plot
- Use: Showing correlations or trends

**Compositions:**
- Stacked bar, pie chart (use sparingly), treemap
- Use: Part-to-whole relationships

**Time Series:**
- Line plot, area chart
- Use: Temporal patterns

### 2. Design Principles

**Less is More:**
```python
# Bad: Too much decoration
fig, ax = plt.subplots()
ax.plot(x, y, linewidth=5, color='red', linestyle='--',
        marker='o', markersize=15, markerfacecolor='yellow')
ax.grid(True, linewidth=2, color='blue')
ax.set_facecolor('lightgray')

# Good: Clean and focused
fig, ax = plt.subplots()
ax.plot(x, y, linewidth=2, color='steelblue')
ax.grid(True, alpha=0.3)
```

**Effective Use of Color:**
```python
# Use color purposefully
# - Categorical: Distinct hues
# - Sequential: Single hue gradient
# - Diverging: Two hues from neutral center

# Colorblind-friendly palettes
sns.set_palette('colorblind')
# or
import plotly.express as px
fig = px.scatter(df, x='x', y='y', color='category',
                 color_discrete_sequence=px.colors.qualitative.Safe)
```

**Readable Text:**
```python
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(x, y)

# Clear, appropriately sized text
ax.set_title('Clear Title', fontsize=16, pad=20)
ax.set_xlabel('X Axis Label', fontsize=12)
ax.set_ylabel('Y Axis Label', fontsize=12)
ax.tick_params(labelsize=10)
```

### 3. Consistent Styling

```python
# Set style once at beginning
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style='whitegrid', context='talk')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['figure.dpi'] = 100

# All plots will use these settings
```

### 4. Save High-Quality Figures

```python
# For publications
fig.savefig('figure.pdf', dpi=300, bbox_inches='tight')
fig.savefig('figure.png', dpi=300, bbox_inches='tight')

# For presentations
fig.savefig('figure.png', dpi=150, bbox_inches='tight')

# For web
fig.savefig('figure.png', dpi=96, bbox_inches='tight', optimize=True)
```

## Common Gotchas

### 1. Figure Size and DPI

```python
# ❌ Bad: Default size too small
fig, ax = plt.subplots()

# ✅ Good: Set appropriate size
fig, ax = plt.subplots(figsize=(10, 6))  # Width, height in inches
```

### 2. Overlapping Labels

```python
# ❌ Bad: Labels overlap
fig, ax = plt.subplots()
ax.bar(range(10), values)
ax.set_xticklabels(long_labels)

# ✅ Good: Rotate labels
ax.set_xticklabels(long_labels, rotation=45, ha='right')

# ✅ Better: Use tight_layout
plt.tight_layout()
```

### 3. Color Mapping Consistency

```python
# ❌ Bad: Inconsistent colors across plots
sns.scatterplot(data=df1, x='x', y='y', hue='category')
sns.scatterplot(data=df2, x='x', y='y', hue='category')  # Different colors!

# ✅ Good: Define palette
palette = {'A': 'red', 'B': 'blue', 'C': 'green'}
sns.scatterplot(data=df1, x='x', y='y', hue='category', palette=palette)
sns.scatterplot(data=df2, x='x', y='y', hue='category', palette=palette)
```

### 4. Seaborn Doesn't Return Axes

```python
# ❌ Bad: Expecting return value
ax = sns.boxplot(data=df, x='x', y='y')  # Returns Axes, but...

# ✅ Good: Pass axes explicitly
fig, ax = plt.subplots()
sns.boxplot(data=df, x='x', y='y', ax=ax)
ax.set_title('My Title')  # Now can customize
```

### 5. Plotly Memory with Large Datasets

```python
# ❌ Bad: Plotting millions of points
fig = px.scatter(huge_df)  # Slow, large file

# ✅ Good: Sample or aggregate
fig = px.scatter(huge_df.sample(10000))
# or use datashader for huge datasets
```

### 6. Matplotlib State Machine Confusion

```python
# ❌ Bad: Mixing pyplot and OO API
plt.figure()
ax = plt.gca()
ax.plot(x, y)
plt.xlabel('x')  # State machine call
ax.set_ylabel('y')  # Object-oriented call

# ✅ Good: Be consistent
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y')
```

## Quick Reference

### Matplotlib

```python
# Create figure
fig, ax = plt.subplots(figsize=(10, 6))

# Plot types
ax.plot(x, y)                    # Line
ax.scatter(x, y)                 # Scatter
ax.bar(x, y)                     # Bar
ax.hist(data, bins=30)           # Histogram
ax.boxplot([data1, data2])       # Box plot

# Customization
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_title('Title')
ax.legend()
ax.grid(True)

# Save
fig.savefig('plot.png', dpi=300, bbox_inches='tight')
```

### Seaborn

```python
import seaborn as sns

# Set theme
sns.set_theme(style='whitegrid')

# Plot types
sns.histplot(data=df, x='col')
sns.scatterplot(data=df, x='x', y='y', hue='category')
sns.boxplot(data=df, x='category', y='value')
sns.heatmap(corr_matrix, annot=True)

# Multi-panel
sns.pairplot(df, hue='species')
```

### Plotly Express

```python
import plotly.express as px

# Plot types
fig = px.scatter(df, x='x', y='y', color='category')
fig = px.line(df, x='x', y='y')
fig = px.bar(df, x='x', y='y', color='category')
fig = px.histogram(df, x='value')
fig = px.box(df, x='category', y='value')

# Show/Save
fig.show()
fig.write_html('plot.html')
```

## Installation

```bash
# Matplotlib
pip install matplotlib

# Seaborn
pip install seaborn

# Plotly
pip install plotly

# For static image export (plotly)
pip install kaleido

# All together
pip install matplotlib seaborn plotly kaleido
```

## Additional Resources

- **Matplotlib:** https://matplotlib.org/stable/gallery/index.html
- **Seaborn:** https://seaborn.pydata.org/examples/index.html
- **Plotly:** https://plotly.com/python/
- **Python Graph Gallery:** https://python-graph-gallery.com/
- **From Data to Viz:** https://www.data-to-viz.com/

## Related Skills

- `pycse` - Scientific computing with confidence intervals for plots
- `python-ase` - Atomic structure visualization
- `python-best-practices` - Code quality for visualization scripts

---

### python-regression-statistics

**Path**: `skills/programming/python-regression-statistics/SKILL.md`

# Python Regression & Statistical Analysis

Expert assistant for regression analysis, statistical modeling, and outlier detection in Python. This skill covers statsmodels for statistical regression, scikit-learn for machine learning approaches, scipy for statistical methods, and PyOD for comprehensive outlier detection.

## When to Use This Skill

- Fitting linear and nonlinear regression models with proper statistical inference
- Checking and validating regression assumptions (LINE: Linearity, Independence, Normality, Equal variance)
- Performing model diagnostics: residual analysis, influential points, multicollinearity
- Detecting and handling outliers using statistical, proximity-based, and ensemble methods
- Comparing statistical vs machine learning regression approaches
- Building robust regression pipelines with proper validation
- Time series modeling and forecasting
- Generalized linear models (GLM) for non-normal response variables

## Philosophy: Statistical vs Machine Learning Approaches

### Statistical Regression (statsmodels, scipy.stats)
**Use when:**
- You need p-values, confidence intervals, and hypothesis testing
- Understanding which variables are significant is important
- You want to interpret coefficients causally
- Sample size is moderate and you need inference guarantees
- You need diagnostic tests for assumptions

**Advantages:**
- Rich statistical inference (t-tests, F-tests, likelihood ratios)
- Comprehensive diagnostics and assumption tests
- Formula interface for easy model specification
- Standard errors, confidence intervals, prediction intervals
- Well-understood theoretical properties

**Limitations:**
- Less flexible for complex nonlinear relationships
- Requires careful assumption checking
- May not handle high-dimensional data well
- Less automated feature selection

### Machine Learning Regression (scikit-learn)
**Use when:**
- Prediction accuracy is the primary goal
- You have many features and complex interactions
- Relationships are highly nonlinear
- You don't need statistical inference (p-values)
- You have sufficient data for train/test splits

**Advantages:**
- Handles complex nonlinear patterns
- Built-in regularization (Ridge, Lasso, ElasticNet)
- Excellent for high-dimensional data
- Automated cross-validation and hyperparameter tuning
- Ensemble methods for improved accuracy

**Limitations:**
- No automatic p-values or statistical inference
- Less interpretable coefficients
- Requires more data for complex models
- Black-box nature for some algorithms

## Recommended Workflow

```python
# 1. Exploratory Data Analysis
# - Visualize relationships (scatter plots, pair plots)
# - Check distributions (histograms, Q-Q plots)
# - Identify potential outliers (box plots, scatter plots)
# - Calculate summary statistics

# 2. Initial Outlier Screening (Optional)
# - Use multiple detection methods (IQR, Z-score, isolation forest)
# - Visualize flagged points
# - Investigate why they're outliers (data errors vs real extremes)
# - Document decision to keep or remove

# 3. Check Assumptions (for statistical regression)
# - Linearity: residual vs fitted plots, partial regression plots
# - Independence: Durbin-Watson test, autocorrelation plots
# - Normality: Q-Q plots, Shapiro-Wilk test, histogram of residuals
# - Equal variance: scale-location plot, Breusch-Pagan test

# 4. Model Fitting
# - Start simple (OLS) then add complexity as needed
# - Try transformations if assumptions violated (log, Box-Cox)
# - Consider robust methods if outliers present
# - Use regularization if many predictors (Ridge/Lasso)

# 5. Model Diagnostics
# - Residual analysis
# - Check for influential points (Cook's D, DFFITS, leverage)
# - Multicollinearity (VIF)
# - Outliers in predictor space vs response space

# 6. Validation
# - Train/test split or cross-validation
# - Compare multiple models (AIC, BIC, R², RMSE)
# - Check prediction intervals (statistical) or residual distribution (ML)

# 7. Interpretation & Reporting
# - Coefficient interpretation with confidence intervals
# - Feature importance (if ML model)
# - Visualize predictions vs actuals
# - Report diagnostics and assumption checks
```

---

## Statistical Regression with statsmodels

### Ordinary Least Squares (OLS)

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import pandas as pd

# Arrays API - more control
X = sm.add_constant(X_raw)  # Add intercept
model = sm.OLS(y, X)
results = model.fit()
print(results.summary())

# Formula API - easier specification
df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'category': cat})
model = smf.ols('y ~ x1 + x2 + C(category)', data=df)
results = model.fit()

# Access key results
results.params           # Coefficients
results.pvalues         # P-values
results.conf_int()      # Confidence intervals
results.rsquared        # R²
results.rsquared_adj    # Adjusted R²
results.aic, results.bic  # Information criteria
results.resid           # Residuals
results.fittedvalues    # Predicted values
```

### Weighted Least Squares (WLS)

```python
# When heteroskedasticity is present
# Weights = 1/variance of each observation

# If you know the variance structure
weights = 1 / variance_array
model = sm.WLS(y, X, weights=weights)
results = model.fit()

# Iteratively Reweighted Least Squares
# Start with OLS, estimate variance function, refit
ols_results = sm.OLS(y, X).fit()
resid_squared = ols_results.resid**2

# Fit variance as function of predictors
variance_model = sm.OLS(np.log(resid_squared), X).fit()
weights = 1 / np.exp(variance_model.fittedvalues)

wls_results = sm.WLS(y, X, weights=weights).fit()
```

### Robust Regression (RLM)

```python
# Resistant to outliers - downweights influential points
import statsmodels.robust.robust_linear_model as rlm

# Huber's T norm (default)
model = rlm.RLM(y, X, M=sm.robust.norms.HuberT())
results = model.fit()

# Other M-estimators
# TukeyBiweight - more aggressive outlier downweighting
model = rlm.RLM(y, X, M=sm.robust.norms.TukeyBiweight())
results = model.fit()

# Check weights assigned to each observation
weights = results.weights  # Outliers get lower weights
```

### Generalized Linear Models (GLM)

```python
# For non-normal response variables

# Poisson regression (count data)
model = sm.GLM(y, X, family=sm.families.Poisson())
results = model.fit()

# Negative Binomial (overdispersed counts)
model = sm.GLM(y, X, family=sm.families.NegativeBinomial())
results = model.fit()

# Gamma regression (positive continuous data)
model = sm.GLM(y, X, family=sm.families.Gamma())
results = model.fit()

# Binomial/Logistic regression
model = sm.GLM(y, X, family=sm.families.Binomial())
results = model.fit()

# Link functions
# family=sm.families.Gaussian(link=sm.families.links.log())
```

### Time Series Models

```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.vector_ar.var_model import VAR

# ARIMA (AutoRegressive Integrated Moving Average)
model = ARIMA(y, order=(p, d, q))  # p=AR order, d=differencing, q=MA order
results = model.fit()

# Seasonal ARIMA
model = SARIMAX(y, order=(p, d, q), seasonal_order=(P, D, Q, s))
results = model.fit()

# Forecasting
forecast = results.forecast(steps=10)
forecast_with_ci = results.get_forecast(steps=10)
forecast_df = forecast_with_ci.summary_frame()

# Vector Autoregression (multivariate time series)
model = VAR(df[['y1', 'y2', 'y3']])
results = model.fit(maxlags=5)
results.forecast(df.values[-results.k_ar:], steps=10)
```

### Model Diagnostics - statsmodels

```python
# Comprehensive diagnostic plots
from statsmodels.graphics.gofplots import qqplot
import matplotlib.pyplot as plt

# 1. Residual vs Fitted
plt.scatter(results.fittedvalues, results.resid)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
# Should show random scatter, no pattern

# 2. Q-Q plot for normality
qqplot(results.resid, line='s')
# Points should follow diagonal line

# 3. Scale-Location (heteroskedasticity)
plt.scatter(results.fittedvalues, np.sqrt(np.abs(results.resid_pearson)))
plt.xlabel('Fitted values')
plt.ylabel('√|Standardized Residuals|')
# Should show constant spread

# 4. Residuals vs Leverage
from statsmodels.stats.outliers_influence import OLSInfluence
influence = OLSInfluence(results)
fig, ax = plt.subplots()
ax.scatter(influence.hat_matrix_diag, results.resid_pearson)
# Identifies influential points (high leverage + large residual)

# Statistical tests
from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox
from statsmodels.stats.stattools import durbin_watson

# Heteroskedasticity test
lm_stat, lm_pval, f_stat, f_pval = het_breuschpagan(results.resid, results.model.exog)
print(f"Breusch-Pagan p-value: {lm_pval}")  # p < 0.05 indicates heteroskedasticity

# Autocorrelation
dw = durbin_watson(results.resid)
print(f"Durbin-Watson: {dw}")  # Should be ~2 for no autocorrelation

# Autocorrelation test
lb_test = acorr_ljungbox(results.resid, lags=10)

# Normality test
from scipy.stats import shapiro
stat, pval = shapiro(results.resid)
print(f"Shapiro-Wilk p-value: {pval}")  # p < 0.05 suggests non-normality
```

### Influential Points and Multicollinearity

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence

# Variance Inflation Factor (multicollinearity)
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data)
# VIF > 10 indicates serious multicollinearity
# VIF > 5 is concerning

# Influence measures
influence = OLSInfluence(results)

# Cook's Distance (overall influence)
cooks_d = influence.cooks_distance[0]
# Rule of thumb: D > 4/n is influential

# DFFITS (influence on fitted value)
dffits = influence.dffits[0]
# Rule of thumb: |DFFITS| > 2√(p/n) is influential

# Leverage (hat values)
leverage = influence.hat_matrix_diag
# Rule of thumb: h > 2p/n or h > 3p/n is high leverage

# Studentized residuals
student_resid = influence.resid_studentized_internal
# |r| > 2 or 3 suggests outlier in response space

# Summary table
influence_summary = influence.summary_frame()
print(influence_summary)
```

---

## Machine Learning Regression with scikit-learn

### Linear Models with Regularization

```python
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet,
    Lars, LassoLars, BayesianRidge, HuberRegressor,
    RANSACRegressor, TheilSenRegressor
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, GridSearchCV

# Standard Linear Regression
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
model.coef_, model.intercept_

# Ridge Regression (L2 regularization)
# Good when many correlated predictors
ridge = Ridge(alpha=1.0)  # Higher alpha = more regularization
ridge.fit(X_train, y_train)

# Lasso Regression (L1 regularization)
# Performs feature selection by setting coefficients to zero
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
# Check which features selected
selected_features = X.columns[lasso.coef_ != 0]

# ElasticNet (L1 + L2)
# l1_ratio=1.0 is Lasso, l1_ratio=0.0 is Ridge
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)

# Huber Regression (robust to outliers)
huber = HuberRegressor(epsilon=1.35)
huber.fit(X_train, y_train)

# RANSAC (extremely robust, finds inlier subset)
ransac = RANSACRegressor(min_samples=0.5, residual_threshold=2.0)
ransac.fit(X_train, y_train)
inlier_mask = ransac.inlier_mask_

# Theil-Sen (robust, good for small datasets)
theil = TheilSenRegressor()
theil.fit(X_train, y_train)
```

### Cross-Validation and Hyperparameter Tuning

```python
from sklearn.model_selection import (
    cross_val_score, cross_validate,
    GridSearchCV, RandomizedSearchCV,
    KFold, RepeatedKFold
)
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error,
    r2_score, mean_absolute_percentage_error
)

# Simple cross-validation
scores = cross_val_score(model, X, y, cv=5,
                         scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)
print(f"RMSE: {rmse_scores.mean():.3f} (+/- {rmse_scores.std():.3f})")

# Multiple metrics
cv_results = cross_validate(
    model, X, y, cv=5,
    scoring=['neg_mean_squared_error', 'r2', 'neg_mean_absolute_error'],
    return_train_score=True
)

# Grid search for optimal hyperparameters
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],
    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
}
grid_search = GridSearchCV(
    ElasticNet(),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {np.sqrt(-grid_search.best_score_):.3f}")

# Use best model
best_model = grid_search.best_estimator_
```

### Tree-Based and Ensemble Methods

```python
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor,
    AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
)
from sklearn.tree import DecisionTreeRegressor

# Random Forest
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# Feature importance
importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Gradient Boosting
gb = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    random_state=42
)
gb.fit(X_train, y_train)

# XGBoost (if installed)
try:
    import xgboost as xgb
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    )
    xgb_model.fit(X_train, y_train)
except ImportError:
    print("XGBoost not installed. Use: pip install xgboost")
```

### Other ML Regression Methods

```python
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

# Support Vector Regression
svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)
svr.fit(X_train, y_train)

# Kernel Ridge Regression
kr = KernelRidge(alpha=1.0, kernel='rbf')
kr.fit(X_train, y_train)

# K-Nearest Neighbors
knn = KNeighborsRegressor(n_neighbors=5, weights='distance')
knn.fit(X_train, y_train)

# Gaussian Process (with uncertainty quantification)
kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)
gp = GaussianProcessRegressor(kernel=kernel, random_state=42)
gp.fit(X_train, y_train)

# Predictions with uncertainty
y_pred, sigma = gp.predict(X_test, return_std=True)
```

### Building Complete Pipelines

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.compose import TransformedTargetRegressor

# Standard pipeline with scaling
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge(alpha=1.0))
])
pipeline.fit(X_train, y_train)

# Polynomial features
poly_pipeline = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('scaler', StandardScaler()),
    ('model', Ridge(alpha=1.0))
])

# Target transformation (e.g., log transform y)
log_model = TransformedTargetRegressor(
    regressor=Ridge(alpha=1.0),
    func=np.log1p,
    inverse_func=np.expm1
)
log_model.fit(X_train, y_train)
```

---

## Outlier Detection

### Statistical Methods (scipy, statsmodels)

```python
import numpy as np
from scipy import stats

# Z-score method
def detect_outliers_zscore(data, threshold=3):
    """
    Outliers are points with |z-score| > threshold
    Works well for normally distributed data
    """
    z_scores = np.abs(stats.zscore(data))
    return z_scores > threshold

# Modified Z-score (MAD - Median Absolute Deviation)
def detect_outliers_mad(data, threshold=3.5):
    """
    More robust than Z-score, uses median instead of mean
    Recommended threshold: 3.5
    """
    median = np.median(data)
    mad = np.median(np.abs(data - median))
    modified_z_scores = 0.6745 * (data - median) / mad
    return np.abs(modified_z_scores) > threshold

# IQR method
def detect_outliers_iqr(data, factor=1.5):
    """
    Outliers are below Q1 - factor*IQR or above Q3 + factor*IQR
    factor=1.5: outliers, factor=3.0: extreme outliers
    """
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    iqr = q3 - q1
    lower_bound = q1 - factor * iqr
    upper_bound = q3 + factor * iqr
    return (data < lower_bound) | (data > upper_bound)

# Grubbs test (for single outlier in normal data)
def grubbs_test(data, alpha=0.05):
    """
    Tests if maximum or minimum is an outlier
    Assumes data is normally distributed
    """
    from scipy.stats import t
    n = len(data)
    mean = np.mean(data)
    std = np.std(data, ddof=1)

    # Test statistic
    G_max = np.max(np.abs(data - mean)) / std

    # Critical value
    t_crit = t.ppf(1 - alpha / (2 * n), n - 2)
    G_crit = ((n - 1) / np.sqrt(n)) * np.sqrt(t_crit**2 / (n - 2 + t_crit**2))

    return G_max > G_crit

# Example usage
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])  # 100 is outlier
outliers_z = detect_outliers_zscore(data)
outliers_mad = detect_outliers_mad(data)
outliers_iqr = detect_outliers_iqr(data)
```

### PyOD - Comprehensive Outlier Detection

PyOD (Python Outlier Detection) provides 40+ algorithms. Install with: `pip install pyod`

```python
from pyod.models.knn import KNN
from pyod.models.lof import LOF
from pyod.models.iforest import IForest
from pyod.models.ocsvm import OCSVM
from pyod.models.pca import PCA as PCA_OD
from pyod.models.mcd import MCD
from pyod.models.abod import ABOD
from pyod.models.copod import COPOD
from pyod.models.ecod import ECOD
from pyod.models.combination import average, maximization

# General pattern for PyOD detectors
detector = KNN(contamination=0.1)  # Expect 10% outliers
detector.fit(X)

# Get outlier labels (0=inlier, 1=outlier)
outlier_labels = detector.labels_

# Get outlier scores (higher = more outlier-like)
outlier_scores = detector.decision_scores_

# Predict on new data
new_labels = detector.predict(X_new)
new_scores = detector.decision_function(X_new)
```

#### Proximity-Based Detectors

```python
# K-Nearest Neighbors (KNN)
# Simple, fast, good baseline
knn = KNN(contamination=0.1, n_neighbors=5, method='mean')
knn.fit(X)

# Local Outlier Factor (LOF)
# Captures local density deviations
# Good for datasets with varying densities
lof = LOF(contamination=0.1, n_neighbors=20)
lof.fit(X)

# Connectivity-Based Outlier Factor (COF)
from pyod.models.cof import COF
cof = COF(contamination=0.1, n_neighbors=20)
cof.fit(X)
```

#### Probabilistic Detectors

```python
# COPOD (Copula-Based Outlier Detection)
# Fast, parameter-free, works well on high-dimensional data
copod = COPOD(contamination=0.1)
copod.fit(X)

# ECOD (Empirical Cumulative Distribution)
# Very fast, no parameters, good for tabular data
ecod = ECOD(contamination=0.1)
ecod.fit(X)

# ABOD (Angle-Based Outlier Detection)
# Effective in high-dimensional spaces
# Slow on large datasets
abod = ABOD(contamination=0.1)
abod.fit(X)
```

#### Linear Model Detectors

```python
# PCA-based detection
# Outliers have large reconstruction error
pca = PCA_OD(contamination=0.1, n_components=None)
pca.fit(X)

# MCD (Minimum Covariance Determinant)
# Robust covariance estimation
# Assumes data is roughly Gaussian
mcd = MCD(contamination=0.1)
mcd.fit(X)

# One-Class SVM
# Learns boundary around normal points
ocsvm = OCSVM(contamination=0.1, kernel='rbf', nu=0.1)
ocsvm.fit(X)
```

#### Isolation-Based Detectors

```python
# Isolation Forest
# Fast, scalable, effective for high-dimensional data
# Isolates outliers by random partitioning
iforest = IForest(
    contamination=0.1,
    n_estimators=100,
    max_features=1.0,
    random_state=42
)
iforest.fit(X)
```

#### Ensemble Methods

```python
# Combine multiple detectors
from pyod.models.lscp import LSCP
from pyod.models.feature_bagging import FeatureBagging

# Feature Bagging
# Trains multiple detectors on random feature subsets
fb = FeatureBagging(
    base_estimator=LOF(contamination=0.1),
    n_estimators=10,
    contamination=0.1,
    random_state=42
)
fb.fit(X)

# Average multiple detector scores
detectors = [KNN(), LOF(), IForest(), COPOD()]
scores_list = []
for detector in detectors:
    detector.fit(X)
    scores_list.append(detector.decision_scores_)

# Average combination
avg_scores = average(scores_list)

# Maximum combination (more conservative)
max_scores = maximization(scores_list)
```

### scikit-learn Outlier Detection

```python
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.svm import OneClassSVM

# Isolation Forest
iso_forest = IsolationForest(
    contamination=0.1,
    random_state=42,
    n_estimators=100
)
outlier_labels = iso_forest.fit_predict(X)  # -1=outlier, 1=inlier
outlier_scores = iso_forest.score_samples(X)  # More negative = more outlier

# Local Outlier Factor (novelty detection mode)
lof = LocalOutlierFactor(
    n_neighbors=20,
    contamination=0.1,
    novelty=True  # Enables predict on new data
)
lof.fit(X_train)
outlier_labels = lof.predict(X_test)
outlier_scores = lof.score_samples(X_test)

# Elliptic Envelope (assumes Gaussian distribution)
elliptic = EllipticEnvelope(contamination=0.1, random_state=42)
outlier_labels = elliptic.fit_predict(X)

# One-Class SVM
oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)
outlier_labels = oc_svm.fit_predict(X)
```

### Choosing the Right Outlier Detection Method

**Use IQR or Z-score when:**
- Univariate data (single variable)
- Quick screening needed
- Interpretability is important

**Use Modified Z-score (MAD) when:**
- Data may already contain outliers (MAD is robust)
- Distribution is roughly symmetric

**Use Isolation Forest when:**
- High-dimensional data
- No assumptions about data distribution
- Fast computation needed
- Good all-purpose choice

**Use LOF when:**
- Varying density clusters in data
- Local structure is important
- Willing to tune n_neighbors parameter

**Use COPOD/ECOD when:**
- Very fast detection needed
- High-dimensional tabular data
- Don't want to tune parameters

**Use PCA-based when:**
- Data lies in lower-dimensional manifold
- Want to understand which features contribute to outlierness

**Use MCD when:**
- Multivariate Gaussian assumption reasonable
- Robust covariance estimation needed

**Use ensemble methods when:**
- Maximum robustness needed
- Can afford computational cost
- Different detector types disagree

---

## Complete Workflow Examples

### Example 1: Statistical Regression with Full Diagnostics

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.graphics.gofplots import qqplot
from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('data.csv')

# 1. Exploratory analysis
print(df.describe())
df.hist(bins=30, figsize=(15, 10))
pd.plotting.scatter_matrix(df, figsize=(15, 15))

# 2. Fit initial model
model = smf.ols('price ~ area + bedrooms + age + location', data=df)
results = model.fit()
print(results.summary())

# 3. Check assumptions
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Residuals vs Fitted
axes[0, 0].scatter(results.fittedvalues, results.resid)
axes[0, 0].axhline(y=0, color='r', linestyle='--')
axes[0, 0].set_xlabel('Fitted values')
axes[0, 0].set_ylabel('Residuals')
axes[0, 0].set_title('Residuals vs Fitted')

# Q-Q plot
qqplot(results.resid, line='s', ax=axes[0, 1])
axes[0, 1].set_title('Normal Q-Q')

# Scale-Location
axes[1, 0].scatter(results.fittedvalues, np.sqrt(np.abs(results.resid_pearson)))
axes[1, 0].set_xlabel('Fitted values')
axes[1, 0].set_ylabel('√|Standardized Residuals|')
axes[1, 0].set_title('Scale-Location')

# Residuals vs Leverage
influence = OLSInfluence(results)
axes[1, 1].scatter(influence.hat_matrix_diag, results.resid_pearson)
axes[1, 1].set_xlabel('Leverage')
axes[1, 1].set_ylabel('Standardized Residuals')
axes[1, 1].set_title('Residuals vs Leverage')

plt.tight_layout()

# 4. Statistical tests
# Heteroskedasticity
lm_stat, lm_pval, f_stat, f_pval = het_breuschpagan(results.resid, results.model.exog)
print(f"\nBreusch-Pagan test p-value: {lm_pval:.4f}")

# Autocorrelation
dw = durbin_watson(results.resid)
print(f"Durbin-Watson statistic: {dw:.4f}")

# 5. Check multicollinearity
X = df[['area', 'bedrooms', 'age']]  # Exclude categorical
X = sm.add_constant(X)
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print("\n", vif_data)

# 6. Identify influential points
influence_df = influence.summary_frame()
print("\nInfluential points (Cook's D > 4/n):")
n = len(df)
influential = influence_df[influence_df['cooks_d'] > 4/n]
print(influential[['cooks_d', 'student_resid', 'hat_diag']])

# 7. If assumptions violated, try transformations or robust regression
if lm_pval < 0.05:  # Heteroskedasticity detected
    print("\nFitting robust regression (RLM)...")
    from statsmodels.robust.robust_linear_model import RLM
    robust_model = RLM(df['price'], X, M=sm.robust.norms.HuberT())
    robust_results = robust_model.fit()
    print(robust_results.summary())
```

### Example 2: ML Regression with Cross-Validation

```python
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import pandas as pd
import numpy as np

# Load and split data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define models to compare
models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'RandomForest': RandomForestRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42)
}

# Cross-validation comparison
results = {}
for name, model in models.items():
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    scores = cross_val_score(
        pipeline, X_train, y_train,
        cv=5, scoring='neg_mean_squared_error'
    )
    rmse_scores = np.sqrt(-scores)
    results[name] = {
        'mean_rmse': rmse_scores.mean(),
        'std_rmse': rmse_scores.std()
    }
    print(f"{name}: RMSE = {rmse_scores.mean():.3f} (+/- {rmse_scores.std():.3f})")

# Hyperparameter tuning for best model
param_grid = {
    'model__n_estimators': [50, 100, 200],
    'model__max_depth': [5, 10, 15, None],
    'model__min_samples_split': [2, 5, 10]
}

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestRegressor(random_state=42))
])

grid_search = GridSearchCV(
    pipeline, param_grid, cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best CV RMSE: {np.sqrt(-grid_search.best_score_):.3f}")

# Evaluate on test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"\nTest set performance:")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.3f}")
print(f"R²: {r2_score(y_test, y_pred):.3f}")

# Feature importance
if hasattr(best_model.named_steps['model'], 'feature_importances_'):
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.named_steps['model'].feature_importances_
    }).sort_values('importance', ascending=False)
    print("\nTop 10 features:")
    print(importance_df.head(10))
```

### Example 3: Outlier Detection Pipeline

```python
import numpy as np
import pandas as pd
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.copod import COPOD
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1).values

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Multiple detection methods
detectors = {
    'Isolation Forest': IForest(contamination=0.1, random_state=42),
    'LOF': LOF(contamination=0.1, n_neighbors=20),
    'COPOD': COPOD(contamination=0.1)
}

# Fit detectors and collect scores
scores_df = pd.DataFrame(index=df.index)
labels_df = pd.DataFrame(index=df.index)

for name, detector in detectors.items():
    detector.fit(X_scaled)
    scores_df[name] = detector.decision_scores_
    labels_df[name] = detector.labels_

# Consensus: point is outlier if flagged by at least 2 methods
consensus_outliers = (labels_df.sum(axis=1) >= 2)
print(f"Outliers detected by consensus: {consensus_outliers.sum()}")

# Investigate outliers
outlier_indices = df[consensus_outliers].index
print("\nOutlier summary statistics:")
print(df.loc[outlier_indices].describe())
print("\nNormal data summary statistics:")
print(df.loc[~consensus_outliers].describe())

# Visualize outlier scores
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for i, (name, scores) in enumerate(scores_df.items()):
    axes[i].hist(scores, bins=50, alpha=0.7)
    threshold = np.percentile(scores, 90)  # 90th percentile
    axes[i].axvline(threshold, color='r', linestyle='--', label='Threshold')
    axes[i].set_title(f'{name} Scores')
    axes[i].set_xlabel('Outlier Score')
    axes[i].set_ylabel('Frequency')
    axes[i].legend()
plt.tight_layout()

# Decision: remove, investigate, or keep?
# Option 1: Remove outliers
df_clean = df[~consensus_outliers].copy()

# Option 2: Use robust regression that handles outliers
# (see RLM, Huber, RANSAC examples above)

# Option 3: Keep but analyze separately
df['is_outlier'] = consensus_outliers
```

### Example 4: Combined Regression + Outlier Detection

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from pyod.models.iforest import IForest
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('data.csv')
X = df[['x1', 'x2', 'x3']].values
y = df['y'].values

# Step 1: Detect outliers in predictor space
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

detector = IForest(contamination=0.05, random_state=42)
detector.fit(X_scaled)
outliers_X = detector.labels_ == 1

print(f"Outliers in predictor space: {outliers_X.sum()}")

# Step 2: Fit initial model
X_with_const = sm.add_constant(X)
model = sm.OLS(y, X_with_const)
results = model.fit()

# Step 3: Identify outliers in response space
from statsmodels.stats.outliers_influence import OLSInfluence
influence = OLSInfluence(results)
studentized_resid = influence.resid_studentized_internal
outliers_y = np.abs(studentized_resid) > 3

print(f"Outliers in response space: {outliers_y.sum()}")

# Step 4: Identify influential points
cooks_d = influence.cooks_distance[0]
n, p = X_with_const.shape
influential = cooks_d > 4/n

print(f"Influential points: {influential.sum()}")

# Step 5: Categorize points
df['outlier_X'] = outliers_X
df['outlier_y'] = outliers_y
df['influential'] = influential
df['any_flag'] = outliers_X | outliers_y | influential

# Step 6: Visualize
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Residuals vs Fitted
colors = ['red' if flag else 'blue' for flag in df['any_flag']]
axes[0].scatter(results.fittedvalues, results.resid, c=colors, alpha=0.6)
axes[0].axhline(y=0, color='black', linestyle='--')
axes[0].set_xlabel('Fitted values')
axes[0].set_ylabel('Residuals')
axes[0].set_title('Residuals vs Fitted (red = flagged)')

# Plot 2: Cook's distance
axes[1].stem(range(len(cooks_d)), cooks_d, markerfmt=',')
axes[1].axhline(y=4/n, color='r', linestyle='--', label='Threshold (4/n)')
axes[1].set_xlabel('Observation index')
axes[1].set_ylabel("Cook's distance")
axes[1].set_title("Cook's Distance")
axes[1].legend()

plt.tight_layout()

# Step 7: Compare models
print("\n=== Original Model ===")
print(results.summary())

# Remove flagged points and refit
df_clean = df[~df['any_flag']].copy()
X_clean = df_clean[['x1', 'x2', 'x3']].values
y_clean = df_clean['y'].values
X_clean_const = sm.add_constant(X_clean)

model_clean = sm.OLS(y_clean, X_clean_const)
results_clean = model_clean.fit()

print("\n=== Model After Removing Flagged Points ===")
print(results_clean.summary())

# Alternative: Robust regression (keeps all data)
from statsmodels.robust.robust_linear_model import RLM
robust_model = RLM(y, X_with_const, M=sm.robust.norms.HuberT())
robust_results = robust_model.fit()

print("\n=== Robust Regression (All Data) ===")
print(robust_results.summary())
```

---

## Common Pitfalls and Solutions

### 1. P-hacking and Multiple Testing

**Problem:** Testing many models/features and reporting only significant ones.

**Solution:**
- Pre-specify your model based on theory/prior knowledge
- Use adjusted p-values (Bonferroni, Benjamini-Hochberg) for multiple comparisons
- Focus on effect sizes and confidence intervals, not just p-values
- Use train/test split or cross-validation for model selection

```python
from statsmodels.stats.multitest import multipletests

# Multiple hypothesis testing correction
p_values = [0.04, 0.03, 0.001, 0.15, 0.08]
reject, pvals_corrected, _, _ = multipletests(p_values, method='fdr_bh')
# Benjamini-Hochberg controls false discovery rate
```

### 2. Overfitting with Many Features

**Problem:** Model fits training data perfectly but generalizes poorly.

**Solution:**
- Use regularization (Ridge, Lasso, ElasticNet)
- Cross-validation to estimate generalization performance
- Feature selection based on domain knowledge
- Simpler models when sample size is small

```python
# Rule of thumb: need at least 10-20 observations per predictor
n_samples, n_features = X.shape
if n_samples < 10 * n_features:
    print(f"Warning: Only {n_samples/n_features:.1f} samples per feature")
    print("Consider: (1) regularization, (2) feature selection, (3) collect more data")
```

### 3. Ignoring Assumptions

**Problem:** Using OLS when assumptions are violated leads to invalid inference.

**Solution:**
- Always check diagnostics (residual plots, Q-Q plots, tests)
- Transform variables if needed (log, Box-Cox, square root)
- Use robust methods when outliers present
- Use WLS when heteroskedasticity detected
- Use GLM when response distribution is non-normal

```python
# Box-Cox transformation for normality
from scipy.stats import boxcox

y_transformed, lambda_param = boxcox(y)  # Only works for positive y
print(f"Optimal lambda: {lambda_param}")
# lambda=0: log transform
# lambda=0.5: square root
# lambda=1: no transform needed
```

### 4. Removing Outliers Without Justification

**Problem:** Arbitrarily removing points to improve fit.

**Solution:**
- Investigate WHY points are outliers (data errors vs real extremes)
- Document all data cleaning decisions
- Try robust methods before removing
- Compare results with/without outliers
- Never remove points just because they don't fit the model

```python
# Systematic approach to outlier investigation
def investigate_outliers(df, outlier_mask):
    """
    Print detailed information about detected outliers
    """
    outliers = df[outlier_mask]
    normal = df[~outlier_mask]

    print(f"Number of outliers: {len(outliers)} ({100*len(outliers)/len(df):.1f}%)")
    print("\nOutlier characteristics:")
    print(outliers.describe())
    print("\nNormal data characteristics:")
    print(normal.describe())

    # Check for data entry errors
    print("\nPotential data errors:")
    for col in df.columns:
        if df[col].dtype in [np.float64, np.int64]:
            min_val, max_val = df[col].min(), df[col].max()
            suspicious = (outliers[col] < min_val * 0.1) | (outliers[col] > max_val * 0.9)
            if suspicious.any():
                print(f"  {col}: {suspicious.sum()} suspicious values")
```

### 5. Confusing Outliers vs Influential Points

**Problem:** Not all outliers are influential, and not all influential points are outliers.

**Concepts:**
- **Outlier in X-space:** Unusual predictor values (high leverage)
- **Outlier in Y-space:** Unusual response value (large residual)
- **Influential point:** Removing it significantly changes model (high Cook's D)

**Solution:**
- Check multiple diagnostics: leverage, residuals, Cook's D
- High leverage + small residual = not very influential
- Low leverage + large residual = not very influential
- High leverage + large residual = very influential

```python
# Categorize points
def categorize_points(influence, results, n, p):
    """
    Categorize observations based on influence measures
    """
    leverage = influence.hat_matrix_diag
    studentized_resid = influence.resid_studentized_internal
    cooks_d = influence.cooks_distance[0]

    high_leverage = leverage > 2 * p / n
    outlier_y = np.abs(studentized_resid) > 3
    influential = cooks_d > 4 / n

    categories = []
    for i in range(n):
        if influential[i]:
            categories.append('Influential')
        elif high_leverage[i] and outlier_y[i]:
            categories.append('High Leverage Outlier')
        elif high_leverage[i]:
            categories.append('High Leverage')
        elif outlier_y[i]:
            categories.append('Outlier')
        else:
            categories.append('Normal')

    return pd.Series(categories, name='Category')
```

### 6. Not Checking Prediction Intervals

**Problem:** Reporting point predictions without uncertainty.

**Solution:**
- Use prediction intervals (statistical models) or quantile regression
- Report confidence intervals for coefficients
- Visualize uncertainty in predictions

```python
# Prediction intervals with statsmodels
predictions = results.get_prediction(X_new)
pred_df = predictions.summary_frame(alpha=0.05)  # 95% intervals

# pred_df has columns: mean, mean_se, mean_ci_lower, mean_ci_upper,
#                       obs_ci_lower, obs_ci_upper

# Confidence interval: uncertainty in mean prediction
# Prediction interval: where we expect new observation to fall (wider)

import matplotlib.pyplot as plt
plt.scatter(X_test, y_test, label='Actual')
plt.plot(X_test, pred_df['mean'], 'r-', label='Prediction')
plt.fill_between(X_test, pred_df['obs_ci_lower'], pred_df['obs_ci_upper'],
                 alpha=0.2, label='95% Prediction Interval')
plt.legend()
```

---

## Visualization Best Practices

### Diagnostic Plots

```python
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.gofplots import qqplot

def plot_diagnostics(results, figsize=(12, 10)):
    """
    Create comprehensive diagnostic plots for regression
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)

    # 1. Residuals vs Fitted
    axes[0, 0].scatter(results.fittedvalues, results.resid, alpha=0.6)
    axes[0, 0].axhline(y=0, color='r', linestyle='--')
    axes[0, 0].set_xlabel('Fitted values')
    axes[0, 0].set_ylabel('Residuals')
    axes[0, 0].set_title('Residuals vs Fitted')

    # Add lowess smooth
    from statsmodels.nonparametric.smoothers_lowess import lowess
    smoothed = lowess(results.resid, results.fittedvalues, frac=0.3)
    axes[0, 0].plot(smoothed[:, 0], smoothed[:, 1], 'g-', lw=2)

    # 2. Q-Q plot
    qqplot(results.resid, line='s', ax=axes[0, 1])
    axes[0, 1].set_title('Normal Q-Q')

    # 3. Scale-Location
    resid_std = np.sqrt(np.abs(results.resid_pearson))
    axes[1, 0].scatter(results.fittedvalues, resid_std, alpha=0.6)
    axes[1, 0].set_xlabel('Fitted values')
    axes[1, 0].set_ylabel('√|Standardized Residuals|')
    axes[1, 0].set_title('Scale-Location')
    smoothed = lowess(resid_std, results.fittedvalues, frac=0.3)
    axes[1, 0].plot(smoothed[:, 0], smoothed[:, 1], 'r-', lw=2)

    # 4. Residuals vs Leverage
    from statsmodels.stats.outliers_influence import OLSInfluence
    influence = OLSInfluence(results)
    leverage = influence.hat_matrix_diag
    cooks_d = influence.cooks_distance[0]

    axes[1, 1].scatter(leverage, results.resid_pearson, alpha=0.6)
    axes[1, 1].set_xlabel('Leverage')
    axes[1, 1].set_ylabel('Standardized Residuals')
    axes[1, 1].set_title('Residuals vs Leverage')

    # Highlight high Cook's D points
    n = len(results.resid)
    high_cooks = cooks_d > 4/n
    axes[1, 1].scatter(leverage[high_cooks], results.resid_pearson[high_cooks],
                       color='red', s=100, alpha=0.8, label="High Cook's D")
    axes[1, 1].legend()

    plt.tight_layout()
    return fig

# Usage
# fig = plot_diagnostics(results)
# plt.show()
```

### Outlier Score Distributions

```python
def plot_outlier_scores(detectors_dict, X, figsize=(15, 5)):
    """
    Plot outlier score distributions for multiple detectors
    """
    n_detectors = len(detectors_dict)
    fig, axes = plt.subplots(1, n_detectors, figsize=figsize)
    if n_detectors == 1:
        axes = [axes]

    for ax, (name, detector) in zip(axes, detectors_dict.items()):
        detector.fit(X)
        scores = detector.decision_scores_
        labels = detector.labels_

        # Histogram
        ax.hist(scores[labels == 0], bins=50, alpha=0.6, label='Inliers')
        ax.hist(scores[labels == 1], bins=50, alpha=0.6, label='Outliers')
        ax.set_xlabel('Outlier Score')
        ax.set_ylabel('Frequency')
        ax.set_title(name)
        ax.legend()

    plt.tight_layout()
    return fig
```

---

## Quick Reference

### When to Use Which Method

| Goal | Method | Library |
|------|--------|---------|
| Simple linear regression with inference | `OLS` | statsmodels |
| Regression with correlated predictors | `Ridge` | sklearn |
| Feature selection in regression | `Lasso` | sklearn |
| Robust to outliers | `RLM`, `HuberRegressor`, `RANSACRegressor` | statsmodels, sklearn |
| Count data | `GLM(Poisson)`, `GLM(NegativeBinomial)` | statsmodels |
| Time series forecasting | `ARIMA`, `SARIMAX` | statsmodels |
| High-dimensional data | `Lasso`, `ElasticNet`, `Ridge` | sklearn |
| Complex nonlinear patterns | `RandomForest`, `GradientBoosting` | sklearn |
| Fast outlier detection | `COPOD`, `ECOD`, `IsolationForest` | PyOD, sklearn |
| Robust outlier detection | `LOF`, `Ensemble methods` | PyOD, sklearn |
| Univariate outliers | `IQR`, `MAD`, `Z-score` | scipy |

### Key Diagnostics Checklist

- [ ] **Linearity:** Residual vs fitted plot shows random scatter
- [ ] **Independence:** Durbin-Watson ≈ 2, no autocorrelation
- [ ] **Normality:** Q-Q plot follows diagonal, Shapiro-Wilk p > 0.05
- [ ] **Equal variance:** Scale-location plot shows constant spread, Breusch-Pagan p > 0.05
- [ ] **Multicollinearity:** All VIF < 5 (ideally < 10)
- [ ] **Outliers:** Identify using |studentized residual| > 3
- [ ] **Influential points:** Cook's D < 4/n, check DFFITS
- [ ] **Model comparison:** Use AIC, BIC, cross-validation RMSE

### Installation Commands

```bash
# Statistical modeling
pip install statsmodels scipy

# Machine learning
pip install scikit-learn

# Outlier detection
pip install pyod

# Optional: advanced methods
pip install xgboost lightgbm
```

---

## Additional Resources

### Documentation
- **statsmodels:** https://www.statsmodels.org/stable/index.html
- **scikit-learn:** https://scikit-learn.org/stable/
- **PyOD:** https://pyod.readthedocs.io/
- **scipy.stats:** https://docs.scipy.org/doc/scipy/reference/stats.html

### Key Statsmodels Pages
- OLS: https://www.statsmodels.org/stable/regression.html
- GLM: https://www.statsmodels.org/stable/glm.html
- RLM: https://www.statsmodels.org/stable/rlm.html
- Diagnostics: https://www.statsmodels.org/stable/diagnostic.html

### Key Sklearn Pages
- Linear Models: https://scikit-learn.org/stable/modules/linear_model.html
- Ensemble Methods: https://scikit-learn.org/stable/modules/ensemble.html
- Outlier Detection: https://scikit-learn.org/stable/modules/outlier_detection.html
- Model Selection: https://scikit-learn.org/stable/model_selection.html

---

### scientific-reviewer

**Path**: `skills/research/scientific-reviewer/SKILL.md`

# Scientific Reviewer

This skill transforms Claude into a rigorous scientific peer reviewer, systematically evaluating research documents across multiple dimensions of scientific quality and integrity.

## Review Framework

Conduct reviews using this structured approach:

### 1. Document Classification
First, identify the document type and scope:
- **Research Article**: Original empirical research with novel findings
- **Review Paper**: Synthesis of existing literature (narrative, systematic, meta-analysis)
- **Methods Paper**: New methodology, technique, or protocol development
- **Brief Communication/Letter**: Short report of preliminary or specific findings
- **Technical Report**: Detailed documentation of procedures, software, or data
- **Commentary/Perspective**: Opinion piece or interpretation of existing work
- **Case Study**: Detailed examination of specific example or instance

### 2. Claims Analysis
For each major claim in the document:
- **Identify the claim**: Extract explicit and implicit assertions
- **Locate supporting evidence**: Map claims to specific data, figures, tables, or citations
- **Assess evidence quality**: Evaluate if evidence is sufficient, appropriate, and convincing
- **Flag unsupported claims**: Highlight assertions lacking adequate backing
- **Check claim-evidence alignment**: Verify that conclusions follow from presented data

### 3. Logic and Argumentation Review
Evaluate the reasoning structure:
- **Logical flow**: Assess if arguments follow coherently from premises to conclusions
- **Methodological soundness**: Review experimental design, controls, sample sizes
- **Statistical analysis**: Check appropriateness of statistical methods and interpretation
- **Alternative explanations**: Consider if authors address competing hypotheses
- **Overgeneralization**: Identify claims that exceed what the data supports
- **Internal consistency**: Look for contradictions within the document

### 4. Citation Analysis
Review reference usage and completeness:
- **Citation adequacy**: Check if key prior work is acknowledged
- **Citation accuracy**: Verify that cited work supports the stated claims
- **Missing citations**: Identify gaps in literature coverage
- **Citation balance**: Assess if references represent diverse perspectives
- **Self-citation patterns**: Note excessive self-citation or citation bias
- **Currency**: Evaluate if recent relevant work is included

### 5. Methodological Assessment
For empirical research, evaluate:
- **Experimental design**: Controls, randomization, blinding where appropriate
- **Sample selection**: Representativeness, size, inclusion/exclusion criteria
- **Data collection**: Standardization, bias minimization, quality control
- **Analysis methods**: Appropriateness of analytical approaches
- **Reproducibility**: Sufficient detail for replication
- **Data availability**: Transparency in data sharing and accessibility

### 6. Technical Accuracy
Check domain-specific elements:
- **Terminology**: Correct usage of technical terms and concepts
- **Units and calculations**: Verification of numerical accuracy
- **Figure quality**: Clarity, labeling, and appropriate visualization
- **Table construction**: Organization, completeness, and statistical reporting
- **Methodological details**: Sufficient precision for reproducibility

## Review Output Structure

Organize reviews into clearly delineated sections:

### Executive Summary
- Document type and research contribution
- Overall assessment (1-2 paragraphs)
- Major strengths and weaknesses
- Recommendation (accept, minor revisions, major revisions, reject)

### Detailed Review

#### Claims and Evidence Assessment
For each major claim:
```
Claim: [Quote or paraphrase the assertion]
Evidence provided: [Description of supporting data/analysis]
Assessment: [Adequate/Insufficient/Partially supported]
Comments: [Specific feedback on evidence quality]
```

#### Logic and Methodology
- Strengths in reasoning and approach
- Logical gaps or methodological concerns
- Suggestions for improvement

#### Citation Review
- Well-cited areas and notable gaps
- Suggested additional references with brief rationales
- Citation accuracy concerns if any

#### Technical Comments
- Accuracy of calculations, figures, and tables
- Methodological suggestions
- Reproducibility concerns

### Minor Issues (Separate Section)
- Grammar, spelling, and language clarity
- Formatting inconsistencies
- Figure/table presentation improvements
- Reference formatting

## Suggested Citations Protocol

When recommending additional citations:
1. **Provide specific rationale**: Explain why each suggested reference is relevant
2. **Include key details**: Author names, approximate publication year, and main contribution
3. **Prioritize impact**: Focus on high-impact, recent, or seminal works
4. **Avoid over-citation**: Suggest only genuinely important missing references
5. **Consider diversity**: Include work from different research groups and perspectives

## Reviewer Tone and Approach

Maintain professional, constructive feedback:
- Be specific rather than vague in criticisms
- Acknowledge strengths alongside weaknesses
- Provide actionable suggestions for improvement
- Use diplomatic language while being direct about problems
- Focus on scientific merit rather than personal opinions
- Support critiques with specific examples from the text

## Quality Assurance Checks

Before finalizing review:
- Verify all claims about the document are accurate
- Ensure suggested citations are relevant and accessible
- Check that criticism is balanced with recognition of strengths
- Confirm all major issues are addressed systematically
- Review for internal consistency in the evaluation

This framework ensures comprehensive, fair, and constructive scientific review that maintains high standards while supporting authors' development of stronger research contributions.

---

### scientific-workflows

**Path**: `skills/research/scientific-workflows/SKILL.md`

# Scientific Workflow Management Skill

You are an expert assistant for scientific workflow management, helping users choose and implement the right workflow tool for their computational science needs. **Always recommend the simplest, lightest-weight solution** that satisfies the requirements, following the principle of "use the simplest tool that works."

## Philosophy

Scientific workflows range from simple parameter sweeps to complex multi-stage pipelines across heterogeneous compute resources. The key is matching tool complexity to problem complexity:

**Simplicity First:** Start with the minimal tooling needed. Only introduce orchestration frameworks when simpler approaches become limiting.

**Progressive Enhancement:** Begin with basic solutions (joblib, simple scripts) and migrate to sophisticated tools (Prefect, Parsl) only when requirements demand it.

## Decision Tree

Use this decision tree to recommend the appropriate tool:

```
START: What type of workflow do you need?

┌─ Single script with caching/memoization?
│  → USE: joblib (subskill: joblib)
│  • Function result caching
│  • Simple parallel loops
│  • NumPy array persistence
│
├─ Parameter sweep or embarrassingly parallel tasks?
│  ├─ Small scale (single machine)?
│  │  → USE: joblib.Parallel
│  │
│  └─ Large scale (cluster/cloud)?
│     ├─ HPC with SLURM/PBS?
│     │  → USE: Parsl (subskill: parsl)
│     │
│     └─ Cloud-native or hybrid?
│        → USE: Covalent (subskill: covalent)
│
├─ Complex DAG with dependencies and monitoring?
│  ├─ Pure Python, modern stack?
│  │  → USE: Prefect (subskill: prefect)
│  │
│  ├─ Materials science production workflows?
│  │  → USE: FireWorks + atomate2 (subskill: fireworks)
│  │
│  └─ High-throughput materials screening?
│     → USE: quacc (subskill: quacc)
│
└─ Event-driven or real-time workflows?
   → USE: Prefect (subskill: prefect)
```

## Tool Overview

### Tier 1: Lightweight (Start Here)

**joblib** - Function caching and simple parallelization
- **When:** Single scripts, iterative development, simple parameter sweeps
- **Complexity:** Minimal (decorator-based)
- **Setup:** `pip install joblib`
- **Scale:** Single machine
- **Best for:** Prototyping, small projects, avoiding recomputation

### Tier 2: Medium Orchestration

**Prefect** - Modern Python workflow orchestration
- **When:** Complex DAGs, dynamic workflows, need monitoring/retry logic
- **Complexity:** Medium (Python-first, no DAG syntax)
- **Setup:** `pip install prefect`
- **Scale:** Single machine → cloud
- **Best for:** Data pipelines, ML workflows, dynamic branching

**Parsl** - Parallel programming for HPC
- **When:** Scientific computing on HPC clusters, implicit dataflow
- **Complexity:** Medium (decorator-based, implicit parallelism)
- **Setup:** `pip install parsl`
- **Scale:** Laptop → supercomputers
- **Best for:** HPC scientific workflows, Jupyter notebooks

**Covalent** - Quantum/cloud workflow orchestration
- **When:** Quantum computing workflows, cloud-agnostic deployment
- **Complexity:** Medium (electron/lattice model)
- **Setup:** `pip install covalent`
- **Scale:** Local → cloud (AWS/Azure/GCP)
- **Best for:** ML/quantum workflows, infrastructure independence

### Tier 3: Domain-Specific / Production

**FireWorks** - Production workflow engine
- **When:** Large-scale production workflows, complex failure recovery
- **Complexity:** High (client-server, MongoDB, queue managers)
- **Setup:** `pip install fireworks`
- **Scale:** Thousands of jobs, HPC clusters
- **Best for:** Long-running production systems, Materials Project-style workflows

**quacc** - High-level materials science workflows
- **When:** Materials screening, quantum chemistry at scale
- **Complexity:** Medium-High (abstracts backend complexity)
- **Setup:** `pip install quacc`
- **Scale:** HPC/cloud (uses Parsl, Dask, or Prefect backends)
- **Best for:** Materials discovery, pre-built computational chemistry recipes

## Quick Recommendation Guide

### I just need to...

**Cache expensive function calls:**
```python
from joblib import Memory
# USE: joblib subskill
```

**Run 100 similar calculations in parallel:**
```python
from joblib import Parallel, delayed
# USE: joblib subskill (small scale)
# USE: Parsl subskill (HPC scale)
```

**Build a multi-step pipeline with error handling:**
```python
from prefect import flow, task
# USE: Prefect subskill
```

**Run materials science workflows (DFT, phonons, etc.):**
```python
from quacc import flow, job
# USE: quacc subskill
```

**Submit thousands of jobs to SLURM cluster:**
```python
# USE: Parsl subskill (if tasks are Python functions)
# USE: FireWorks subskill (if need complex dependencies, retries)
```

## Feature Comparison Matrix

| Feature | joblib | Prefect | Parsl | Covalent | FireWorks | quacc |
|---------|--------|---------|-------|----------|-----------|-------|
| **Caching** | ✓✓✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| **Simple Parallel** | ✓✓✓ | ✓✓ | ✓✓✓ | ✓✓ | ✓ | ✓✓ |
| **DAG Workflows** | ✗ | ✓✓✓ | ✓✓ | ✓✓ | ✓✓✓ | ✓✓ |
| **HPC Integration** | ✗ | ✓ | ✓✓✓ | ✓✓ | ✓✓✓ | ✓✓✓ |
| **Cloud Native** | ✗ | ✓✓✓ | ✓✓ | ✓✓✓ | ✓ | ✓✓ |
| **Error Recovery** | ✗ | ✓✓✓ | ✓✓ | ✓✓ | ✓✓✓ | ✓✓ |
| **Monitoring UI** | ✗ | ✓✓✓ | ✓ | ✓✓ | ✓✓✓ | ✓ |
| **Learning Curve** | Easy | Medium | Medium | Medium | Hard | Medium |
| **Setup Complexity** | None | Low | Low | Low | High | Medium |
| **Materials Focus** | ✗ | ✗ | ✗ | ✗ | ✓✓ | ✓✓✓ |

**Legend:** ✓✓✓ Excellent, ✓✓ Good, ✓ Basic, ✗ Not available

## Typical Migration Path

1. **Start:** Plain Python scripts
2. **Add caching:** joblib.Memory
3. **Add parallelism:** joblib.Parallel
4. **Complex workflows needed:**
   - General science → Prefect or Parsl
   - Materials science → quacc or FireWorks
5. **Production scale:** FireWorks (if materials) or Prefect Cloud

## Common Use Cases

### Computational Chemistry Parameter Sweep
```
Recommendation: joblib → Parsl → quacc
- Start: joblib for local testing (10s of calculations)
- Scale: Parsl for HPC (100s-1000s)
- Production: quacc for standardized materials workflows
```

### Machine Learning Pipeline
```
Recommendation: joblib → Prefect
- Start: joblib for caching model training
- Scale: Prefect for multi-stage ML pipelines with monitoring
```

### High-Throughput Materials Screening
```
Recommendation: quacc (or FireWorks for existing infrastructure)
- quacc: Modern, supports multiple backends
- FireWorks: If already using Materials Project ecosystem
```

### Data Processing Pipeline
```
Recommendation: joblib → Prefect
- Start: joblib for simple ETL
- Scale: Prefect for complex dependencies and scheduling
```

## Subskill Invocation

To get detailed guidance on a specific tool, invoke the corresponding subskill:

- **Simple caching/parallelism:** Use `joblib` subskill
- **Modern Python orchestration:** Use `prefect` subskill
- **HPC scientific computing:** Use `parsl` subskill
- **Cloud/quantum workflows:** Use `covalent` subskill
- **Materials production workflows:** Use `fireworks` subskill
- **Materials high-throughput:** Use `quacc` subskill

## Anti-Patterns to Avoid

**❌ Using FireWorks for 10 calculations**
→ Use joblib instead

**❌ Using joblib for 10,000 cluster jobs**
→ Use Parsl or FireWorks instead

**❌ Building custom DAG logic with multiprocessing**
→ Use Prefect instead

**❌ Deploying Prefect server for single-script caching**
→ Use joblib.Memory instead

**❌ Using general tools for materials science when domain tools exist**
→ Consider quacc or atomate2 instead

## Best Practices

1. **Start Simple:** Begin with joblib or plain Python. Add complexity only when needed.

2. **Prototype Locally:** Test workflows on small datasets with simple tools before scaling.

3. **Version Control Workflows:** All workflow definitions should be in git.

4. **Separate Concerns:**
   - Computation logic (Python functions)
   - Workflow orchestration (tool-specific decorators)
   - Infrastructure (deployment configs)

5. **Plan for Failure:** Design workflows assuming tasks will fail and need retries.

6. **Monitor Resource Usage:** Understand computational costs before large-scale deployment.

7. **Document Dependencies:** Clear environment specifications (conda, requirements.txt).

## Getting Started

For a new scientific workflow project:

1. **Assess Requirements:**
   - How many tasks? (10s, 100s, 1000s, 10000s+)
   - Where do they run? (laptop, HPC, cloud)
   - What dependencies exist? (simple parallel vs complex DAG)
   - What error handling needed? (fail fast vs retry/recover)

2. **Choose Tool Based on Assessment:**
   - Tasks < 100, single machine, simple → joblib
   - Tasks > 100, HPC cluster, Python-based → Parsl
   - Complex DAG, monitoring needed → Prefect
   - Materials science workflows → quacc or FireWorks

3. **Implement Minimally:**
   - Start with 2-3 representative tasks
   - Verify workflow logic
   - Add error handling
   - Scale gradually

4. **Iterate:**
   - Monitor performance
   - Add features as needed
   - Migrate to more powerful tools only if requirements evolve

## Additional Tools Worth Knowing

**Snakemake** - Make-like workflows with Python
- Popular in bioinformatics
- Rule-based workflow definition
- Good for file-based pipelines

**Dask** - Parallel computing with task graphs
- NumPy/Pandas-like API
- Good for array/dataframe operations
- Can integrate with Prefect/Parsl

**Luigi** - Spotify's workflow engine
- Target-based execution
- Good for data pipelines
- More complex than Prefect

**Apache Airflow** - Enterprise workflow orchestration
- Very powerful, very complex
- Overkill for most scientific workflows
- Consider only for large organizations

## When to Use This Skill

Invoke this skill when:
- Designing a new computational workflow
- Choosing between workflow tools
- Migrating from simple scripts to orchestrated workflows
- Troubleshooting workflow performance or complexity
- Learning workflow best practices for scientific computing

## Examples

See `examples/` directory for:
- `simple_caching.py` - joblib basics
- `parameter_sweep.py` - Comparison across tools
- `materials_workflow.py` - quacc example
- `hpc_workflow.py` - Parsl on SLURM
- `ml_pipeline.py` - Prefect for ML

## References

- **joblib:** https://joblib.readthedocs.io/
- **Prefect:** https://docs.prefect.io/
- **Parsl:** https://parsl-project.org/
- **Covalent:** https://github.com/AgnostiqHQ/covalent
- **FireWorks:** https://materialsproject.github.io/fireworks/
- **quacc:** https://quantum-accelerators.github.io/quacc/
- **atomate2:** https://github.com/materialsproject/atomate2
- **jobflow:** https://materialsproject.github.io/jobflow/

## See Also

- `materials-properties` skill - For ASE-based materials calculations
- Subskills in `subskills/` directory for tool-specific guidance

---

### scientific-writing

**Path**: `skills/communication/scientific-writing/SKILL.md`

* Scientific Writing

Systematic guidance for writing research papers, grants, and scientific communications with
emphasis on clarity, reproducibility, and adherence to field conventions.

** Scientific Writing Workflow

*** 1. Identify Document Type and Requirements

Determine what you're writing:
- Research article → Full paper with novel findings
- Review article → Synthesis of existing literature
- Grant proposal → Funding application
- Conference abstract → Brief standalone summary
- Revision response → Reply to peer reviewers
- Technical report → Documentation of work

Gather venue requirements:
- Target journal/conference/funder name
- Word/page limits by section
- Citation style required
- Figure/table format specifications
- Submission guidelines and templates

*** 2. Select Structure and Route to Guidance

For research articles:
- Use IMRAD structure (see references/paper-structure.md)
- Introduction: references/introduction-writing.md
- Methods: references/methods-writing.md
- Results: references/results-writing.md
- Discussion: references/discussion-writing.md
- Abstract: references/abstract-writing.md

For review articles:
- See references/review-writing.md for thematic organization
- Focus on synthesis, not chronological listing
- Critical evaluation of literature

For grant proposals:
- See references/grant-writing.md for funder-specific guidance
- Specific Aims structure
- Significance, Innovation, Approach

For revision responses:
- See references/revision-response.md for point-by-point format
- Diplomatic language and tracking changes

*** 3. Write Section by Section

Follow section-specific guidance:

Introduction (references/introduction-writing.md)
- Funnel structure: Broad → Narrow → Gap → Objective
- Establish context and importance
- Identify knowledge gap
- State clear objective

Methods (references/methods-writing.md)
- Reproducibility is paramount
- Include all essential details: equipment, reagents, software, statistics
- Organize chronologically or by subsystem
- Past tense, field-appropriate voice

Results (references/results-writing.md)
- Report observations objectively
- No interpretation (save for discussion)
- Integrate figures and tables
- Include statistical details
- Past tense

Discussion (references/discussion-writing.md)
- Interpret findings in context
- Compare with literature
- Acknowledge limitations honestly
- Discuss implications and future work
- Mix of present (facts) and past (your results)

*** 4. Create Figures and Tables

Figures (references/figure-design.md)
- Match figure type to data type
- Publication quality: 300+ DPI, readable fonts, vector when possible
- Complete captions with all essential information
- Colorblind-friendly palettes

Tables (references/table-design.md)
- Use when precise values needed
- Clear headers with units
- Consistent decimal places
- Descriptive captions

Tools:
#+begin_src bash
# Generate publication-quality figures
python scripts/figure_generator.py data.csv config.yaml --output fig1.pdf

# Format tables for journals
python scripts/table_formatter.py data.csv --format latex --journal nature
#+end_src

*** 5. Manage Citations and References

Citation formatting (references/citation-management.md)
- Match journal's required style:
  - Numbered: [1], [2,3], [1-5]
  - Author-year: (Smith, 2020; Jones, 2021)
  - Author-number: Smith (1)
- Check all in-text citations have references
- Check all references are cited
- Consistent formatting throughout

Tools:
#+begin_src bash
# Validate citation consistency
python scripts/citation_checker.py manuscript.docx --style apa
#+end_src

*** 6. Improve Clarity and Conciseness

Language clarity (references/language-clarity.md)
- Active voice preferred (field norms vary)
- Concise: remove filler words and redundancies
- Precise: avoid vague terms ("very," "quite," "many")
- One main idea per sentence
- Logical paragraph flow

Tools:
#+begin_src bash
# Analyze readability
python scripts/readability_analyzer.py section.txt

# Count words by section
python scripts/word_counter.py manuscript.docx
#+end_src

*** 7. Review Against Requirements

Pre-submission checklist:
- [ ] Word limits met for each section
- [ ] All figures/tables referenced in text
- [ ] All citations formatted consistently
- [ ] Statistical details complete
- [ ] Methods reproducible
- [ ] Limitations discussed
- [ ] Abstract within word limit
- [ ] Keywords selected
- [ ] Author contributions stated
- [ ] Conflicts of interest disclosed
- [ ] Data availability statement
- [ ] Ethics approvals included

** Document Type Routing

*** Research Article → IMRAD Structure

Structure overview (references/paper-structure.md)

IMRAD sections:
- /I/ntroduction: Background, gap, objective
- /M/ethods: Reproducible experimental details
- /R/esults: Objective observations with data
- /A/nd
- /D/iscussion: Interpretation, implications, limitations

What goes where:
- Introduction: Why this work matters, what's unknown, what you'll do
- Methods: How you did it (enough detail to reproduce)
- Results: What you found (observations, not interpretations)
- Discussion: What it means, how it fits, what's next

Common mistakes to avoid:
- ❌ Interpretation in results section
- ❌ New results in discussion section
- ❌ Methods scattered in results
- ❌ Missing gap identification in introduction
- ❌ Ignoring limitations

*** Review Article → Thematic Organization

See references/review-writing.md

Types:
- Narrative review: Broad overview
- Systematic review: Structured search (PRISMA)
- Meta-analysis: Quantitative synthesis

Key principles:
- Synthesize thematically, not chronologically
- Critical evaluation, not just summary
- Identify patterns, gaps, controversies
- Future directions

*** Grant Proposal → Funder-Specific Structure

See references/grant-writing.md

Common elements:
- Specific Aims (1 page): Clear, testable objectives
- Significance: Why it matters, current knowledge, impact
- Innovation: What's novel about approach
- Approach: Methods, timeline, pitfalls, alternatives
- Preliminary data: Feasibility demonstration

Review criteria alignment:
- Address all criteria explicitly
- Use headings matching criteria
- Make reviewers' job easy

*** Revision Response → Point-by-Point

See references/revision-response.md

Structure:
- Thank reviewers
- Summary of major changes
- Point-by-point responses with line numbers
- Track changes in manuscript

Response strategies:
- Agree and comply: Describe changes made
- Agree but can't comply: Explain why, offer alternative
- Disagree: Provide evidence diplomatically

** Section-Specific Guidance

*** Introduction Writing

See references/introduction-writing.md

Funnel structure:
1. Broad context (why general topic matters)
2. Narrow to specific problem area
3. Identify gap in knowledge
4. State objective of this work
5. Brief approach overview (optional)

Length: Typically 1-3 pages

Tense: Present for established facts, past for previous studies

*** Methods Writing

See references/methods-writing.md

Reproducibility checklist:
- Equipment: manufacturer, model, specs
- Reagents: source, catalog #, concentration
- Software: name, version, parameters
- Statistical tests with corrections
- Sample size justification
- Ethics approvals

Organization:
- Chronological (when order matters)
- By subsystem (complex systems)
- By measurement type (multiple assays)

Detail level:
- Standard procedures: Brief + citation
- Novel procedures: Full detail
- Modified procedures: Highlight changes

*** Results Writing

See references/results-writing.md

Objectivity principles:
- Report observations, not interpretations
- "Data show" not "data prove"
- Present negative results honestly
- Save interpretation for discussion

Statistical reporting:
- Test statistic, df, exact p-value
- Effect sizes and confidence intervals
- Multiple comparison corrections
- Measures of variability (SD, SE, CI)

Figure integration:
- Reference every figure/table in text
- Describe key finding from each
- Don't just say "see Figure X"

*** Discussion Writing

See references/discussion-writing.md

Structure:
1. Restate main findings (brief)
2. Interpret in context of literature
3. Compare with previous work
4. Explain unexpected findings
5. Acknowledge limitations
6. Discuss implications
7. Suggest future directions

Limitations:
- Be honest but not self-defeating
- Explain impact of limitations
- Suggest how to address
- Don't introduce obscure limitations

*** Abstract Writing

See references/abstract-writing.md

Essential elements:
- Background (1-2 sentences)
- Objective
- Methods (brief approach)
- Results (key findings with data)
- Conclusions (significance)

Word limit strategies:
- Typical: 150-300 words
- Prioritize results and conclusions
- Remove modifiers
- Define abbreviations sparingly
- No citations (usually)

Self-contained requirement:
- Understandable without reading paper
- Include actual data, not just "significant"

** Universal Scientific Writing Principles

*** 1. Clarity Over Complexity

- Simple words over complex when meaning is same
- Short sentences preferred
- One idea per sentence
- Active voice when possible (field norms vary)
- Define specialized terms

*** 2. Precision in Language

- Specific numbers over vague terms
- "Approximately 50%" not "many"
- Distinguish "significant" (statistical) from "important"
- Clear about causation vs correlation
- Avoid hedging excessively ("relatively," "quite," "very")

*** 3. Objectivity in Reporting

- Separate observations (results) from interpretations (discussion)
- Present negative results honestly
- Acknowledge contradictory evidence
- Don't overstate conclusions
- Appropriate level of certainty

*** 4. Reproducibility in Methods

- Include all information needed to reproduce
- Equipment specifications
- Exact concentrations and conditions
- Statistical methods fully described
- Software versions and parameters
- Code availability when applicable

*** 5. Logical Flow of Argument

- Each section builds on previous
- Clear transitions between ideas
- Coherent narrative throughout
- Introduction sets up discussion
- Discussion answers introduction's questions

*** 6. Appropriate Detail for Audience

- Assume audience has field background
- Define specialized terms first use
- Balance between too much and too little detail
- Don't explain fundamental concepts
- Do explain novel techniques or applications

** Tense and Voice Usage Guide

*** Tense by Section

Introduction:
- Present: Established facts ("DNA is composed of...")
- Past: Previous studies ("Smith et al. showed...")
- Present: Your objective ("This study investigates...")

Methods:
- Past: What you did ("Cells were cultured...")
- Past: ("We cultured cells...")

Results:
- Past: Your findings ("Expression increased...")
- Past: ("We observed increased expression...")

Discussion:
- Past: Your results ("Our data showed...")
- Present: Established facts ("This gene regulates...")
- Present: Interpretations ("These results suggest...")

Conclusions:
- Present: General conclusions ("This approach provides...")

*** Voice by Field

Biology/Medicine:
- More passive acceptable in methods
- Either active or passive in other sections

Engineering/Computer Science:
- Active voice preferred throughout
- "We designed..." rather than "A design was created..."

Physics/Chemistry:
- Varies by journal and author preference
- Either acceptable if consistent

** Quick Reference - Common Issues

Results vs Discussion: Keep observations in results, interpretations in discussion

Citation Consistency: Use one style throughout (numbered [1] or author-year)

Figure References: Reference all figures/tables in text with findings, not just "see Figure X"

Statistical Reporting: Include test statistic, df, exact p-value, effect size, variability measure

Methods Detail: Include manufacturer, catalog #, concentrations for reproducibility

Limitations: Be honest but not self-defeating; explain impact, suggest solutions

See individual reference files for detailed examples and guidance.

** Tools and Utilities

*** Citation Checker

#+begin_src bash
python scripts/citation_checker.py manuscript.docx --style apa
#+end_src

Validates citation consistency between in-text and reference list.

*** Figure Generator

#+begin_src bash
python scripts/figure_generator.py data.csv config.yaml --output fig1.pdf
#+end_src

Creates publication-quality figures from data.

*** Word Counter

#+begin_src bash
python scripts/word_counter.py manuscript.docx
#+end_src

Counts words by section for length requirements.

*** Readability Analyzer

#+begin_src bash
python scripts/readability_analyzer.py section.txt
#+end_src

Analyzes text complexity and suggests improvements.

*** Table Formatter

#+begin_src bash
python scripts/table_formatter.py data.csv --format latex --journal nature
#+end_src

Formats tables for specific journals.

** Templates

Access templates for common document types:
- assets/templates/manuscript_template.docx - Generic manuscript
- assets/templates/imrad_template.md - IMRAD structure
- assets/templates/grant_aims_template.docx - NIH Specific Aims
- assets/templates/response_letter_template.docx - Revision response
- assets/templates/abstract_template.txt - Structured abstract
- assets/templates/methods_checklist.md - Reproducibility checklist

** Style Guides

Journal-specific quick references:
- assets/style-guides/apa_guide.md - APA style
- assets/style-guides/nature_guide.md - Nature requirements
- assets/style-guides/science_guide.md - Science requirements
- assets/style-guides/cell_guide.md - Cell requirements
- assets/style-guides/plos_guide.md - PLOS requirements
- assets/style-guides/nih_guide.md - NIH grant guidelines
- assets/style-guides/nsf_guide.md - NSF grant guidelines

---

### tdd

**Path**: `skills/development/tdd/SKILL.md`

# Test-Driven Development (TDD) Skill

Guide users through disciplined test-first development using the red-green-refactor cycle.

## Core TDD Workflow

**RED → GREEN → REFACTOR → REPEAT**

### RED: Write a Failing Test
1. Identify next small behavior to implement
2. Write test that specifies that behavior
3. Run test to verify it **fails for the right reason**
4. If test passes unexpectedly, test is wrong

### GREEN: Make It Pass
1. Write **minimal** code to make test pass
2. Don't worry about perfection yet
3. Simplest solution that works
4. Run test to verify it passes

### REFACTOR: Improve the Code
1. Improve code quality while keeping tests green
2. Remove duplication
3. Improve names and structure
4. Run tests after each change to ensure still passing

### REPEAT
1. Commit when tests are green
2. Identify next behavior
3. Start cycle again with new test

## TDD Discipline

**Critical rules to follow:**

### Test First (RED Phase)
- **Always write test before implementation**
- Resist urge to write code first
- Test defines what "done" means
- See test fail before making it pass

### Minimal Implementation (GREEN Phase)
- **Write simplest code to pass**
- Don't over-engineer
- Don't add features not tested
- One test at a time

### Refactor Only When Green
- **Never refactor with failing tests**
- Keep tests passing during refactoring
- Small, incremental improvements
- Run tests after each refactoring step

### Run Tests Frequently
- After writing test (should fail)
- After writing implementation (should pass)
- After each refactoring step (should stay green)
- Before committing

## When to Use This Skill

Activate for requests involving:
- "Use TDD for..." / "Test-driven development..."
- "Write tests first..." / "Red-green-refactor..."
- Developing new features test-first
- Learning TDD practices
- Setting up test infrastructure
- Test design and organization

## Test Structure Patterns

### Arrange-Act-Assert (AAA)
**Arrange** - Set up test data and environment
**Act** - Execute the code under test
**Assert** - Verify the results

```python
def test_add_two_numbers():
    calculator = Calculator()           # Arrange
    result = calculator.add(2, 3)      # Act
    assert result == 5                  # Assert
```

### Given-When-Then (BDD Style)
**Given** - Initial context/preconditions
**When** - Action/event occurs
**Then** - Expected outcome

## Test Design Principles

### What to Test
- **Public interface** - Test behavior users depend on
- **Edge cases** - Boundaries, empty inputs, max values
- **Error conditions** - Invalid inputs, exceptions
- **Business logic** - Core algorithms and rules
- **Integration points** - Where components interact

### What NOT to Test
- **Private implementation details** - Test behavior, not internals
- **Third-party libraries** - Trust they work, test your usage
- **Simple getters/setters** - Unless they have logic
- **Framework code** - Test your code, not the framework

### One Behavior Per Test
- Each test should verify single behavior
- Makes failures easier to diagnose
- Keeps tests focused and readable
- Prefer multiple small tests over one large test

### Make Tests Readable
- **Descriptive names** - `test_add_returns_sum_of_two_positive_numbers`
- **Clear structure** - AAA or Given-When-Then
- **Self-documenting** - Test shows how code should be used
- **Minimal setup** - Only what's needed for this test

### Keep Tests Independent
- Tests should not depend on each other
- Tests can run in any order
- Each test starts with clean state
- No shared mutable state between tests

See `references/test-design-patterns.md` for comprehensive guidance.

## Language-Specific Guidance

### For Python
See `references/python-tdd.md` for:
- pytest and unittest frameworks
- Fixtures and parametrized tests
- Mocking with unittest.mock
- Testing async code
- Coverage with pytest-cov
- Running and organizing tests

### For Emacs Lisp
See `references/elisp-tdd.md` for:
- ERT (Emacs Lisp Regression Testing)
- Testing interactive functions
- Buffer manipulation testing
- Mocking with cl-letf
- Buttercup (BDD alternative)
- Running tests in Emacs and batch mode

### For Other Languages
See `references/general-tdd.md` for:
- Finding testing frameworks
- Universal test patterns
- Common testing concepts
- Build tool integration
- Language-agnostic principles

## Test Types and When to Use

### Unit Tests
**What:** Test individual functions/methods in isolation

**When:**
- Testing pure functions
- Testing business logic
- Testing algorithms
- Fast, focused tests

**Example:** `test_calculate_discount(price, percentage)`

### Integration Tests
**What:** Test multiple components working together

**When:**
- Testing database interactions
- Testing API calls
- Testing service integration
- Verifying components connect correctly

**Example:** `test_user_service_saves_to_database()`

### End-to-End Tests
**What:** Test complete user workflows

**When:**
- Testing critical user paths
- Verifying system as a whole
- Smoke tests for deployment

**Example:** `test_user_can_register_and_login()`

**Test Pyramid:**
```
      /\      ← Few E2E tests (slow, brittle)
     /  \
    / IT \    ← Some Integration tests
   /______\
  /  Unit  \  ← Many Unit tests (fast, focused)
 /__________\
```

## TDD Red-Green-Refactor Example

**Goal:** Implement factorial function

**Iteration 1 - Base case:**
- RED: `test_factorial_of_zero_is_one()` → ❌ factorial not defined
- GREEN: `def factorial(n): return 1` → ✅ Passes
- REFACTOR: Nothing yet. Commit.

**Iteration 2 - Positive numbers:**
- RED: `test_factorial_of_five()` expects 120 → ❌ Got 1
- GREEN: Implement loop to calculate factorial → ✅ Passes
- REFACTOR: Use recursion for elegance → ✅ Still passes. Commit.

**Iteration 3 - Error handling:**
- RED: `test_factorial_negative_raises_error()` → ❌ No error raised
- GREEN: Add if n < 0: raise ValueError → ✅ All tests pass
- REFACTOR: Add docstring → ✅ Still passes. Commit.

**Done!** Function is complete, fully tested, documented. Three test-driven iterations.

## Mocking and Test Doubles

### When to Mock
- **External dependencies** - Databases, APIs, file systems
- **Slow operations** - Network calls, large computations
- **Unpredictable behavior** - Random, time-dependent, external state
- **Hard to trigger scenarios** - Error conditions, edge cases

### When NOT to Mock
- **Your own code** - Prefer real objects for your code
- **Simple objects** - Data classes, value objects
- **Logic being tested** - Don't mock what you're testing

### Types of Test Doubles

**Mock** - Programmed with expectations, verifies interactions
**Stub** - Provides canned responses, doesn't verify
**Fake** - Working implementation, simpler than real
**Spy** - Records calls, allows verification after

See `references/test-design-patterns.md` for detailed mocking strategies.

## Common TDD Anti-Patterns

**Don't:**

❌ **Write implementation before test** - Defeats TDD purpose

❌ **Write multiple tests before making them pass** - Stay in rhythm (one test, make it pass, next test)

❌ **Refactor with red tests** - Only refactor when green

❌ **Test implementation details** - Test behavior, not internals

❌ **Skip refactor step** - Technical debt accumulates

❌ **Write tests that are hard to understand** - Tests are documentation

❌ **Create dependencies between tests** - Tests must be independent

❌ **Mock everything** - Use real objects when practical

❌ **Fake it with hardcoded values forever** - "Fake it till you make it" is temporary

❌ **Write slow tests** - Slow test suite won't be run frequently

## Test Naming Conventions

**Good test names are descriptive and specific:**

### Pattern: `test_<function>_<scenario>_<expected_result>`
```python
test_add_two_positive_numbers_returns_sum()
test_add_with_negative_number_returns_correct_result()
test_add_with_zero_returns_other_number()
```

### Pattern: `should_<expected_behavior>_when_<condition>`
```python
should_return_empty_list_when_no_items_match()
should_raise_error_when_input_is_null()
should_calculate_discount_when_user_is_premium()
```

### Pattern: `<behavior>_<state>_<expected>`
```elisp
(ert-deftest save-buffer-modified-saves-to-file ())
(ert-deftest load-file-missing-raises-error ())
```

**Test name should:**
- Describe what's being tested
- Describe the scenario/condition
- Describe expected outcome
- Be readable as documentation

## Test Organization

### Directory Structure

**Python:**
```
project/
├── src/
│   └── calculator.py
└── tests/
    ├── __init__.py
    ├── test_calculator.py
    └── conftest.py  # pytest fixtures
```

**Elisp:**
```
package/
├── my-package.el
└── test/
    └── test-my-package.el
```

### Naming Conventions
- Test files: `test_*.py`, `*_test.py`, `test-*.el`
- Test functions: Start with `test_` or `ert-deftest`
- Test classes: `Test*` (if using classes)

### Grouping Tests
- One test file per source file (generally)
- Group related tests in same file
- Separate unit/integration/e2e tests

See language-specific references for detailed organization patterns.

## Refactoring with Tests

### Safe Refactoring Process

1. **Ensure all tests are green** before starting
2. **Make small changes** - One refactoring at a time
3. **Run tests after each change** - Catch breaks immediately
4. **Commit frequently** - When tests pass
5. **Don't add features while refactoring** - Separate concerns

### Common Refactorings
- Extract function (break up large functions)
- Rename for clarity
- Remove duplication (DRY)
- Simplify conditional logic
- Extract variable for readability
- Inline unnecessary abstraction

### When Tests Break During Refactoring

**If test is testing implementation detail:**
- Update test to test behavior instead
- Make test more resilient to changes

**If test is testing behavior:**
- Fix the code, not the test
- Behavior shouldn't change during refactoring

**If too many tests break:**
- Change is too large, revert
- Make smaller incremental changes

See `references/refactoring-with-tests.md` for detailed guidance.

## Test Coverage

**Coverage measures which code is executed by tests, not whether tests are good.**

### Types of Coverage
- **Line coverage** - Which lines executed
- **Branch coverage** - Which paths taken
- **Function coverage** - Which functions called

### Coverage Goals
- Aim for high coverage (80%+) but not 100%
- 100% coverage doesn't mean bug-free
- Focus on critical code paths
- Don't test just to increase coverage

### Using Coverage Tools
- **Python:** pytest-cov, coverage.py
- **JavaScript:** Jest with coverage
- **Java:** JaCoCo
- **Ruby:** SimpleCov

Use `scripts/coverage_analyzer.py` to identify coverage gaps.

## Using Supporting Resources

Additional resources in this skill:

- **references/python-tdd.md**: Comprehensive Python testing guide (pytest, unittest, mocking, async)
- **references/elisp-tdd.md**: Comprehensive Elisp testing guide (ERT, Buttercup, interactive functions)
- **references/general-tdd.md**: Universal TDD principles for any language
- **references/test-design-patterns.md**: What to test, test organization, anti-patterns
- **references/refactoring-with-tests.md**: Safe refactoring process and common refactorings
- **scripts/test_template_generator.py**: Generate test file boilerplate
- **scripts/coverage_analyzer.py**: Analyze coverage reports
- **assets/templates/**: Test file templates for multiple languages

## Quick Reference

**TDD Cycle:**
1. RED - Write failing test
2. GREEN - Make it pass (minimal code)
3. REFACTOR - Improve while keeping green
4. REPEAT

**Test Structure:**
- Arrange (setup)
- Act (execute)
- Assert (verify)

**Test Principles:**
- Test first
- One test at a time
- One behavior per test
- Independent tests
- Fast tests
- Readable tests

**Refactoring Rules:**
- Only refactor when green
- Small changes
- Run tests frequently
- Don't add features

---

**Remember:** TDD is a discipline. The value comes from following the cycle strictly. Test first. See it fail. Make it pass. Refactor. Repeat. The rhythm creates quality code.

---

### troubleshooting

**Path**: `skills/technical/troubleshooting/SKILL.md`

# Troubleshooting & Debugging Skill

Guide users through systematic problem diagnosis and resolution using structured methodologies applicable to any domain.

## Quick Start Workflow

When a troubleshooting request arrives, follow this systematic approach:

1. **Understand** - What's expected vs actual, when started, what changed
2. **Gather** - Collect diagnostic information (logs, errors, environment, steps to reproduce)
3. **Hypothesize** - List possible causes ranked by probability
4. **Design** - Create tests that isolate variables
5. **Test** - Execute systematically, document results
6. **Identify** - Distinguish symptoms from root cause
7. **Verify** - Confirm fix resolves the issue
8. **Document** - Record solution for future reference

## When to Use This Skill

Activate for requests involving:
- "Debug this..." / "Troubleshoot..."
- "Why isn't this working..." / "Getting an error..."
- "Something's wrong with..." / "How do I fix..."
- Any problem description or malfunction report
- Unexpected behavior or failures
- Performance issues or degradation
- Intermittent problems

## Problem Understanding Phase

### Essential First Questions

Before diving into solutions, gather critical context:

**Problem Description:**
- What are you trying to do? (expected behavior)
- What's actually happening? (actual behavior)
- What error messages or symptoms do you see?
- Can you reproduce it consistently?

**Timeline:**
- When did this start?
- Did it ever work correctly?
- What changed recently? (code, config, environment, data, dependencies)

**Scope:**
- Does this happen every time or intermittently?
- Does it happen for everyone or just some users?
- Does it happen in all environments or just specific ones?

**Environment:**
- What system/platform/version?
- What's the configuration?
- What are the dependencies and their versions?

**Reproduction:**
- What are the exact steps to reproduce?
- Can you provide a minimal example that demonstrates the issue?
- What's the simplest case that fails?

### Problem Type Classification

Categorize to guide troubleshooting approach:

**By Consistency:**
- **Consistent** - Happens every time (easier to debug)
- **Intermittent** - Happens sometimes (harder, often timing/race conditions)
- **Progressive** - Gets worse over time (resource leaks, degradation)

**By History:**
- **Regression** - Worked before, broke recently (what changed?)
- **Never worked** - New feature/setup (design or setup issue)

**By Scope:**
- **Universal** - Happens everywhere (likely code/logic issue)
- **Environmental** - Happens in specific environment (config, dependencies, resources)
- **User-specific** - Happens for specific users (permissions, data, state)

**By Domain:**
- **Code/Logic** - Software bugs, algorithmic errors
- **Configuration** - Settings, parameters, environment variables
- **Data** - Input validation, corruption, format issues
- **Infrastructure** - Network, storage, resources, services
- **Integration** - API failures, service dependencies
- **Human process** - Workflow, communication, missing steps

Classification helps prioritize hypotheses and diagnostic approaches.

## Diagnostic Information Gathering

### Log and Error Analysis

**Error Messages:**
- Copy exact error text (don't paraphrase)
- Note error code/type if provided
- Identify stack trace or error location
- Look for nested/chained errors (root cause often deeper)

**Log Examination:**
- Check timestamps (when did issue occur)
- Look for ERROR, WARN, EXCEPTION entries near failure time
- Trace request/operation ID through logs
- Check for patterns (repeating errors, sequences)
- Review logs before failure (what was system doing)

**System State:**
- Resource usage (CPU, memory, disk, network)
- Process/service status
- Recent restarts or crashes
- System event logs

### Environment Documentation

Capture complete environment picture:

**Versions:**
- Application/service version
- Runtime/interpreter version (Node, Python, Java, etc.)
- OS version
- Dependency versions (libraries, packages)
- Database/cache versions

**Configuration:**
- Config files and their values
- Environment variables
- Feature flags or toggles
- Settings that differ from defaults

**Infrastructure:**
- Network topology
- Service endpoints and connectivity
- Firewall/security rules
- Load balancers, proxies
- DNS configuration

### Reproduction Case Development

**Goal**: Find minimal reproducible example (MRE)

**Process:**
1. Document exact steps from clean state
2. Strip away non-essential steps
3. Minimize data/input to simplest case
4. Remove dependencies where possible
5. Verify it still reproduces

**Good reproduction case:**
- Anyone can follow steps and see same issue
- Minimal (fewest steps, smallest data)
- Self-contained (no external dependencies if possible)
- Deterministic (reproduces consistently)

See `references/information-gathering.md` for detailed techniques.

## Hypothesis Generation

### Forming Testable Hypotheses

**Generate possibilities:**
- Brainstorm what could cause observed symptoms
- Consider each problem type (code, config, data, infrastructure)
- Review known failure patterns for this domain
- Don't self-censor - list everything plausible

**Rank by probability:**
- Most likely causes first
- Consider base rates (common vs rare failures)
- Factor in recent changes
- Weight by evidence strength

**Make hypotheses testable:**
- "If X is the cause, then Y should be true"
- Design specific test that would confirm/refute
- Ensure test is practical to execute

**Example hypotheses (ranked):**
1. **High probability:** Recent config change introduced typo → Check config against previous version
2. **Medium probability:** Dependency version mismatch → Verify all dependencies match working environment
3. **Low probability:** Hardware failure → Run diagnostics, check system logs for hardware errors

### Common Failure Patterns

**Code/Logic:** Off-by-one, null/undefined, type mismatch, logic errors, unhandled edge cases, race conditions

**Configuration:** Typos, wrong environment variables, permissions, path issues, missing settings

**Data:** Invalid format, null/empty values, corruption, encoding issues, schema mismatch

**Infrastructure:** Network connectivity, port conflicts, insufficient resources, permissions, services, firewall

**Integration:** API changes, expired credentials, rate limiting, timeouts, version incompatibility

See `references/hypothesis-generation.md` and `references/domain-specific-patterns.md` for comprehensive patterns.

## Systematic Testing

### Scientific Method Principles

**Change one variable at a time:**
- Modify single factor per test
- Know exactly what you changed
- Document change and result
- Reset to baseline between tests

**Verify assumptions:**
- Don't assume anything works
- Test each layer (can you reach service? is it responding? is auth working?)
- Question "obvious" things
- Build confidence from known-good baseline

**Isolate variables:**
- Remove unnecessary components
- Test in controlled environment
- Disable features to narrow scope
- Use minimal configuration

**Document everything:**
- What you tested
- What you changed
- What happened (exact result)
- Conclusions drawn
- Next steps

Use `scripts/hypothesis_tracker.py` to maintain investigation log.

### Binary Search Troubleshooting

**Concept**: Divide problem space in half, determine which half contains issue, repeat.

**Applications:**
- **Version bisect:** Which commit introduced bug? (git bisect)
- **Config narrowing:** Which of 50 config settings causes issue?
- **Data range:** Which records in dataset cause failure?
- **Code sections:** Which function/module has the bug?
- **Time range:** When did problem start occurring?

**Process:**
1. Identify search space boundaries (working ↔ broken)
2. Test midpoint
3. Determine which half contains issue
4. Repeat with smaller range
5. Continue until isolated to specific element

**Example - Config debugging:**
- Start: All 50 config settings (broken)
- Disable half (25 settings) → Still broken
- Issue is in disabled half
- Disable half of those (12-13) → Works!
- Issue is in recently disabled set
- Continue narrowing...
- Result: Isolated to single problematic setting

Use `scripts/binary_search_helper.py` for interactive guidance.

### Differential Diagnosis

Compare working vs non-working cases to identify differences:

**Compare:**
- Configurations
- Environments
- Data inputs
- Dependencies
- Timing/sequence
- System state

**Look for:**
- What's present in failing case but not working case
- What's different between the two
- What versions differ
- What settings differ

**Technique:**
- Start with known-good case
- Progressively make it more like failing case
- Stop when you introduce the failure
- That change is the cause

## Root Cause Analysis

### 5 Whys Technique

Dig deeper by asking "why" repeatedly (typically 5 times):

**Example:**
1. **Problem:** Website is down
2. **Why?** Database connection failed
3. **Why?** Connection pool exhausted
4. **Why?** Connections not being released
5. **Why?** Code missing connection.close() in error path
6. **Why?** Developer didn't know about finally blocks
7. **Root cause:** Missing error-handling training, missing code review checklist

**Guidelines:**
- Each "why" should be factual (not speculative)
- Stop when you reach actionable root cause
- May need more or fewer than 5 whys
- Focus on process/system causes, not blaming people

### Distinguishing Symptoms from Root Cause

**Symptom:** Observable manifestation of problem
**Root Cause:** Underlying reason problem exists

**Example:**
- **Symptom:** Application crashes
- **Symptom:** Out of memory error
- **Symptom:** Memory leak in caching module
- **Root Cause:** Cache eviction policy not implemented

**Test:** Would fixing this prevent all instances of the problem?
- If no → It's a symptom, keep digging
- If yes → Likely root cause

**Verify:**
- Fix the root cause
- Test that all symptoms disappear
- Verify issue doesn't recur

See `references/diagnostic-frameworks.md` for Fishbone diagrams and FMEA.

## Troubleshooting Principles

**Core Principles:**

1. **Reproduce before fixing** - If you can't reproduce it, you can't verify the fix

2. **Change one thing at a time** - Otherwise you won't know what fixed it

3. **Verify assumptions** - "It should work" ≠ "It does work"

4. **Start with simple explanations** - Occam's Razor (common problems are common)

5. **Follow the data** - Believe logs/evidence over intuition

6. **Work from known-good baseline** - Establish what works, then build up

7. **Document as you go** - Your future self will thank you

8. **Know when to stop** - Time-box investigation, escalate if stuck

9. **Avoid confirmation bias** - Look for evidence that disproves your hypothesis too

10. **Treat the disease, not symptoms** - Fix root cause, not just visible symptoms

## Domain-Specific Guidance

**Software/Code:** Syntax errors, logic bugs, runtime errors, concurrency issues, memory problems
- Use debugger, add logging, rubber duck debugging, review changes, check typos

**System/Infrastructure:** Network, permissions, services, ports, resources, configuration
- Check service status, test connectivity, review logs, verify config, check resources

**Process/Workflow:** Missing steps, unmet dependencies, communication gaps, timing issues
- Map actual vs expected process, identify handoffs, check prerequisites

See `references/domain-specific-patterns.md` for comprehensive diagnostic approaches by domain.

## Common Anti-Patterns to Avoid

❌ **Change multiple things at once** - You won't know what fixed it
❌ **Assume without verifying** - "It worked last week" doesn't mean it works now
❌ **Skip reproduction** - Can't verify fix without reproducing first
❌ **Pursue only favorite hypothesis** - Confirmation bias blinds you
❌ **Give up on intermittent issues** - Often most critical
❌ **Forget to document** - Others will duplicate your work
❌ **Treat symptoms not root cause** - Suppressing errors doesn't fix problems
❌ **Blame the user** - Stops investigation prematurely
❌ **Rush to solution** - Premature fixes waste time

## Solution Verification

**After implementing fix:**

1. **Test reproduction case** - Does it now work?

2. **Test edge cases** - Does fix work in all scenarios?

3. **Regression check** - Did fix break anything else?

4. **Performance check** - Did fix introduce performance issues?

5. **Monitor in production** - Watch for recurrence or side effects

6. **Verify with users** - Confirm issue is resolved from their perspective

## Documentation

**Document for future:**

**Troubleshooting Log:**
- Problem description
- Investigation steps
- Hypotheses tested
- Results of each test
- Root cause identified
- Solution implemented

Use `assets/templates/troubleshooting_log_template.md`

**Root Cause Analysis Report:**
- Executive summary
- Problem statement
- Timeline of events
- Root cause analysis (5 Whys, Fishbone)
- Corrective actions
- Preventive measures

Use `assets/templates/rca_report_template.md`

**Reproduction Steps:**
- Minimal reproduction example
- Prerequisites
- Step-by-step instructions
- Expected vs actual results

Use `assets/templates/reproduction_steps_template.md`

## When to Escalate

**Know when to ask for help:**

- You've exhausted reasonable hypotheses
- Issue is outside your domain expertise
- Problem is time-critical and investigation is taking too long
- You need access/permissions you don't have
- Problem appears to be in external system
- You've been going in circles (revisiting same hypotheses)

**Before escalating:**
- Document what you've tried
- Share reproduction case
- Provide all diagnostic information
- Explain your hypothesis and reasoning
- Suggest what help you need

## Using Supporting Resources

Additional resources in this skill:

- **references/diagnostic-frameworks.md**: 5 Whys, Fishbone, FMEA, Binary Search, Comparative Analysis
- **references/information-gathering.md**: Log analysis, error interpretation, environment documentation
- **references/hypothesis-generation.md**: Failure patterns, hypothesis ranking, test design
- **references/domain-specific-patterns.md**: Common issues by domain (software, systems, hardware, process, data)
- **scripts/hypothesis_tracker.py**: Track hypotheses and test results
- **scripts/binary_search_helper.py**: Interactive binary search guidance
- **assets/templates/**: Documentation templates for investigation logs, RCA reports, reproduction cases

Reference these for deeper guidance on specific techniques.

---

**Remember**: The best debuggers are methodical, patient, and systematic. Resist the urge to jump to solutions. Follow the data. Document your process. The answer will reveal itself through systematic elimination.

---

### vasp

**Path**: `skills/programming/vasp/SKILL.md`

# VASP Calculation Setup Skill

You are an expert assistant for setting up VASP (Vienna Ab initio Simulation Package) calculations. Help users generate correct input files (INCAR, POSCAR, KPOINTS, POTCAR), select optimal parameters for their calculation type, and follow best practices for accurate and efficient DFT calculations.

## Overview

VASP is a plane-wave DFT code widely used in materials science and computational chemistry. This skill covers:

**Input Files:**
- INCAR: Control parameters
- POSCAR: Atomic positions and lattice
- KPOINTS: k-point sampling
- POTCAR: Pseudopotentials

**Calculation Types:**
- Structure relaxation
- Static calculations (single-point energy)
- Band structure and DOS
- Molecular dynamics
- Phonons and elastic properties
- Advanced: GW, hybrid functionals, DFPT

**Parameter Selection:**
- Accuracy vs efficiency trade-offs
- System-specific recommendations
- Convergence testing strategies

## Quick Parameter Guide

### Essential INCAR Parameters

**Energy Cutoff (ENCUT):**
```
ENCUT = 520  # eV, typical for PAW potentials
```
- **Default:** 1.3 × ENMAX from POTCAR
- **Recommendation:** 1.3-1.5 × ENMAX for standard calculations
- **Convergence test:** Test 400, 450, 500, 550, 600 eV
- **When to increase:** Forces, stresses, elastic constants

**k-Point Sampling:**
```
# Method 1: Automatic mesh
KSPACING = 0.5  # Å⁻¹, automatic generation

# Method 2: Manual KPOINTS file
# Recommended density: 30-50 k-points per Å⁻¹
```

**Precision (PREC):**
```
PREC = Accurate  # High, Normal, Accurate
```
- **Low:** Fast, testing only
- **Normal:** Standard calculations
- **Accurate:** Forces, phonons, production

**Electronic Convergence (EDIFF):**
```
EDIFF = 1E-6  # eV, energy convergence
```
- **Standard:** 1E-6 eV
- **Tight:** 1E-8 eV (forces, phonons)
- **Loose:** 1E-4 eV (quick testing)

## Input File Templates

### INCAR: Control Parameters

```bash
# System description
SYSTEM = Cu bulk FCC

# Electronic minimization
ENCUT = 520           # Cutoff energy (eV)
EDIFF = 1E-6          # SCF convergence (eV)
NELM = 100            # Max electronic steps
ALGO = Fast           # Algorithm: Normal, Fast, All
ISMEAR = 1            # Smearing: -5(tetra), 0(Gauss), 1(M-P)
SIGMA = 0.2           # Smearing width (eV)

# Precision
PREC = Accurate       # Precision level
LREAL = Auto          # Real-space projection

# Ionic relaxation
IBRION = 2            # 0=static, 1=RMM-DIIS, 2=CG
ISIF = 3              # 2=relax ions, 3=relax cell+ions
NSW = 100             # Max ionic steps
EDIFFG = -0.02        # Force convergence (eV/Å)

# Output
LWAVE = .FALSE.       # Write WAVECAR
LCHARG = .FALSE.      # Write CHGCAR
```

### POSCAR: Atomic Structure

```bash
Cu FCC bulk
1.0                    # Universal scaling
  3.61  0.00  0.00     # Lattice vectors
  0.00  3.61  0.00
  0.00  0.00  3.61
Cu                     # Element symbols
  4                    # Number of atoms
Direct                 # Direct (fractional) coordinates
  0.00  0.00  0.00
  0.50  0.50  0.00
  0.50  0.00  0.50
  0.00  0.50  0.50
```

**Key Points:**
- Line 1: Comment (system description)
- Line 2: Universal scaling factor
- Lines 3-5: Lattice vectors (Å)
- Line 6: Element symbols (must match POTCAR order)
- Line 7: Number of atoms per element
- Line 8: Coordinate type (Direct or Cartesian)
- Lines 9+: Atomic positions

### KPOINTS: k-Point Sampling

**Gamma-Centered Mesh (most common):**
```bash
Automatic mesh
0                      # 0=automatic
Gamma                  # Gamma or Monkhorst-Pack
  8  8  8              # k-point grid
  0  0  0              # Shift
```

**Monkhorst-Pack:**
```bash
Automatic mesh
0
Monkhorst-Pack
  8  8  8
  0  0  0
```

**Band Structure Path:**
```bash
k-points for band structure
10                     # Number of points between high-symmetry points
Line-mode              # Line mode for band structure
Reciprocal
  0.0  0.0  0.0   !Γ
  0.5  0.0  0.5   !X

  0.5  0.0  0.5   !X
  0.5  0.25 0.75  !W
```

### POTCAR: Pseudopotentials

**Generation:**
```bash
# Concatenate POTCARs in same order as POSCAR
cat ~/vasp/potpaw_PBE/Cu/POTCAR > POTCAR

# For compounds:
cat ~/vasp/potpaw_PBE/Cu/POTCAR \
    ~/vasp/potpaw_PBE/O/POTCAR > POTCAR
```

**Choosing POTCARs:**
- **Standard:** `potpaw_PBE/Element/POTCAR`
- **GW calculations:** `potpaw_PBE.52/Element/POTCAR` or `potpaw_PBE.54/`
- **_sv:** Include semicore states (more accurate, slower)
- **_pv:** Include p as valence
- **_h:** Harder potential (higher ENMAX)

## Parameter Selection by Calculation Type

### 1. Structure Relaxation

**INCAR:**
```bash
IBRION = 2            # Conjugate gradient
ISIF = 3              # Relax cell + ions
NSW = 100
EDIFFG = -0.02        # Force convergence
ISMEAR = 1            # Methfessel-Paxton
SIGMA = 0.2
```

**Convergence Criteria:**
- `EDIFFG < 0`: Force-based (recommended: -0.01 to -0.05 eV/Å)
- `EDIFFG > 0`: Energy-based (less common)

### 2. Static Calculation (Single-Point)

**INCAR:**
```bash
IBRION = -1           # No ionic updates
NSW = 0
ISMEAR = -5           # Tetrahedron (accurate DOS)
# OR
ISMEAR = 0            # Gaussian (if tetra not converged)
SIGMA = 0.05
```

### 3. Band Structure

**Step 1: Self-consistent calculation**
```bash
ICHARG = 2            # From atoms
LCHARG = .TRUE.       # Write CHGCAR
```

**Step 2: Non-self-consistent band structure**
```bash
ICHARG = 11           # Read CHGCAR, no update
LORBIT = 11           # Write PROCAR
# Use line-mode KPOINTS
```

### 4. Density of States (DOS)

**INCAR:**
```bash
ISMEAR = -5           # Tetrahedron method
LORBIT = 11           # Projected DOS
NEDOS = 3000          # DOS resolution
# Use dense k-point mesh
```

### 5. Molecular Dynamics

**INCAR:**
```bash
IBRION = 0            # MD
NSW = 1000            # MD steps
POTIM = 1.0           # Time step (fs)
TEBEG = 300           # Start temperature (K)
TEEND = 300           # End temperature
SMASS = 0             # NVE: 0, NVT: >0
MDALGO = 2            # 1=Andersen, 2=Nose-Hoover
```

### 6. Phonons (DFPT)

**INCAR:**
```bash
IBRION = 6            # DFPT for phonons
NFREE = 2             # Central differences
POTIM = 0.015         # Displacement (Å)
EDIFF = 1E-8          # Tight convergence!
```

### 7. Elastic Constants

**INCAR:**
```bash
IBRION = 6            # DFPT
ISIF = 3
NFREE = 4             # For elastic constants
```

## Advanced Parameters

### Hybrid Functionals (HSE06, PBE0)

**HSE06:**
```bash
LHFCALC = .TRUE.      # Activate hybrid
HFSCREEN = 0.2        # HSE screening parameter
AEXX = 0.25           # Exact exchange fraction
ALGO = All            # Or Damped
TIME = 0.4            # Damping for convergence
```

### GW Calculations

**Step 1: DFT (PBE)**
```bash
ALGO = Exact
NBANDS = 200          # Many empty bands
LOPTICS = .TRUE.
```

**Step 2: GW**
```bash
ALGO = GW0  # Or EVGW
NOMEGA = 50
```

### DFT+U (Correlated Systems)

**INCAR:**
```bash
LDAU = .TRUE.
LDAUTYPE = 2          # Dudarev
LDAUL = 2 -1          # l quantum number (d, s/p)
LDAUU = 5.0 0.0       # U value (eV)
LDAUJ = 0.0 0.0       # J value
```

### van der Waals Corrections

**DFT-D3:**
```bash
IVDW = 11             # DFT-D3 (Grimme)
```

**vdW-DF:**
```bash
GGA = MK              # optPBE-vdW
LUSE_VDW = .TRUE.
AGGAC = 0.0000
```

## Convergence Testing Strategy

### 1. k-Point Convergence

```bash
# Test sequence
KPOINTS: 4x4x4, 6x6x6, 8x8x8, 10x10x10, 12x12x12

# Converged when ΔE < 1 meV/atom between successive grids
```

### 2. Energy Cutoff Convergence

```bash
# Test ENCUT
ENCUT: 400, 450, 500, 550, 600 eV

# Converged when ΔE < 1 meV/atom
# Forces may need higher cutoff
```

### 3. Systematic Approach

1. **First:** Converge ENCUT (fix k-points at moderate density)
2. **Second:** Converge k-points (use converged ENCUT)
3. **Document:** Save convergence test results

## Smearing Methods (ISMEAR)

| ISMEAR | Method | Use Case |
|--------|--------|----------|
| -5 | Tetrahedron | Static calcs, DOS, accurate energies |
| -4 | Tetrahedron+Blöchl | Like -5, slightly different |
| -1 | Fermi smearing | Metals |
| 0 | Gaussian | General purpose |
| 1+ | Methfessel-Paxton order N | Relaxations, metals |

**Recommendations:**
- **Metals, relaxation:** ISMEAR=1, SIGMA=0.2
- **Semiconductors, relaxation:** ISMEAR=0, SIGMA=0.05
- **Static, DOS:** ISMEAR=-5 (no SIGMA needed)
- **Very large systems:** ISMEAR=-1, SIGMA=0.1

## Common Parameter Combinations

### Standard Relaxation (Metals)

```bash
# INCAR
ENCUT = 520
PREC = Accurate
IBRION = 2
ISIF = 3
NSW = 100
EDIFFG = -0.02
ISMEAR = 1
SIGMA = 0.2
ALGO = Fast
LREAL = Auto

# KPOINTS
Gamma-centered
0
Gamma
  8  8  8
  0  0  0
```

### High-Accuracy Static Calculation

```bash
# INCAR
ENCUT = 600          # Higher cutoff
PREC = Accurate
IBRION = -1
NSW = 0
EDIFF = 1E-8         # Tight convergence
ISMEAR = -5          # Tetrahedron
ALGO = Normal
LREAL = .FALSE.      # Reciprocal space

# KPOINTS (very dense)
0
Gamma
  12 12 12
  0  0  0
```

### Fast Testing Setup

```bash
# INCAR
ENCUT = 400          # Lower cutoff
PREC = Normal
EDIFF = 1E-4         # Loose
ISMEAR = 0
SIGMA = 0.1
ALGO = Fast
LREAL = Auto

# KPOINTS (coarse)
0
Gamma
  4  4  4
  0  0  0
```

## Performance Optimization

### Parallelization

**INCAR:**
```bash
NCORE = 4            # Cores per band (orbital parallelization)
# OR
NPAR = 8             # Number of groups for band parallelization
KPAR = 4             # k-point parallelization
LPLANE = .TRUE.      # Plane-wise distribution
```

**Guidelines:**
- NCORE ≈ number of cores per node / 2-4
- KPAR = number of k-points (or divisor)
- For large systems (>100 atoms): NCORE=1-4
- For many k-points: Use KPAR

### Memory Management

```bash
LREAL = Auto         # Reduce memory for large systems
NCORE = 4            # Reduce memory per core
```

## Error Handling

### Common Errors and Fixes

**"ZBRENT: fatal error in bracketing"**
```bash
# Fix: Reduce POTIM or use different IBRION
POTIM = 0.2
```

**"EDDDAV: X eigenvalues not converged"**
```bash
# Fix: Increase NELM, change ALGO
NELM = 200
ALGO = All
```

**"Sub-Space-Matrix is not hermitian"**
```bash
# Fix: Reduce POTIM, check structure
POTIM = 0.1
SYMPREC = 1E-8
```

**SCF not converging:**
```bash
# Try sequential fixes:
1. ALGO = All
2. Increase NELM = 200
3. AMIX = 0.2, BMIX = 0.0001
4. Check initial structure (too close atoms?)
```

## Best Practices

1. **Always Converge:** Test k-points and ENCUT before production runs
2. **Use Symmetry:** Let VASP detect symmetry (speeds up calculations)
3. **Check OUTCAR:** Verify "reached required accuracy" message
4. **Monitor:** Check OSZICAR during run for convergence
5. **Save Everything:** Keep all outputs (OUTCAR, vasprun.xml) for analysis
6. **Consistent Pseudopotentials:** Use same POTCAR set for all related calculations
7. **Document Settings:** Record all INCAR parameters used

## Calculation Workflows

### Full Relaxation → Properties

1. **Relaxation:** Optimize structure (ISIF=3, IBRION=2)
2. **Static:** Accurate energy (ISMEAR=-5, dense k-points)
3. **Band Structure:** Non-SCF with line-mode k-points
4. **DOS:** Dense k-mesh with ISMEAR=-5
5. **Properties:** Phonons, elastic, etc.

### Convergence Testing Workflow

1. **Rough optimization:** Low ENCUT, coarse k-points
2. **Test ENCUT:** Fixed k-points, vary ENCUT
3. **Test k-points:** Converged ENCUT, vary k-mesh
4. **Production:** Use converged parameters

## Subskills

Invoke specific subskills for detailed guidance:

- **relaxation** - Structure optimization workflows
- **electronic-structure** - Band structure and DOS calculations
- **molecular-dynamics** - MD simulations in VASP
- **advanced-functionals** - Hybrid, GW, DFT+U methods
- **phonons** - DFPT phonon calculations
- **convergence** - Systematic convergence testing

## Quick Decision Guide

**What type of calculation?**

| Goal | IBRION | ISIF | ISMEAR | EDIFFG |
|------|--------|------|--------|--------|
| Relax ions only | 2 | 2 | 1 | -0.02 |
| Relax cell+ions | 2 | 3 | 1 | -0.02 |
| Static energy | -1 | 2 | -5 | N/A |
| MD simulation | 0 | 2 | 0 | N/A |
| Band structure | -1 | 2 | 0 | N/A |
| Phonons (DFPT) | 6 | 2 | 0 | N/A |

## References

- VASP Manual: https://vasp.at/wiki/The_VASP_Manual
- VASP Tutorials: https://vasp.at/tutorials/latest/
- Parameter Index: https://vasp.at/wiki/index.php/Category:INCAR

## See Also

- `materials-properties` skill - For ASE-based workflows with VASP
- Examples in `examples/` directory
- Detailed references in `references/` directory

---

### version-control

**Path**: `skills/development/version-control/SKILL.md`

# Version Control & Git Workflow Skill

Guide users through professional version control practices using Git with trunk-based development, Conventional Commits, and GitHub integration.

## Core Philosophy

**Trunk-Based Development:**
- Keep main branch always deployable
- Short-lived feature branches (days, not weeks)
- Frequent integration to main
- Small, incremental changes
- Continuous integration mindset

**Conventional Commits:**
- Structured, semantic commit messages
- Enables automated tooling (changelogs, versioning)
- Clear intent and scope in history
- Searchable, filterable commits

**GitHub Integration:**
- Pull requests for code review
- Branch protection for main
- CI/CD with GitHub Actions
- Clear PR descriptions and discussions

## Trunk-Based Development Workflow

### The Main Branch
- **Always deployable** - Main branch should always be in working state
- **Protected** - Require PRs, reviews, passing CI
- **Source of truth** - All branches stem from and merge back to main
- **No long-lived branches** - Avoid develop, staging branches

### Feature Branch Workflow

**1. Start from main:**
```bash
git checkout main
git pull origin main
git checkout -b feat/user-authentication
```

**2. Make small, focused changes:**
- One feature/fix per branch
- Commit frequently with Conventional Commits
- Keep branch lifespan short (1-3 days ideal)

**3. Stay synchronized:**
```bash
# Update from main frequently
git checkout main
git pull origin main
git checkout feat/user-authentication
git rebase main  # Keep history linear
```

**4. Push and create PR:**
```bash
git push -u origin feat/user-authentication
# Create PR on GitHub
```

**5. After PR approval:**
```bash
# Squash merge or merge commit to main
# Delete feature branch immediately
git checkout main
git pull origin main
git branch -d feat/user-authentication
```

### Branch Naming Conventions

**Pattern: `<type>/<short-description>`**

Types match Conventional Commit types:
- `feat/add-oauth-login` - New feature
- `fix/authentication-timeout` - Bug fix
- `refactor/database-queries` - Code refactoring
- `docs/api-endpoints` - Documentation
- `test/user-service` - Tests only
- `chore/update-dependencies` - Maintenance

Keep descriptions short, lowercase, hyphen-separated.

## Conventional Commits

### Format

```
<type>(<scope>): <subject>

<body>

<footer>
```

**Required:** type, subject
**Optional:** scope, body, footer

### Types

**feat:** New feature for users
```
feat(auth): add OAuth2 authentication
```

**fix:** Bug fix
```
fix(api): handle null response in user endpoint
```

**docs:** Documentation changes
```
docs(readme): update installation instructions
```

**style:** Code style/formatting (no logic change)
```
style(components): fix indentation in Button
```

**refactor:** Code restructuring (no behavior change)
```
refactor(database): extract query builder class
```

**perf:** Performance improvement
```
perf(images): implement lazy loading
```

**test:** Adding or updating tests
```
test(auth): add integration tests for login flow
```

**build:** Build system or dependencies
```
build(deps): upgrade React to v18.2
```

**ci:** CI/CD configuration
```
ci(github): add automated deployment workflow
```

**chore:** Maintenance tasks
```
chore(deps): update development dependencies
```

**revert:** Revert previous commit
```
revert: revert "feat(auth): add OAuth2 authentication"
```

### Scope

Optional but recommended. Indicates what part of codebase changed:
- `feat(api):` - API changes
- `fix(ui):` - UI fix
- `refactor(database):` - Database refactoring
- `feat(auth):` - Authentication feature

Use project-specific scopes that make sense for your codebase.

### Subject Rules

- Use imperative mood: "add" not "added" or "adds"
- No capitalization of first letter
- No period at the end
- Keep under 50 characters
- Complete the sentence: "If applied, this commit will..."

**Good:**
```
feat(api): add user profile endpoint
fix(auth): prevent token expiration race condition
```

**Bad:**
```
feat(api): Added user profile endpoint.
fix(auth): Fixes a bug
Update stuff
```

### Body (Optional)

- Explain **what** and **why**, not **how**
- Wrap at 72 characters
- Separate from subject with blank line
- Use bullet points for multiple points

```
feat(search): implement full-text search

- Add PostgreSQL full-text search indexes
- Create search API endpoint with pagination
- Implement search result ranking by relevance

This enables users to search across all content with better
performance than previous LIKE queries.
```

### Footer (Optional)

**Breaking changes:**
```
feat(api): change authentication endpoint

BREAKING CHANGE: /auth/login moved to /api/v2/auth/login
```

**Issue references:**
```
fix(ui): prevent modal from closing on backdrop click

Fixes #123
Closes #456
```

### Examples

**Simple feature:**
```
feat(dashboard): add user activity chart
```

**Bug fix with details:**
```
fix(api): handle network timeout errors

Previously, network timeouts would crash the application.
Now we catch timeout errors and return appropriate error
response to client.

Fixes #789
```

**Breaking change:**
```
feat(api)!: redesign user authentication

BREAKING CHANGE: Auth tokens now expire after 1 hour instead
of 24 hours. Clients must implement token refresh logic.
```

**Multiple changes:**
```
refactor(database): optimize query performance

- Add indexes on frequently queried columns
- Implement connection pooling
- Cache common queries with Redis
- Update ORM configuration for better performance

These changes reduce average query time from 200ms to 50ms.
```

See `references/conventional-commits.md` for comprehensive guide.

## Essential Git Commands

### Daily Commands

**Check status:**
```bash
git status                    # See working tree status
git diff                      # Show unstaged changes
git diff --staged            # Show staged changes
```

**Make commits:**
```bash
git add <file>               # Stage specific file
git add .                    # Stage all changes
git commit -m "type: message" # Commit with message
git commit                   # Open editor for commit message
```

**Sync with remote:**
```bash
git pull origin main         # Fetch and merge from main
git push origin <branch>     # Push branch to remote
git fetch origin            # Fetch without merging
```

**Branch management:**
```bash
git branch                   # List local branches
git branch -a               # List all branches (including remote)
git checkout -b <branch>    # Create and switch to branch
git checkout <branch>       # Switch to existing branch
git branch -d <branch>      # Delete local branch
```

### Workflow Commands

**Update feature branch from main:**
```bash
git checkout main
git pull origin main
git checkout feat/my-feature
git rebase main             # Keep linear history
# Resolve conflicts if any
git push --force-with-lease origin feat/my-feature
```

**Amend last commit:**
```bash
git commit --amend          # Edit last commit message/content
git commit --amend --no-edit # Add changes to last commit
```

**Unstage changes:**
```bash
git reset HEAD <file>       # Unstage file
git reset HEAD .           # Unstage all
```

**Discard changes:**
```bash
git checkout -- <file>      # Discard changes in file
git restore <file>         # Modern alternative
git clean -fd              # Remove untracked files/dirs
```

**Stash changes:**
```bash
git stash                   # Stash working changes
git stash push -m "message" # Stash with message
git stash list             # List stashes
git stash apply            # Apply most recent stash
git stash pop              # Apply and remove stash
git stash drop             # Remove stash
```

**View history:**
```bash
git log                     # Show commit history
git log --oneline          # Compact history
git log --graph            # Visual branch graph
git log -p                 # Show patches
git log --follow <file>    # Follow file history
```

**Compare branches:**
```bash
git diff main..feat/branch  # Changes in branch vs main
git log main..feat/branch   # Commits in branch not in main
```

See `references/git-commands.md` for complete command reference.

## GitHub Pull Request Workflow

### Creating a PR

**1. Push branch:**
```bash
git push -u origin feat/user-authentication
```

**2. Create PR on GitHub with:**

**Title:** Use Conventional Commit format
```
feat(auth): add OAuth2 authentication
```

**Description template:**
```markdown
## Summary
Brief description of changes and motivation.

## Changes
- List key changes
- Use bullet points
- Be specific

## Testing
How to test these changes:
1. Step-by-step instructions
2. Expected behavior

## Screenshots (if UI changes)
[Add screenshots]

## Checklist
- [ ] Tests added/updated
- [ ] Documentation updated
- [ ] No breaking changes (or documented)
- [ ] Follows Conventional Commits
```

**3. Request reviewers**
**4. Link related issues**

### Reviewing PRs

**As reviewer:**
- Check code quality and style
- Verify tests are adequate
- Test functionality locally
- Ask questions, suggest improvements
- Approve or request changes

**PR review checklist:**
- [ ] Code follows project conventions
- [ ] Changes are well-tested
- [ ] No unnecessary changes included
- [ ] Documentation updated if needed
- [ ] Commit messages follow Conventional Commits
- [ ] No security vulnerabilities introduced
- [ ] Performance implications considered

### Addressing Review Feedback

**1. Make requested changes:**
```bash
# Make changes in your branch
git add .
git commit -m "fix(auth): address PR feedback"
git push origin feat/user-authentication
```

**2. Respond to comments**
**3. Mark conversations as resolved**
**4. Request re-review**

### Merging PRs

**Merge strategies:**

**Squash and merge (recommended for most features):**
- Combines all commits into one
- Clean main branch history
- Loses detailed commit history
- Good for: Features with many small commits

**Merge commit (for collaborative branches):**
- Preserves all commits
- Shows branch history
- Can clutter main history
- Good for: Complex features with meaningful commit history

**Rebase and merge (for clean branches):**
- Applies commits directly to main
- Linear history
- Requires clean branch history
- Good for: Branches with well-crafted commits

**After merge:**
```bash
# Delete remote branch (or use GitHub auto-delete)
git push origin --delete feat/user-authentication

# Update local main
git checkout main
git pull origin main

# Delete local branch
git branch -d feat/user-authentication
```

See `references/github-workflow.md` for detailed GitHub guidance.

## Branch Protection Rules

**Configure on GitHub for main branch:**

**Required:**
- [x] Require pull request before merging
- [x] Require approvals (at least 1)
- [x] Require status checks to pass
- [x] Require branches to be up to date

**Recommended:**
- [x] Require conversation resolution
- [x] Require linear history
- [x] Do not allow bypassing settings
- [x] Automatically delete head branches

**Optional (for teams):**
- [ ] Require review from Code Owners
- [ ] Restrict push access
- [ ] Require signed commits

## Handling Merge Conflicts

### When conflicts occur

**During merge:**
```bash
git merge main
# CONFLICT in file.py
```

**During rebase:**
```bash
git rebase main
# CONFLICT in file.py
```

### Resolution process

**1. View conflicts:**
```bash
git status  # Shows conflicted files
```

**2. Open conflicted file:**
```python
<<<<<<< HEAD
# Your changes
=======
# Their changes
>>>>>>> main
```

**3. Resolve manually:**
- Choose one version, or
- Combine both, or
- Write new solution

**4. Mark as resolved:**
```bash
git add file.py
```

**5. Continue operation:**
```bash
# For merge:
git commit

# For rebase:
git rebase --continue
```

**6. Abort if needed:**
```bash
# For merge:
git merge --abort

# For rebase:
git rebase --abort
```

### Preventing conflicts

- Pull/rebase from main frequently
- Keep branches short-lived
- Coordinate with team on overlapping work
- Make small, focused changes

See `references/troubleshooting.md` for complex conflict scenarios.

## Common Workflows

### Starting New Feature

```bash
# 1. Ensure main is current
git checkout main
git pull origin main

# 2. Create feature branch
git checkout -b feat/new-feature

# 3. Make changes and commit
git add .
git commit -m "feat(module): add new functionality"

# 4. Push and create PR
git push -u origin feat/new-feature
# Create PR on GitHub
```

### Quick Bug Fix

```bash
# 1. Create fix branch from main
git checkout main
git pull origin main
git checkout -b fix/bug-description

# 2. Fix and commit
git add .
git commit -m "fix(module): resolve bug description"

# 3. Push and create PR
git push -u origin fix/bug-description
# Create PR with "Fixes #issue-number"
```

### Updating Branch from Main

```bash
# Option 1: Rebase (preferred for clean history)
git checkout feat/my-feature
git fetch origin
git rebase origin/main
# Resolve conflicts if any
git push --force-with-lease origin feat/my-feature

# Option 2: Merge (preserves branch history)
git checkout feat/my-feature
git fetch origin
git merge origin/main
git push origin feat/my-feature
```

### Splitting Large Branch

```bash
# If branch too large, split into smaller PRs:

# 1. Create first sub-feature
git checkout -b feat/user-auth-part1 feat/user-auth
git rebase -i main  # Mark only first commits as pick
git push -u origin feat/user-auth-part1
# Create PR for part 1

# 2. After part 1 merges, continue with part 2
git checkout main
git pull origin main
git checkout -b feat/user-auth-part2 feat/user-auth
# Continue with remaining commits
```

## Git Best Practices

### Commit Practices

✅ **Do:**
- Commit frequently (multiple times per day)
- Keep commits focused (one logical change)
- Write meaningful commit messages
- Follow Conventional Commits format
- Test before committing
- Review changes before committing (`git diff`)

❌ **Don't:**
- Commit broken code to main
- Make giant commits with unrelated changes
- Write vague messages ("fix stuff", "updates")
- Commit commented-out code
- Commit sensitive data (secrets, credentials)
- Force push to main

### Branch Practices

✅ **Do:**
- Keep branches short-lived (days not weeks)
- One feature/fix per branch
- Use descriptive branch names
- Delete branches after merging
- Rebase/merge from main frequently
- Keep main always deployable

❌ **Don't:**
- Create long-lived feature branches
- Mix multiple features in one branch
- Push directly to main (without PR)
- Leave stale branches
- Let branches get too far behind main

### Collaboration Practices

✅ **Do:**
- Pull before starting work
- Communicate about overlapping work
- Review others' PRs promptly
- Provide constructive feedback
- Respond to review comments
- Use PR templates
- Link issues in PRs

❌ **Don't:**
- Work on main directly
- Force push to shared branches (without communication)
- Approve PRs without reviewing
- Ignore CI failures
- Merge your own PRs without review

### Repository Hygiene

✅ **Do:**
- Use `.gitignore` properly
- Keep repository clean
- Document in README
- Tag releases
- Archive old branches
- Use GitHub releases

❌ **Don't:**
- Commit build artifacts
- Commit dependencies (unless necessary)
- Commit IDE-specific files
- Commit large binary files
- Let main branch diverge from production

## Troubleshooting Common Issues

### "I committed to wrong branch"

```bash
# Move commit to new branch
git branch feat/new-branch    # Create branch at current commit
git reset --hard HEAD~1       # Remove commit from current branch
git checkout feat/new-branch  # Switch to new branch
```

### "I need to undo last commit"

```bash
# Keep changes, undo commit
git reset --soft HEAD~1

# Undo commit and changes
git reset --hard HEAD~1

# Undo but keep as uncommitted
git reset HEAD~1
```

### "I committed sensitive data"

```bash
# Remove from last commit
git rm --cached <file>
git commit --amend
git push --force-with-lease

# Remove from history (use tools like BFG Repo-Cleaner)
# Then rotate any exposed secrets!
```

### "My branch diverged from main"

```bash
# Rebase to fix
git fetch origin
git rebase origin/main
# Resolve conflicts
git push --force-with-lease origin feat/my-branch
```

### "I need to change commit message"

```bash
# Last commit only
git commit --amend

# Older commits
git rebase -i HEAD~3  # For last 3 commits
# Change 'pick' to 'reword' for commits to change
```

### "Rebase went wrong"

```bash
# Abort and start over
git rebase --abort

# Or if already messed up
git reflog  # Find previous state
git reset --hard <previous-commit>
```

See `references/troubleshooting.md` for more scenarios.

## Advanced Git Topics

### Interactive Rebase

```bash
# Rebase last 3 commits interactively
git rebase -i HEAD~3

# Options:
# pick - keep commit
# reword - change commit message
# edit - pause to amend commit
# squash - combine with previous commit
# fixup - like squash but discard message
# drop - remove commit
```

### Cherry-Pick

```bash
# Apply specific commit to current branch
git cherry-pick <commit-hash>

# Cherry-pick multiple commits
git cherry-pick <commit1> <commit2>

# Cherry-pick range
git cherry-pick <start>..<end>
```

### Reflog (recover lost commits)

```bash
# View reference log
git reflog

# Find lost commit
git reflog show HEAD

# Restore to previous state
git reset --hard HEAD@{2}
```

### Bisect (find bug-introducing commit)

```bash
# Start bisect
git bisect start
git bisect bad              # Current commit is bad
git bisect good <commit>    # Last known good commit

# Git will checkout middle commit, test it
git bisect good  # If works
git bisect bad   # If broken

# Repeat until found
git bisect reset  # Return to original state
```

### Worktrees (work on multiple branches)

```bash
# Create worktree for different branch
git worktree add ../project-feature feat/feature

# List worktrees
git worktree list

# Remove worktree
git worktree remove ../project-feature
```

See `references/advanced-git.md` for detailed coverage.

## GitHub Actions Integration

### Basic CI Workflow

```yaml
# .github/workflows/ci.yml
name: CI

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      - name: Install
        run: npm ci
      - name: Test
        run: npm test
      - name: Lint
        run: npm run lint
```

### Conventional Commit Validation

```yaml
# .github/workflows/commit-lint.yml
name: Commit Lint

on: [pull_request]

jobs:
  lint-commits:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - uses: wagoid/commitlint-github-action@v5
```

See `examples/github-actions/` for more workflow examples.

## Tools and Resources

### Recommended Tools

**Commit message helpers:**
- `commitizen` - Interactive commit message builder
- `commitlint` - Validate commit messages
- `husky` - Git hooks for validation

**GitHub CLI:**
```bash
gh pr create --title "feat: add feature" --body "Description"
gh pr checkout 123
gh pr review --approve
gh pr merge --squash
```

**Visual tools:**
- `gitk` - Built-in Git GUI
- GitKraken, SourceTree - Full-featured GUIs
- VS Code Git integration
- GitHub Desktop

### Configuration

**Useful git config:**
```bash
# User info
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"

# Editor
git config --global core.editor "vim"

# Default branch
git config --global init.defaultBranch main

# Rebase by default
git config --global pull.rebase true

# Helpful aliases
git config --global alias.st status
git config --global alias.co checkout
git config --global alias.br branch
git config --global alias.cm commit
git config --global alias.lg "log --oneline --graph --decorate"
```

### Learning Resources

- **references/conventional-commits.md** - Complete Conventional Commits guide
- **references/trunk-based-development.md** - Detailed TBD workflow
- **references/github-workflow.md** - GitHub-specific practices
- **references/git-commands.md** - Comprehensive command reference
- **references/troubleshooting.md** - Problem-solving guide
- **references/advanced-git.md** - Advanced techniques
- **examples/** - Example commit messages, PR templates, workflows
- **assets/templates/** - Templates for PRs, commits, checklists

## When to Use This Skill

Activate for requests involving:
- Git commands and operations
- Branching and merging strategies
- Commit message formatting
- Pull request creation and review
- Merge conflict resolution
- Repository management
- GitHub workflow questions
- Git troubleshooting
- Version control best practices
- Team collaboration with Git

## Quick Reference

**Daily Commands:**
```bash
git status                      # Check status
git add .                       # Stage changes
git commit -m "type: message"  # Commit
git pull origin main           # Update from main
git push origin branch         # Push changes
```

**Branch Workflow:**
```bash
git checkout main              # Switch to main
git pull origin main           # Update
git checkout -b feat/name      # Create branch
# ... make changes ...
git push -u origin feat/name   # Push and create PR
```

**Conventional Commit:**
```
type(scope): subject

- Use feat, fix, docs, style, refactor, test, chore
- Imperative mood in subject
- Optional body for details
- Optional footer for breaking changes/issues
```

**Stay Updated:**
```bash
git fetch origin               # Fetch updates
git rebase origin/main         # Rebase on main
git push --force-with-lease    # Force push safely
```

---

**Remember:** Small commits, frequent integration, clear messages, short-lived branches, and always keep main deployable. This is the path to professional version control.

---

### video-storytelling

**Path**: `skills/creative/video-storytelling/SKILL.md`

# Video Storytelling

## Purpose

This skill creates coherent video story sequences by combining AI-generated images with narrated audio. Acts as a story director and visual coordinator, maintaining perfect consistency across characters, visual style, lighting, and narrative tone throughout all scenes. Produces complete MP4 videos with synchronized images and character voiceovers.

## When to Use

This skill should be invoked when the user asks to:
- Create a video story or animated sequence
- Generate a narrated story with visuals
- Produce educational video content with characters
- Make a visual storybook with voiceover
- Create character-driven video narratives
- Generate multi-scene story videos
- Produce children's stories with pictures and narration

## Core Capabilities

### Visual Consistency System

**Global Style Lock:**
- Fixed aspect ratio, camera settings, lighting
- Consistent color palette across all scenes
- Uniform visual style and post-processing
- Prevents visual discontinuities

**Character Lock:**
- Maintains character appearance across scenes
- Same outfit, colors, facial features throughout
- Consistent accessories and distinctive traits
- Visual identity preservation

**Multi-Turn Image Generation:**
- Each scene references previous scene's image
- Builds visual continuity through the sequence
- Prevents character/style drift

### Narrative System

**Character Voices:**
- Maps characters to ElevenLabs voices
- Maintains voice consistency per character
- Supports multiple characters in dialogue

**Emotion Tags:**
- Expressive narration with emotion markers
- Sound effects and pauses
- Natural pacing and delivery

**Narrative Structure:**
- 50-80 words per scene (15-20 seconds)
- Neutral narrator for transitions
- Character-specific dialogue

### Video Assembly

**Automated Pipeline:**
- Generates all images in sequence
- Creates character voice narration
- Combines into synchronized MP4 video
- Equal time per scene based on total audio length

## Default Configuration

### Scene Structure
- **Default:** 1 title scene (scene 0) + 5 story scenes (scenes 1-5)
- **Total:** 6 scenes
- **Customizable:** User can specify different scene counts

### Default Style Lock

```
STYLE_LOCK:
- Aspect ratio: 1080×1080 (square)
- Camera: 50mm lens, eye-level perspective
- Lighting: soft three-point lighting, warm key light (4500K)
- Color palette: #0B5FFF, #FFB703, #FB8500, #023047, #8ECAE6
- Materials: matte finish, no film grain or heavy bloom
- Background: subtle gradient, clean composition
- Style: semi-realistic cartoon with clear lines and gentle shading
- Post: crisp focus, no vignette or text artifacts

NEGATIVE_LOCK:
no text errors, no misspellings, no watermarks, no stickers,
no extra characters, no visual noise, no drastic lighting changes
```

**Customization:** Users can override with custom style locks, but defaults ensure consistency.

### Default Voice Mapping

**From ElevenLabs Voices:**

**Narrators:**
- Neutral Narrator (male): George (`JBFqnCBsd6RMkjVDRZzb`)
- Neutral Narrator (female): Rachel (`21m00Tcm4TlvDq8ikWAM`)

**Character Voices:**
- Young Male (energetic): Josh (`TxGEqnHWrfWFTfGW9XjX`)
- Young Female (calm): Rachel (`21m00Tcm4TlvDq8ikWAM`)
- Young Female (expressive): Bella (`EXAVITQu4vr4xnSDxMaL`)
- Male (authoritative): Adam (`pNInz6obpgDQGcFmaJgB`)
- Female (warm): Matilda (`XrExE9yKIg1WjnnlVkGX`)
- Young Male (friendly): Antoni (`ErXwobaYiN019PkySvjV`)

**Assignment Logic:**
- If character gender/age specified, match to appropriate voice
- If unspecified, use Josh for male, Rachel for female
- Narrator defaults to George (male) or Rachel (female)

## Instructions

### Step 1: Gather Story Information

Collect necessary information from the user:

**Required:**
- **Story concept:** What is the story about?
- **Tone/Genre:** Educational, adventure, comedy, drama, etc.

**Optional (prompt if missing):**
- **Number of scenes:** Default is 6 (1 title + 5 story), but user can specify
- **Character descriptions:** Names, appearance, personality
- **Custom style locks:** Override defaults if user has specific requirements

**Example Prompts:**
```
"What's your story about?"
"How many scenes would you like? (Default: 1 title + 5 story scenes)"
"Describe your main character(s): name, appearance, personality"
"Any specific visual style preferences? (Default: semi-realistic cartoon)"
```

### Step 2: Define Characters

For each character in the story, create a character profile:

**Character Profile Template:**
```python
character = {
    "name": "Character Name",
    "species": "human/animal/creature",
    "description": "brief description",
    "colors": {
        "primary": "#HEX",
        "secondary": "#HEX"
    },
    "outfit": "clothing description",
    "features": ["distinctive trait 1", "trait 2", "trait 3"],
    "personality": "personality description",
    "voice_id": "elevenlabs-voice-id",
    "voice_name": "ElevenLabs voice name"
}
```

**Example:**
```python
pyter_python = {
    "name": "Pyter Python",
    "species": "friendly snake mascot",
    "description": "A cheerful coding mentor snake",
    "colors": {
        "body": "#0B5FFF",  # Blue
        "belly": "#FFB703"   # Yellow
    },
    "outfit": "tiny white lab coat with circular π logo",
    "features": ["large brown eyes", "rounded head", "cheerful smile"],
    "personality": "enthusiastic, helpful, curious",
    "voice_id": "TxGEqnHWrfWFTfGW9XjX",
    "voice_name": "Josh"
}
```

**Voice Assignment:**
- Ask user for voice preference or auto-assign based on character
- Use default mapping for common types
- Allow custom voice selection from ElevenLabs library

### Step 3: Plan Story Sequence

Create scene-by-scene outline:

**Scene 0 (Title Scene):**
- Visual: Title card with main character(s)
- Audio: Story introduction (narrator or main character)
- Duration: ~15-20 seconds

**Scenes 1-N (Story Scenes):**
- Visual: Sequential story moments
- Audio: Narrative with character dialogue
- Duration: ~15-20 seconds each

**Example Scene Plan:**
```python
scene_plan = [
    {
        "number": 0,
        "type": "title",
        "visual_description": "Pyter Python with laptop, 'Pyter's Coding Adventure' text overlay",
        "characters": ["Pyter Python"],
        "narrative": "[cheerful] Join Pyter Python on an exciting coding adventure!",
        "speaker": "Narrator",
        "voice_id": "JBFqnCBsd6RMkjVDRZzb"
    },
    {
        "number": 1,
        "type": "story",
        "visual_description": "Pyter at desk looking at computer screen showing error message, confused expression",
        "characters": ["Pyter Python"],
        "narrative": "[confused] Hmm... what does this error message mean? [pause] I thought my code was perfect!",
        "speaker": "Pyter Python",
        "voice_id": "TxGEqnHWrfWFTfGW9XjX"
    },
    # ... more scenes
]
```

### Step 4: Build Style and Character Locks

**Prepare Global Style Lock:**
```python
STYLE_LOCK = """
Aspect ratio: 1080×1080 (square)
Camera: 50mm lens, eye-level perspective
Lighting: soft three-point lighting, warm key light (4500K)
Color palette: #0B5FFF, #FFB703, #FB8500, #023047, #8ECAE6
Materials: matte finish, no film grain or heavy bloom
Background: subtle gradient, clean composition
Style: semi-realistic cartoon with clear lines and gentle shading
Post: crisp focus, no vignette or text artifacts
"""

NEGATIVE_LOCK = """
no text errors, no misspellings, no watermarks, no stickers,
no extra characters, no visual noise, no drastic lighting changes
"""
```

**Build Character Lock for Each Scene:**
```python
def build_character_lock(characters_in_scene):
    lock = ""
    for character in characters_in_scene:
        lock += f"""
Character: {character['name']}
Species: {character['species']}
Colors: body {character['colors']['primary']}, secondary {character['colors']['secondary']}
Outfit: {character['outfit']}
Key features: {', '.join(character['features'])}
"""
    return lock
```

### Step 5: Generate Image Sequence

Generate images using multi-turn generation for consistency:

**Implementation:**
```python
from pathlib import Path
import json

# Initialize tracking
previous_image_id = None
image_files = []

# Generate each scene
for scene in scene_plan:
    print(f"Generating Scene {scene['number']}: {scene['visual_description']}")

    # Build character lock for this scene
    character_lock = build_character_lock(
        [char_profiles[name] for name in scene['characters']]
    )

    # Build complete image prompt
    image_prompt = f"""
{STYLE_LOCK}

{character_lock}

Scene Description:
{scene['visual_description']}

{NEGATIVE_LOCK}
"""

    # Add reference to previous scene if not first scene
    if previous_image_id:
        image_prompt += f"\nReference previous scene for consistency: {previous_image_id}"

    # Generate image using image-generation skill
    # (This would invoke the image-generation skill)
    # For implementation, use appropriate model (DALL-E 3 or Gemini Pro)

    result = generate_image(
        prompt=image_prompt,
        model="dall-e-3",  # or gemini-3-pro-image-preview
        size="1024x1024",
        reference_image=previous_image_id
    )

    # Save image
    filename = f"scene-{scene['number']:02d}.png"
    save_image(result, filename)
    image_files.append(filename)

    # Track for next scene reference
    previous_image_id = result['image_id']

    print(f"  ✓ Saved: {filename}")
```

**Key Points:**
- Scene 0 generates base image
- Scenes 1+ reference previous scene for consistency
- Apply STYLE_LOCK and CHARACTER_LOCK to every prompt
- Save with sequential numbering

### Step 6: Generate Narrative Audio

Create voice narration for each scene:

**Implementation:**
```python
from elevenlabs.client import ElevenLabs

client = ElevenLabs(api_key=os.environ['ELEVENLABS_API_KEY'])
audio_files = []

for scene in scene_plan:
    print(f"Generating audio for Scene {scene['number']}")

    # Prepare dialogue input
    dialogue_input = {
        "text": scene['narrative'],
        "name": scene['speaker'],
        "voice_id": scene['voice_id']
    }

    # Generate audio using text_to_dialogue
    audio = client.text_to_dialogue.convert(
        inputs=[dialogue_input]
    )

    # Save audio file
    filename = f"scene-{scene['number']:02d}.mp3"
    with open(filename, 'wb') as f:
        for chunk in audio:
            f.write(chunk)

    audio_files.append(filename)
    print(f"  ✓ Saved: {filename}")
```

**Narrative Guidelines:**
- 50-80 words per scene
- Use emotion tags: `[excited]`, `[thoughtful]`, `[confused]`, `[pause]`
- Include sound effects when appropriate: `[sound effect: door creaking]`
- Vary pacing with pauses

### Step 7: Concatenate Audio

Combine all scene audio into single track:

**Implementation:**
```python
import subprocess

# Build ffmpeg concat command
concat_filter = "concat=n={}:v=0:a=1[out]".format(len(audio_files))

inputs = []
for audio_file in audio_files:
    inputs.extend(['-i', audio_file])

cmd = ['ffmpeg', '-y'] + inputs + [
    '-filter_complex', concat_filter,
    '-map', '[out]',
    'full_audio.mp3'
]

subprocess.run(cmd, check=True)
print("✓ Audio concatenated: full_audio.mp3")
```

### Step 8: Assemble Final Video

Use the included `assemble_video.sh` script:

**Implementation:**
```python
import subprocess
from pathlib import Path

# Prepare command
script_path = Path(__file__).parent / "scripts" / "assemble_video.sh"
cmd = [str(script_path), "full_audio.mp3"] + image_files

# Run assembly
subprocess.run(cmd, check=True)

# Output will be full_audio.mp4
print("✓ Video created: full_audio.mp4")
```

**Script Details:**
- Calculates equal time per image based on total audio length
- Creates video segment for each image
- Ensures all images are exactly 1080×1080 (pads if needed)
- Concatenates segments
- Muxes with audio track
- Outputs high-quality MP4 with H.264

### Step 9: Deliver Results

Provide user with:
1. **Final video file:** `<story-name>.mp4`
2. **Scene breakdown:** Summary of each scene
3. **Individual assets:** Images and audio files (if requested)
4. **Story metadata:** Character profiles, scene plan (if requested)

**Example Output:**
```
✓ Video Story Created: pyter-coding-adventure.mp4

Scenes:
  0. Title: "Pyter's Coding Adventure" (20s)
  1. Pyter encounters an error (18s)
  2. Pyter realizes the mistake (17s)
  3. Pyter fixes the code (19s)
  4. Code runs successfully (16s)
  5. Pyter celebrates (15s)

Total Duration: 1:45
Resolution: 1080×1080
Characters: Pyter Python (voiced by Josh)

Files generated:
  - pyter-coding-adventure.mp4 (final video)
  - scene-00.png through scene-05.png (images)
  - scene-00.mp3 through scene-05.mp3 (audio)
  - full_audio.mp3 (concatenated audio)
```

## Character Voice Reference

### ElevenLabs Voice IDs

**Narrators:**
- **George** (male, middle-aged, narrative): `JBFqnCBsd6RMkjVDRZzb`
- **Rachel** (female, young, calm): `21m00Tcm4TlvDq8ikWAM`

**Young Characters:**
- **Josh** (male, energetic): `TxGEqnHWrfWFTfGW9XjX`
- **Bella** (female, expressive): `EXAVITQu4vr4xnSDxMaL`
- **Antoni** (male, friendly): `ErXwobaYiN019PkySvjV`
- **Elli** (female, emotional): `MF3mGyEYCl7XYWbV9V6O`

**Adult Characters:**
- **Adam** (male, authoritative): `pNInz6obpgDQGcFmaJgB`
- **Domi** (female, confident): `AZnzlk1XvdvUeBnXmlld`
- **Matilda** (female, warm): `XrExE9yKIg1WjnnlVkGX`

**Assignment Strategy:**
```python
def assign_voice(character):
    """Auto-assign voice based on character attributes"""

    # Check for explicit assignment
    if 'voice_preference' in character:
        return get_voice_id(character['voice_preference'])

    # Auto-assign based on attributes
    age = character.get('age', 'young')
    gender = character.get('gender', 'male')

    if age == 'young':
        if gender == 'male':
            return 'TxGEqnHWrfWFTfGW9XjX'  # Josh
        else:
            return '21m00Tcm4TlvDq8ikWAM'  # Rachel
    else:  # adult
        if gender == 'male':
            return 'pNInz6obpgDQGcFmaJgB'  # Adam
        else:
            return 'XrExE9yKIg1WjnnlVkGX'  # Matilda
```

## Example Story Generation

### Complete Example: "Pyter's First Bug"

**User Request:** "Create a short story about a coding snake fixing his first bug"

**Step 1: Character Definition**
```python
pyter = {
    "name": "Pyter Python",
    "species": "friendly snake",
    "colors": {"body": "#0B5FFF", "belly": "#FFB703"},
    "outfit": "white lab coat with π logo",
    "features": ["large brown eyes", "rounded head", "cheerful smile"],
    "personality": "enthusiastic learner",
    "voice_id": "TxGEqnHWrfWFTfGW9XjX"  # Josh
}
```

**Step 2: Scene Plan**
```python
scenes = [
    {
        "number": 0,
        "visual": "Pyter with laptop, title 'Pyter's First Bug'",
        "narrative": "[cheerful] Today, Pyter Python will fix his very first coding bug!",
        "speaker": "Narrator",
        "voice_id": "JBFqnCBsd6RMkjVDRZzb"
    },
    {
        "number": 1,
        "visual": "Pyter staring at screen with red error message",
        "narrative": "[confused] Wait... why isn't my code working? [pause] The computer says there's a syntax error!",
        "speaker": "Pyter",
        "voice_id": "TxGEqnHWrfWFTfGW9XjX"
    },
    {
        "number": 2,
        "visual": "Pyter reading a Python book, thoughtful",
        "narrative": "[thoughtful] Let me check the Python book... [pause] Oh! I need to look at line 5 carefully.",
        "speaker": "Pyter",
        "voice_id": "TxGEqnHWrfWFTfGW9XjX"
    },
    {
        "number": 3,
        "visual": "Close-up of Pyter pointing at screen, realization",
        "narrative": "[excited] I found it! I forgot to close the parentheses! [pause] That's the bug!",
        "speaker": "Pyter",
        "voice_id": "TxGEqnHWrfWFTfGW9XjX"
    },
    {
        "number": 4,
        "visual": "Screen showing 'Success!' with green checkmark",
        "narrative": "[proud] I fixed it! My code is running perfectly now!",
        "speaker": "Pyter",
        "voice_id": "TxGEqnHWrfWFTfGW9XjX"
    },
    {
        "number": 5,
        "visual": "Pyter celebrating, confetti in background",
        "narrative": "[warm] And that's how Pyter learned that every programmer makes mistakes... and that's okay!",
        "speaker": "Narrator",
        "voice_id": "JBFqnCBsd6RMkjVDRZzb"
    }
]
```

**Step 3: Generate** (using process described above)

**Output:** `pyters-first-bug.mp4` with 6 scenes, ~90 seconds total

## Requirements

**Skills:**
- `image-generation` - For creating consistent visual scenes
- `elevenlabs` - For character voice narration

**Python Packages:**
```bash
pip install elevenlabs pillow
```

**System:**
- Python 3.8+
- ffmpeg (for video assembly)
- Bash shell (for assemble_video.sh script)
- 2GB+ free disk space (for temporary files)

**API Keys:**
- OpenAI or Google (for image generation)
- ElevenLabs (for voice narration)

**File Permissions:**
- Execute permission for `assemble_video.sh`

## Best Practices

### Story Planning

1. **Keep it Simple:**
   - Start with 6 scenes (1 title + 5 story)
   - Clear beginning, middle, end
   - Single main character for first stories

2. **Character Consistency:**
   - Define characters completely before starting
   - Use distinctive visual features
   - Maintain outfit/colors throughout

3. **Pacing:**
   - 15-20 seconds per scene ideal
   - Use pauses for dramatic effect
   - Vary emotion tags for expressiveness

### Visual Consistency

1. **Use Style Locks:**
   - Apply to every scene without exception
   - Don't modify mid-story
   - Custom locks should be complete, not partial

2. **Character Locks:**
   - Specify colors with hex codes
   - List 3-5 distinctive features
   - Include outfit details

3. **Multi-Turn References:**
   - Always reference previous scene
   - Mention "maintain character appearance"
   - Note "same lighting and style"

### Audio Quality

1. **Narrative Guidelines:**
   - Write naturally for speech
   - Use emotion tags sparingly (1-2 per scene)
   - Include pauses for pacing

2. **Voice Selection:**
   - Match voice to character age/personality
   - Keep narrator voice neutral
   - Maintain voice consistency per character

3. **Audio Testing:**
   - Generate one scene first to test
   - Verify voice/emotion match intent
   - Adjust before generating all scenes

### Video Assembly

1. **File Organization:**
   - Use consistent naming (scene-XX.png/mp3)
   - Keep in flat directory structure
   - Clean up temp files after assembly

2. **Quality Settings:**
   - Default 1080×1080 ensures quality
   - H.264 baseline profile for compatibility
   - AAC audio at 192kbps

3. **Testing:**
   - Verify all images are same size
   - Check audio files are valid
   - Test script with 2-3 scenes first

## Troubleshooting

### Visual Inconsistencies

**Problem:** Character looks different across scenes

**Solutions:**
- Ensure character lock is applied to every prompt
- Verify previous image is referenced
- Add "maintain exact character appearance from previous scene"
- Use more specific color hex codes

### Audio Issues

**Problem:** Voice doesn't match character

**Solutions:**
- Verify voice_id is correct
- Test voice with sample text first
- Check character voice assignment logic

**Problem:** Concatenated audio has gaps

**Solutions:**
- Ensure all audio files are valid MP3
- Check ffmpeg concat filter syntax
- Verify no missing scene audio files

### Video Assembly Errors

**Problem:** Script fails with "file not found"

**Solutions:**
- Verify all image files exist
- Check audio file path
- Ensure script has execute permissions

**Problem:** Images different sizes in video

**Solutions:**
- Verify all images are 1080×1080
- Check image generation settings
- Script auto-pads, but prefer exact size

## Limitations

1. **Scene Count:**
   - Practical limit: 10-12 scenes (video length ~3 minutes)
   - More scenes = longer generation time
   - Audio/video file size considerations

2. **Character Complexity:**
   - 1-3 main characters recommended
   - Too many characters = harder consistency
   - Background characters okay if not detailed

3. **Visual Changes:**
   - Can't change style mid-story
   - Character outfit changes require new character lock
   - Major scene changes (day/night) may reduce consistency

4. **Audio Length:**
   - Each scene 15-20 seconds ideal
   - Very short scenes (<10s) feel rushed
   - Very long scenes (>30s) slow pacing

5. **Processing Time:**
   - Image generation: 30-60s per scene
   - Audio generation: 10-20s per scene
   - Video assembly: 30-60s total
   - Total: ~10-15 minutes for 6-scene story

## Related Skills

- `image-generation` - Required for visual generation
- `elevenlabs` - Required for voice narration
- `python-plotting` - For visualizing story analytics
- `scientific-writing` - For writing narrative scripts

## Additional Resources

- **Image Generation Skill**: See `image-generation/SKILL.md`
- **ElevenLabs Skill**: See `elevenlabs/SKILL.md`
- **Style Lock Reference**: See `references/style-locks.md`
- **Narrative Design**: See `references/narrative-design.md`
- **Video Assembly**: See `references/video-assembly.md`
- **Example Stories**: See `examples/example-stories.md`

---


---

## Commands

### README



**Location**: `commands/README.md`

# Slash Commands

This directory contains slash commands for Claude Code and other LLM platforms.

## What are Slash Commands?

Slash commands are user-invoked shortcuts that provide Claude with specific instructions. Unlike skills (which Claude invokes automatically), commands are explicitly called by users.

## Usage

```bash
# List available commands
claude-skills list --type command

# Install a command
claude-skills install summarize --type command

# View command details
claude-skills info explain-code --type command
```

## Command Format

Commands are markdown files with optional YAML frontmatter:

```markdown
---
description: Brief description for /help
model: sonnet  # Optional: sonnet, opus, or haiku
argument-hint: <your-arg>  # Optional: autocomplete help
allowed-tools: ["Read", "Write"]  # Optional: restrict tools
---

# Command Content

Instructions for Claude...

Use $ARGUMENTS or $1, $2, etc. for parameters.
```

## Directory Structure

```
commands/
├── examples/          # Example commands
│   ├── summarize.md
│   ├── explain-code.md
│   └── review-pr.md
└── [category]/       # Organize by category
    └── command.md
```

## Available Commands

### Examples Category

- **summarize** - Summarize files or content
- **explain-code** - Detailed code explanations
- **review-pr** - Code review with best practices

## Creating Commands

1. Create a `.md` file in the appropriate category
2. Add frontmatter (optional but recommended)
3. Write clear instructions
4. Use parameters: `$ARGUMENTS`, `$1`, `$2`, etc.
5. Validate: `claude-skills validate commands/your-command.md`

### Command Requirements

- **File Extension**: Must be `.md`
- **Description**: Max 256 characters (if provided)
- **Model**: Must be `sonnet`, `opus`, or `haiku` (if specified)
- **Name**: Derived from filename (use lowercase with hyphens)

### Best Practices

1. **Be Specific**: Commands should have clear, focused purposes
2. **Use Parameters**: Make commands flexible with `$ARGUMENTS`
3. **Document Well**: Use `description` and `argument-hint`
4. **Test First**: Validate before committing
5. **Keep Simple**: Complex workflows might be better as skills

### Example Command

```markdown
---
description: Format code according to style guide
argument-hint: <file-pattern>
model: sonnet
allowed-tools: ["Read", "Edit", "Bash"]
---

# Format Code

Please format the following files according to best practices: $ARGUMENTS

For Python files:
- Use Black for formatting
- Run Ruff for linting
- Check with mypy

For other files, apply appropriate language-specific formatters.
```

## Contributing

See [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines on contributing commands.

## See Also

- [Skills Directory](../skills/) - Auto-invoked agent skills
- [Templates](../templates/) - Templates for creating new items
- [CLI Documentation](../README.md) - Full CLI documentation

---

### explain-code

Explain how code works in detail

**Usage**: `explain-code <file-or-function>`

**Location**: `commands/examples/explain-code.md`

# Explain Code

Please explain how the following code works: $ARGUMENTS

Provide a detailed explanation covering:

1. **Purpose**: What does this code do?
2. **Structure**: How is it organized?
3. **Key Logic**: What are the important operations or algorithms?
4. **Data Flow**: How does data move through the code?
5. **Edge Cases**: What special cases are handled?
6. **Dependencies**: What does it rely on?

Use clear language and examples where helpful.

---

### review-pr

Review code changes with best practices

**Usage**: `review-pr <files-or-diff>`

**Location**: `commands/examples/review-pr.md`

# Code Review

Please perform a thorough code review of the changes in: $ARGUMENTS

Review for:

## Code Quality
- [ ] Code is readable and well-structured
- [ ] Naming conventions are clear and consistent
- [ ] Functions are focused and single-purpose
- [ ] No unnecessary complexity

## Best Practices
- [ ] Follows language-specific conventions
- [ ] Error handling is appropriate
- [ ] No security vulnerabilities
- [ ] Performance considerations addressed

## Testing & Documentation
- [ ] Changes are testable
- [ ] Tests are included (if applicable)
- [ ] Documentation is updated
- [ ] Comments explain complex logic

## Suggestions
Provide specific, actionable feedback with examples where helpful.

---

### summarize

Summarize a file or set of files

**Usage**: `summarize <file-pattern>`

**Location**: `commands/examples/summarize.md`

# Summarize Files

Please read and provide a concise summary of the following file(s): $ARGUMENTS

Include:
1. Main purpose/functionality
2. Key components or sections
3. Important details or patterns
4. Any notable observations

Keep the summary clear and focused.

---


---

**Configuration generated**: 2025-12-23 21:33:26